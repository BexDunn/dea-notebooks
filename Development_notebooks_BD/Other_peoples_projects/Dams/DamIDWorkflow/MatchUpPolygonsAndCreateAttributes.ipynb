{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match up polygons and create attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "\n",
    "from datacube import Datacube\n",
    "from datacube.utils import geometry\n",
    "from datacube.storage import masking\n",
    "import fiona\n",
    "import rasterio.features\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All years processed so far\n",
    "CumulativeDamShapeFile = '/g/data/r78/cek156/dea-notebooks/Dams/MinMaxSizeRiverFilteredDams/AllNSW200010pcMerged.shp'\n",
    "\n",
    "# New year to add to the cumulative shapefile\n",
    "NewYearToAdd = '/g/data/r78/cek156/dea-notebooks/Dams/MinMaxSizeRiverFilteredDams/AllNSW200110pcMerged.shp'\n",
    "\n",
    "# Output shapefile\n",
    "CumulativeDamShapeFileOutput = '/g/data/r78/cek156/dea-notebooks/Dams/TestMergeYears.shp'\n",
    "NaughtyShapefile = '/g/data/r78/cek156/dea-notebooks/Dams/TestNaughty.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CumulativeDams = gp.read_file(CumulativeDamShapeFile)\n",
    "NewDams = gp.read_file(NewYearToAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find where the new polygons are contained within the master polygons\n",
    "\n",
    "I.e. where the new polygons sit completely inside the master ones. This means the new polygons are only seeing a part of the water. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join for polygons that are contained within the master polygon set\n",
    "Contains = gp.sjoin(CumulativeDams, NewDams, how='inner', op='contains')\n",
    "\n",
    "# Get the index of the new polygons that are contained within the master polygon dataset\n",
    "ContainsIndex = sorted(Contains['index_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the polygons that are contained within the master polygon set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewDamsNotContained = NewDams.loc[~NewDams.index.isin(ContainsIndex)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find where our new polygons intersect with the master polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join for polygons that intersect\n",
    "IntersectingPolys = gp.sjoin(NewDamsNotContained, CumulativeDams, how='inner', op='intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group polygons based on the new polygons' index\n",
    "\n",
    "This will allow for multiple old polygons to match up with a single new polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedPolygonsNew = IntersectingPolys.groupby(IntersectingPolys.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group polygons based on the master polygons' index\n",
    "\n",
    "This will allow for multiple new polygons to match up with a single master polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedPolygonsMaster = IntersectingPolys.groupby(IntersectingPolys['index_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now deal with all our overlapping polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the polygons based on where:\n",
    "\n",
    "- New and master polygon areas are < 20 pixels or <= 20% of area different\n",
    "    - Append the index of the new larger polygon\n",
    "    - We will keep the larger polygon and replace it in the master polygon set\n",
    "- New polygon is much larger than the original\n",
    "    - Append the indexes of both polygons\n",
    "    - We will move both polygons (new one and master) into the 'naughty corner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PolygonsToKeep = []\n",
    "MasterPolygonsToMerge = []\n",
    "NaughtyCornerMaster = []\n",
    "NaughtyCornerNew = []\n",
    "MergedPolygons = gp.GeoDataFrame()\n",
    "\n",
    "# Deals with multiple masters for a single new polygon\n",
    "for groups in GroupedPolygonsNew:\n",
    "    NewArea = groups[1]['area_left'].values\n",
    "    MasterArea = groups[1]['area_right'].values\n",
    "    for ix, item in enumerate(MasterArea):\n",
    "        # Check if the two polygons are within 20 pixels or 20% of area in size \n",
    "        if (NewArea[0] - int(item) < 12500) or (abs((NewArea[0] - int(item))/int(item) * 100) <= 20):\n",
    "            # Append new larger area index\n",
    "            PolygonsToKeep.append(groups[1].index.values[ix])\n",
    "            # Append the master polygon index so we know to replace it\n",
    "            MasterPolygonsToMerge.append(groups[1]['index_right'].values[ix])\n",
    "            # Now create a unary union of the two polygons and write it to a new dataframe\n",
    "            MasterToUnion = CumulativeDams.loc[groups[1]['index_right']]\n",
    "            NewToUnion = NewDamsNotContained.loc[groups[1].index]\n",
    "            MergedGeometry = pd.concat([MasterToUnion, NewToUnion]).unary_union\n",
    "            MergedGDF = gp.GeoDataFrame(geometry = [MergedGeometry])\n",
    "            MergedGDF['area'] = MergedGeometry.area\n",
    "            MergedPolygons = pd.concat([MergedPolygons, MergedGDF], ignore_index=True)\n",
    "        # The polygons are too different, so we will deal with them later\n",
    "        else:\n",
    "            # Append larger area index\n",
    "            NaughtyCornerNew.append(groups[1].index.values[ix])\n",
    "            # Append the master index\n",
    "            NaughtyCornerMaster.append(groups[1]['index_right'].values[ix])\n",
    "            \n",
    "# Deals with multiple new for a single master\n",
    "for groups in GroupedPolygonsMaster:\n",
    "    MasterArea = groups[1]['area_left'].values\n",
    "    NewArea = groups[1]['area_right'].values\n",
    "    for ix, item in enumerate(NewArea):\n",
    "        # Check if the two polygons are within 20 pixels or 20% of area in size \n",
    "        if (MasterArea[0] - int(item) < 12500) or (abs((MasterArea[0] - int(item))/int(item) * 100) <= 20):\n",
    "            # Append new larger area index\n",
    "            PolygonsToKeep.append(groups[1].index.values[ix])\n",
    "            # Append the master polygon index so we know to replace it\n",
    "            MasterPolygonsToMerge.append(groups[1]['index_right'].values[ix])\n",
    "            # Now create a unary union of the two polygons and write it to a new dataframe\n",
    "            MasterToUnion = CumulativeDams.loc[groups[1]['index_right']]\n",
    "            NewToUnion = NewDamsNotContained.loc[groups[1].index]\n",
    "            MergedGeometry = pd.concat([MasterToUnion, NewToUnion]).unary_union\n",
    "            MergedGDF = gp.GeoDataFrame(geometry = [MergedGeometry])\n",
    "            MergedGDF['area'] = MergedGeometry.area\n",
    "            MergedPolygons = pd.concat([MergedPolygons, MergedGDF], ignore_index=True)\n",
    "        # The polygons are too different, so we will deal with them later\n",
    "        else:\n",
    "            # Append larger area index\n",
    "            NaughtyCornerNew.append(groups[1].index.values[ix])\n",
    "            # Append the master index\n",
    "            NaughtyCornerMaster.append(groups[1]['index_right'].values[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of duplicates in the 'naughty corner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaughtyCornerMaster = list(set(NaughtyCornerMaster))\n",
    "NaughtyCornerMaster.sort()\n",
    "\n",
    "NaughtyCornerNew = list(set(NaughtyCornerNew))\n",
    "NaughtyCornerNew.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And make sure our merged polygons are merged properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnionMergedPolygons = MergedPolygons.unary_union\n",
    "# `Explode` the multipolygon back out into individual polygons\n",
    "UnionGDF = gp.GeoDataFrame(crs=MergedPolygons.crs, geometry=[UnionMergedPolygons])\n",
    "TidiedMergedPolygons = UnionGDF.explode()\n",
    "TidiedMergedPolygons['area']=TidiedMergedPolygons.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our new master polygon list and update the naughty corner\n",
    "\n",
    "The new master polygon set will be comprised of:\n",
    "- Master polygons NOT in the `MasterPolygonsToMerge` index list\n",
    "- New updated polygons from the `MergedPolygons` index list\n",
    "- New polygons identified, but not in master (not in `NaughtyCornerNew` or `PolygonsToKeep`)\n",
    "\n",
    "The new 'naughty corner' polygon set will be comprised of:\n",
    "- Master polygons in the `NaughtyCornerMaster` index list\n",
    "- New polygons in the `NaughtyCornerNew` index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MastersToKeep = CumulativeDams.loc[~CumulativeDams.index.isin(MasterPolygonsToMerge + NaughtyCornerMaster)]\n",
    "NewlyFoundDams = NewDamsNotContained.loc[~NewDamsNotContained.index.isin(NaughtyCornerNew + PolygonsToKeep)]\n",
    "\n",
    "NewMasterPolygons = pd.concat([MastersToKeep, TidiedMergedPolygons, NewlyFoundDams], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MastersToMove = CumulativeDams.loc[NaughtyCornerMaster]\n",
    "NewToMove = NewDamsNotContained.loc[NaughtyCornerNew]\n",
    "\n",
    "NaughtyCorner = pd.concat([MastersToMove, NewToMove], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the results to shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NewMasterPolygons.to_file(CumulativeDamShapeFileOutput)\n",
    "NaughtyCorner.to_file(NaughtyShapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
