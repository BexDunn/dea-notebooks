{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial display for Wetlands Insight Tool results <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatibility:** Notebook currently compatible with only the `NCI VDI` environment\n",
    "\n",
    "\n",
    "* **Special requirements:** \n",
    "    * If running on the [NCI](https://nci.org.au/), ensure that `module load dea` is   run prior to launching this notebook\n",
    "    * Check you have the latest version of the `wit_tooling package` by \n",
    "      copying and pasting the following code into a cell below and running the cell\n",
    "    `!pip install --user git+git://github.com/GeoscienceAustralia/wit_tooling`\n",
    "      \n",
    "      \n",
    "* **Products used:** \n",
    "    * Collection 2 Landsat Surface Reflectance: \n",
    "    [ls5_nbart_albers](https://explorer.dea.ga.gov.au/ls5_nbart_albers),\n",
    "    [ls7_nbart_albers](https://explorer.dea.ga.gov.au/ls7_nbart_albers),\n",
    "    [ls8_nbart_albers](https://explorer.dea.ga.gov.au/ls8_nbart_albers)\n",
    "    * Collection 2 Landsat Fractional Cover, \n",
    "    generated using the Joint Remote Sensing Research Program algorithm: \n",
    "    [ls5_fc_albers](https://explorer.dea.ga.gov.au/ls5_fc_albers),\n",
    "    [ls7_fc_albers](https://explorer.dea.ga.gov.au/ls7_fc_albers),\n",
    "    [ls8_fc_albers](https://explorer.dea.ga.gov.au/ls8_fc_albers)\n",
    "    * Water Observations from Space, \n",
    "    generated using the Geoscience Australia Algorithm:\n",
    "    [wofs_albers](https://explorer.sandbox.dea.ga.gov.au/wofs_albers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The Spatial Wetlands Insight Tool is a tool in development to display the coverage of water, \"wetness\" and vegetation fractional cover in a wetland spatially. It is generated off existing Wetlands Insight Tool temporal runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook uses an existing Wetlands Insight Tool temporal plot, \n",
    "generated from an existing WIT run, to create a spatial plot of water, \"wetness\", green/photosynthetic vegetation, dry/non-photosynthetic vegetation, and bare soil for a chosen observation date. \n",
    "\n",
    "1. First we load the existing WIT data from either: \n",
    "    * a saved csv location\n",
    "    * a shapefile to retrieve the existing WIT data from the database of previous runs\n",
    "    * a csv from an Amazon s3 data bucket\n",
    "2. Then we choose a time of interest to plot Spatial WIT\n",
    "3. Finally we output Spatial WIT to a file for each cover type\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements - A checklist to remind us if we tick all the boxes\n",
    "---------------------\n",
    "- [ ] Make a nice plot to select a time/ period of interest\n",
    "- [ ] Run WIT on a per-pixel basis\n",
    "- [ ] Return Water/Wet/FC percentage per pixel\n",
    "- [ ] Plot and output WIT spatially, with FC percentage represented as an alpha % for the colour\n",
    "- [ ] Output the results as a ArcGIS-compliant Geotiff (uint8), with the shapefile name and the date in the filename\n",
    "***\n",
    "\n",
    "Functions \n",
    "---------\n",
    "`bokeh wit plot`\n",
    "to do a stack plot of wit data with bokeh\n",
    "\n",
    "input: DataFrame\n",
    "\n",
    "output: stack plot of wit data\n",
    "\n",
    "`load_wit_data(**kwargs)` to load data\n",
    "\n",
    "input: csv file or poly_id in database\n",
    "\n",
    "output: DataFrame\n",
    "\n",
    "`load_wofs_fc(query)` to load the data \n",
    "\n",
    "input: a query dictionary with time and geometry\n",
    "\n",
    "output: an xarray with water/wet/FC percentage\n",
    "\n",
    "`plot_spatial_wit(input_pixels_array)` to plot spatially\n",
    "\n",
    "input: an xarray with water/wet/FC percentage\n",
    "\n",
    "output: 2 dimensional plot of input\n",
    "\n",
    "`write_geotiff(input_pixels_array, file_name)` to output to file\n",
    "\n",
    "input: an xarray with water/wet/FC percentage\n",
    "\n",
    "a string as file name\n",
    "\n",
    "output: a geotiff file with input file name\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook:\n",
    "-----------------------------\n",
    "* Follow the instructions under `Special Requirements` above to load `dea` and install `wit_tooling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/GeoscienceAustralia/wit_tooling\n",
      "  Cloning git://github.com/GeoscienceAustralia/wit_tooling to /local/r78/rjd547/tmp/pip-req-build-d22aws81\n",
      "  Running command git clone -q git://github.com/GeoscienceAustralia/wit_tooling /local/r78/rjd547/tmp/pip-req-build-d22aws81\n",
      "Requirement already satisfied (use --upgrade to upgrade): wit-tooling==2.1 from git+git://github.com/GeoscienceAustralia/wit_tooling in /home/547/rjd547/.digitalearthau/dea-env/20200612/local/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.16 in /g/data4/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages (from wit-tooling==2.1) (1.18.5)\n",
      "Requirement already satisfied: Cython>=0.23 in /g/data4/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages (from wit-tooling==2.1) (0.29.20)\n",
      "Building wheels for collected packages: wit-tooling\n",
      "  Building wheel for wit-tooling (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wit-tooling: filename=wit_tooling-2.1-cp36-cp36m-linux_x86_64.whl size=279898 sha256=4cf9f20803e2d1d0b1849d6d2aafaee6be4cc2aff018abb313d29c6be197b7fb\n",
      "  Stored in directory: /local/r78/rjd547/tmp/pip-ephem-wheel-cache-ze68tl3k/wheels/ba/1d/11/5c1b550730e34bc72a7ec1a07bb2ed5e160c3c918dc88ba1c2\n",
      "Successfully built wit-tooling\n"
     ]
    }
   ],
   "source": [
    "!pip install --user git+git://github.com/GeoscienceAustralia/wit_tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary packages in this cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import fiona\n",
    "import yaml\n",
    "from datacube import Datacube\n",
    "from datacube.utils.cog import write_cog\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from bokeh.io import curdoc, output_notebook, show\n",
    "from bokeh.layouts import layout\n",
    "from bokeh.models import (CheckboxGroup, Select, ColumnDataSource, HoverTool, YearsTicker, Legend,\n",
    "                          CustomJS, LegendItem, field, Range1d, Circle, Button, RadioGroup, TextInput, WheelZoomTool,\n",
    "                          ResetTool, BoxZoomTool, SaveTool, LinearColorMapper, Label)\n",
    "from bokeh.models.formatters import DatetimeTickFormatter\n",
    "from bokeh.models.glyphs import Text\n",
    "from bokeh.colors import RGB\n",
    "from bokeh.plotting import figure\n",
    "import datetime\n",
    "\n",
    "from datacube.virtual.impl import VirtualDatasetBox\n",
    "from datacube.virtual import construct\n",
    "from datacube.utils.geometry import CRS, Geometry\n",
    "from shapely.geometry import mapping, box\n",
    "from os import path\n",
    "import os, sys, urllib, logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "\n",
    "from wit_tooling import query_wit_data, load_timeslice, convert_shape_to_polygon, generate_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2221\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"2221\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"2221\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '2221' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"2221\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"2221\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"2221\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '2221' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"2221\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "_LOG = logging.getLogger(__name__)\n",
    "stdout_hdlr = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('[%(asctime)s.%(msecs)03d - %(levelname)s] %(message)s')\n",
    "stdout_hdlr.setFormatter(formatter)\n",
    "_LOG.addHandler(stdout_hdlr)\n",
    "_LOG.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "If you are using a shapefile, csv file, or Amazon s3 link to the existing WIT run, \n",
    "the path must be set in the cell below this cell:\n",
    "\n",
    "* `shapefile`: NCI path to shapefile \n",
    "(e.g. `'/g/data1a/r78/DEA_Wetlands/shapefiles/ramsar_wetlands_3577_20190403.shp'`). \n",
    "You must have permissions to the project directory,\n",
    "and the shapefile must be in [Australian Albers EPSG 3577 projection](https://spatialreference.org/ref/epsg/gda94-australian-albers/)\n",
    "* `csv_file`: NCI path to WIT results CSV (e.g. `'/g/data1a/u46/users/ea6141/dea-notebooks/Spatial_WIT/sample_data/Western Port_Western Port_VIC_19.csv'`)\n",
    "* `pd_yaml`: Yaml file necessary to generate WIT \n",
    "e.g. `'/g/data/u46/users/ea6141/wit_tooling/aux/fc_pd.yaml'`). \n",
    "Specifies input datasets.\n",
    "* `s3_url`: Amazon s3 url link to pre-generated WIT csvs folder \n",
    "(e.g. `'https://dea-public-data-dev.s3-ap-southeast-2.amazonaws.com/Wetlands_Insight_Tool/WIT_v3'`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put global variables in this cell\n",
    "shapefile = '/g/data/r78/DEA_Wetlands/MDBA_tmp/SEA_2018_survey_envelopes.shp'\n",
    "#csv_file = '/g/data1a/u46/users/ea6141/dea-notebooks/Spatial_WIT/sample_data/Western Port_Western Port_VIC_19.csv'\n",
    "pd_yaml = '/g/data/u46/users/ea6141/wit_tooling/aux/fc_pd.yaml'\n",
    "s3_url = 'http://dea-public-data-dev.s3-website-ap-southeast-2.amazonaws.com/?prefix=Wetlands_Insight_Tool/mdba/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used in this notebook to create, display and export Spatial WIT\n",
    "----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geotiff(spatial_wit_xr, filename, force=False):\n",
    "    \"\"\"\n",
    "        save spatial WIT result to geotiffs, each band will be output to individual tiff\n",
    "        input:\n",
    "            an xarray Dataset of spatial WIT\n",
    "            overwrite existing file if force=True, otherwise no output\n",
    "        output:\n",
    "            multiple cloud-optimized geotiffs (cogs) on disk\n",
    "    \"\"\"\n",
    "    for var in spatial_wit_xr.data_vars:       \n",
    "        #create file name per band\n",
    "        band_output = file_name + \"_\" + var + \".tif\"\n",
    "        if path.exists(band_output):\n",
    "            _LOG.warning(\"output geotif %s exists\", band_output)\n",
    "            if force:\n",
    "                _LOG.warning(\"existing geotif %s will be overwritten\", band_output)\n",
    "                os.remove(band_output)\n",
    "            else:\n",
    "                continue\n",
    "        spatial_wit_xr[var].attrs.update(spatial_wit_xr.attrs)\n",
    "        write_cog(spatial_wit_xr[var], band_output, blocksize=16)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(wit_df):\n",
    "    \"\"\"\n",
    "        Rename and reindex the input DataFrame\n",
    "        input: \n",
    "            loaded wit data as pandas DataFrame\n",
    "        output:\n",
    "            renamed and reindexed pandas DataFrame\n",
    "    \"\"\" \n",
    "    wit_df.index.name = 'no'\n",
    "    #Rename the columns so they are easier to understand and plot\n",
    "    wit_df = wit_df.rename(columns={\n",
    "                        \"TIME\": \"utc_time\",                    \n",
    "                        \"WATER\" : \"water\", \n",
    "                            \"WET\" : \"wet\",\n",
    "                           \"PV\" : \"green\",\n",
    "                           \"NPV\" : \"dry\",\n",
    "                           \"BS\" : \"bare\"}) \n",
    "    #converting to percentages to make plotting easier\n",
    "    #first convert if not already a percentage\n",
    "    if wit_df[wit_df.columns[1:]].max().max() <=1.0:\n",
    "        wit_df[wit_df.columns[1:]] = wit_df[wit_df.columns[1:]] * 100\n",
    "    #WITdata.head()\n",
    "    return wit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wit_data(**kwargs):\n",
    "    \"\"\"\n",
    "        Load pre-computed wit data from 3 different sources with the given parameter. Source is chosen by the key\n",
    "        in kwargs.\n",
    "        input parameters:\n",
    "            csv = csv_path: csv file path\n",
    "            shape = a shape from shape file\n",
    "            s3_url = url of s3 bucket: s3 bucket path\n",
    "        output:\n",
    "            pandas dataframe of wit data\n",
    "    \"\"\"\n",
    "    if kwargs.get(\"csv\") is not None:\n",
    "        wit_data = pd.read_csv(kwargs['csv'])\n",
    "    elif kwargs.get('shape') is not None:\n",
    "        _, wit_data = query_wit_data(kwargs['shape'])\n",
    "        wit_data = pd.DataFrame(data=wit_data, columns=['TIME', 'BS', 'NPV', 'PV', 'WET', 'WATER'])\n",
    "    elif kwargs.get('s3_url') is not None:\n",
    "        wit_data = pd.read_csv(kwargs['s3_url'], infer_datetime_format=True)\n",
    "    return wit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next three functions are used to load fc and wofs data with give geometry and time\n",
    "def construct_product(product_yaml):\n",
    "    \"\"\"\n",
    "        Construct a virtual product with the given yaml file\n",
    "        input:\n",
    "            product_yaml: the yaml file path\n",
    "        output:\n",
    "            virtual product instance\n",
    "    \"\"\"\n",
    "    with open(product_yaml, 'r') as f:\n",
    "        recipe = yaml.safe_load(f)\n",
    "    fc_product = construct(**recipe)\n",
    "    return fc_product\n",
    "\n",
    "def query_datasets(fc_product, shape, crs, time_range):\n",
    "    \"\"\"\n",
    "        Query the datasets in datacube database with the given shape and time period\n",
    "        input:\n",
    "            fc_product: virtual product instance\n",
    "            shape: a shape from shape file\n",
    "            crs: crs string from shape file\n",
    "            time_range: a tuple of (start_time, end_time)\n",
    "        output:\n",
    "            grouped datasets: VirtualDatasetBox\n",
    "    \"\"\"\n",
    "    dc = Datacube()\n",
    "    query_poly = convert_shape_to_polygon(shape['geometry'])\n",
    "    query_poly = Geometry(mapping(box(*query_poly.bounds)), CRS(crs))\n",
    "    query = {'geopolygon': query_poly, 'time': time_range}\n",
    "    datasets = fc_product.query(dc, **query)\n",
    "    grouped = fc_product.group(datasets, **query)\n",
    "    return grouped\n",
    "\n",
    "def load_wofs_fc(fc_product, grouped, time_slice):\n",
    "    \"\"\"\n",
    "        Load cloud free wofs, TCW and FC data with the given time or a tuple of (start_time, end_time)\n",
    "        input:\n",
    "            fc_product: virtual product instance\n",
    "            grouped: grouped datasets\n",
    "            time_slice: a single time or tuple of (start_time, end_time)\n",
    "        output:\n",
    "            wofs, TCW and FC data: xr.Dataset\n",
    "    \"\"\"\n",
    "    if not (isinstance(time_slice, list) or isinstance(time_slice, tuple)):\n",
    "         time_slice = [time_slice]\n",
    "    to_load = VirtualDatasetBox(grouped.box.loc[time_slice], grouped.geobox,\n",
    "                grouped.load_natively, grouped.product_definitions, grouped.geopolygon)\n",
    "    fc_wofs_data = load_timeslice(fc_product, to_load)\n",
    "    return fc_wofs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_wit(fc_wofs_data, mask):\n",
    "    \"\"\"\n",
    "        Compute spatial wit with wofs, TCW and FC data with the given polygon mask\n",
    "        input:\n",
    "            fc_wofs_data: wofs, TCW and FC data: xr.Dataset\n",
    "            mask: a polygon mask: np.array\n",
    "        output:\n",
    "            spatial wit results: xr.Dataset\n",
    "    \"\"\"\n",
    "    none_water_vars = list(fc_wofs_data.data_vars)[:-1]\n",
    "    water_var = list(fc_wofs_data.data_vars)[-1]\n",
    "    fc_data = fc_wofs_data[none_water_vars].where(fc_wofs_data[water_var] < 1)\n",
    "    tcw_percent = fc_data['TCW'] >= -350\n",
    "    fc_percent = fc_data.drop('TCW').where(~tcw_percent, 0)\n",
    "    sw_result = xr.merge([fc_percent, (tcw_percent.astype(\"int\") * 100),\n",
    "                             (fc_wofs_data[water_var].astype(\"int\") * 100)])\n",
    "    sw_result = sw_result.where(sw_result > 0, -127)\n",
    "    sw_result = sw_result.where(mask == int(shape['id']), -127).astype(\"int16\")\n",
    "    sw_result.attrs.update(fc_wofs_data.attrs)\n",
    "    for var in sw_result.data_vars:\n",
    "        sw_result[var].attrs['nodata'] = -127\n",
    "    return sw_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WIT data using one of the methods in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fiona.open(shapefile) as allshapes:\n",
    "    #print(next(iter(allshapes)))\n",
    "    testshape = next(iter(allshapes))\n",
    "    #print(testshape['properties'].get('Subwetno',0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of Wetlands for MDBA\n",
    "\n",
    "* Macquarie Marshes: Subwetno 26.01\n",
    "* Barmah: Subwetno 2\n",
    "* Gunbower Subwetno 13\n",
    "* Narran Subwetno 29\n",
    "* Gwydir Subwetno 14\n",
    "* Lowbidgee Subwetno 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 14, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-be46249324ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# #or load from s3 bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0ms3_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'area_percent_1_0.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms3_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3_filename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# or load from local csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-60afe7e69038>\u001b[0m in \u001b[0;36mload_wit_data\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mwit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwit_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TIME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NPV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WATER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3_url'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mwit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwit_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20200612/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 14, saw 2\n"
     ]
    }
   ],
   "source": [
    "# # todo: iterate over all possible loading routes,\n",
    "# # i.e., database and csv on s3 (not responsible for local csv, users supposed to know)\n",
    "# # load wit data from database with a chosen shape\n",
    "# with fiona.open(shapefile) as allshapes:\n",
    "#     shape_crs = allshapes.crs_wkt\n",
    "#     shape_list = iter(allshapes)  \n",
    "#     #print(shape_list)\n",
    "#     while True:\n",
    "#         shape = next(iter(shape_list)) #this change to deal with Fiona update\n",
    "#         if shape['properties'].get('Subwetno', 0) == 26.01:\n",
    "#             break\n",
    "#     polyName = shape['properties'].get('WetlandNam', '')\n",
    "#     poly_outline = np.array(shape['geometry']['coordinates'][0])\n",
    "#     wit_data = load_wit_data(shape=shape)\n",
    "\n",
    "# #or load from s3 bucket\n",
    "s3_filename = 'area_percent_1_0.csv'\n",
    "wit_data = load_wit_data(s3_url='/'.join([s3_url, s3_filename]))\n",
    "\n",
    "# or load from local csv\n",
    "# wit_data = load_wit_data(csv=csv_file)\n",
    "# wit_data = rename_columns(wit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial wit for the chosen shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:42,713.713 - DEBUG] Query datasets <VirtualDatasetBox of shape {'time': 569, 'y': 248, 'x': 220}>\n"
     ]
    }
   ],
   "source": [
    "# it's helpful to get the location of data rather than load them\n",
    "# and it will save you time without querying database multiple times\n",
    "time_range = (wit_data.utc_time.min(), wit_data.utc_time.max())\n",
    "#build a product for our data using the yaml file to specify which datasets we need\n",
    "fc_product = construct_product(pd_yaml)\n",
    "datasets = query_datasets(fc_product, shape, shape_crs, time_range)\n",
    "_LOG.debug(\"Query datasets %s\", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:42,727.727 - DEBUG] load time slice 1988-01-02T23:56:13.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/utils/geometry/_base.py:301: DeprecationWarning: Please use `str(crs)` instead of `crs.crs_str`\n",
      "  warnings.warn(\"Please use `str(crs)` instead of `crs.crs_str`\", category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# take first time slice to init spatial wit plot\n",
    "time_slice = np.datetime64(wit_data.utc_time[0])\n",
    "_LOG.debug(\"load time slice %s\", time_slice)\n",
    "fc_wofs_data = load_wofs_fc(fc_product, datasets, time_slice)\n",
    "# mask by the geometry of given polygon\n",
    "# first parameter of generate_raster is a tuple of (shape geometry, [integer of shape id])\n",
    "mask = generate_raster([(shape['geometry'], int(shape['id']))], datasets.geobox)\n",
    "fc_wofs_perc = spatial_wit(fc_wofs_data, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't wanna comment or change this section on this instance\n",
    "# don't review this section for any non functionality essential occation\n",
    "def plot_doc(doc):\n",
    "    source = ColumnDataSource(data=wit_data)\n",
    "    for i in range(10):\n",
    "        source.data['dummy'+str(i)] = np.ones(source.data['utc_time'].shape) * i * 10\n",
    "\n",
    "    #set up color palate for bokeh WIT plot\n",
    "    pal = [sns.xkcd_rgb[\"cobalt blue\"],\n",
    "           sns.xkcd_rgb[\"neon blue\"],\n",
    "           sns.xkcd_rgb[\"grass\"],\n",
    "           sns.xkcd_rgb[\"beige\"],\n",
    "           sns.xkcd_rgb[\"brown\"]]  \n",
    "\n",
    "    #lets put a title on the plot\n",
    "    title =f'Percentage of area dominated by WOfS, Wetness, Fractional Cover for {polyName}'    \n",
    "\n",
    "    #set up the x axis to recognise date and time. Note that you will only see the days when you zoom in.\n",
    "    wit_plot = figure(plot_width=1200, \n",
    "              plot_height = 400, \n",
    "              x_axis_type='datetime',\n",
    "             title=title, tools=[\"box_select\", ResetTool(), BoxZoomTool(dimensions=\"width\")])\n",
    "    \n",
    "    hovernames = []\n",
    "    nonselected_circle = Circle(fill_alpha=0, fill_color=None, line_color=None)\n",
    "    for i in range(10):\n",
    "        circle_group_name = 'circlegroup' + str(i)\n",
    "        renderer = wit_plot.circle(y=\"dummy\" + str(i), x= 'utc_time', size=20, fill_color=None, \n",
    "                        line_alpha=0, source = source, muted_color=\"white\", muted_alpha=0, name=circle_group_name)\n",
    "        renderer.nonselection_glyph = nonselected_circle\n",
    "        hovernames.append(circle_group_name)\n",
    "    wit_plot.varea_stack(['water', \n",
    "                  'wet',\n",
    "                  'green',\n",
    "                  'dry',\n",
    "                  'bare'], x= 'utc_time', name = \"stackplot\", color=pal, fill_alpha=0.7, source = source, \n",
    "                  legend_label=[\"water\",\"wet\",\"green\",\"dry\",\"bare\"], muted_color=\"grey\", muted_alpha=0.2)\n",
    "    wit_plot.sizing_mode = \"scale_width\"\n",
    "\n",
    "    #align the title in the centre\n",
    "    wit_plot.title.align= \"center\"\n",
    "    wit_plot.title.text_font_size=\"12pt\"\n",
    "\n",
    "    #label axes\n",
    "    wit_plot.yaxis.axis_label=(\"percentage of polygon classified as type\")\n",
    "    wit_plot.yaxis.axis_label_text_font_size=\"8pt\"\n",
    "\n",
    "    #we need screen units to put the attribution label under the plot. Don't ask why.\n",
    "    label_opts = dict(\n",
    "        x=0, \n",
    "        y=0,\n",
    "        x_units='screen', \n",
    "        y_units='screen',\n",
    "        text_font_style=\"italic\", \n",
    "        text_font_size=\"8.5pt\")\n",
    "\n",
    "    #underplot context\n",
    "    msg1 = 'The Fractional Cover algorithm developed by the Joint Remote Sensing Research Program\\n\\\n",
    "    and the Water Observations from Space algorithm developed by Geoscience Australia are used in the production of this data'\n",
    "    caption1 = Label(text=msg1, **label_opts)\n",
    "\n",
    "    wit_plot.add_layout(caption1, 'below')\n",
    "\n",
    "    wit_plot.xaxis.formatter=DatetimeTickFormatter()\n",
    "    wit_plot.xaxis.ticker = YearsTicker(interval=1)\n",
    "    wit_plot.xaxis.major_label_orientation = 45\n",
    "    \n",
    "    #set the new WIT graph ranges.\n",
    "    left, right, bottom, top = source.data['utc_time'][0], source.data['utc_time'][-1], 0, 100 #set \n",
    "    wit_plot.x_range=Range1d(left, right)\n",
    "    wit_plot.y_range=Range1d(bottom, top)\n",
    "    wit_plot.xaxis.bounds=(left,right)\n",
    "    wit_plot.yaxis.bounds=(bottom,top)\n",
    "\n",
    "    #now we want to overplot the data on the plot\n",
    "    #create rectangle borders for no-data times (SLC-off only)\n",
    "    LS5_8_gap_start = datetime.datetime(2011,11,1)\n",
    "    LS5_8_gap_end = datetime.datetime(2013,4,1)\n",
    "\n",
    "    #plot our dead satellite rectangle\n",
    "    wit_plot.hbar(y=50, \n",
    "           height=100,\n",
    "           left=LS5_8_gap_start, \n",
    "           right=LS5_8_gap_end, \n",
    "           name =\"LS7 SLC-OFF\",\n",
    "           color=\"white\", \n",
    "           alpha=0.5, \n",
    "           hatch_color=\"white\", \n",
    "           hatch_pattern='/',\n",
    "           hatch_alpha=0.6,\n",
    "           line_color=\"white\",\n",
    "           line_width =2,\n",
    "           line_alpha=0.6)\n",
    "\n",
    "    wit_plot.legend.location=\"bottom_left\"\n",
    "    wit_plot.legend.background_fill_alpha=0.5\n",
    "    wit_plot.legend.border_line_alpha=0.5\n",
    "    wit_plot.legend.label_text_font_size=\"9pt\" \n",
    "\n",
    "    #reverse the legend \n",
    "    wit_plot.legend[0].items.reverse()\n",
    "    \n",
    "    wit_hover = HoverTool(names = hovernames,\n",
    "                      tooltips = [\n",
    "        (\"observation\", \"$index\"),\n",
    "        (\"date\", \"@utc_time{%F}\"),\n",
    "        (\"bare\",\"@bare{0.0}%\"),\n",
    "        (\"dry\", \"@dry{0.0}%\"),\n",
    "        (\"green\",\"@green{0.0}%\"),\n",
    "        (\"wet\",\"@wet{0.0}%\"),\n",
    "        (\"water\",\"@water{0.0}%\")],     \n",
    "                    formatters=\n",
    "        {\"@utc_time\":\"datetime\"})\n",
    "\n",
    "    #trialling different ways of getting the tools to work. Both adding tools and including in the figure work.\n",
    "    wit_plot.add_tools(wit_hover, WheelZoomTool(), SaveTool())\n",
    "    \n",
    "    # down below is interactivity between plots\n",
    "    # WARNING: DON'T MODIFY UNLESS YOU ABSOLUTELY KNOW WHAT YOU'RE DOING\n",
    "    def sw_update(attrname, old, new):\n",
    "        time_slice = new\n",
    "        fc_wofs_data = load_wofs_fc(fc_product, datasets, np.datetime64(time_slice))\n",
    "        fc_wofs_perc = spatial_wit(fc_wofs_data, mask)\n",
    "        for var in fc_wofs_perc.data_vars:\n",
    "            sw_source.data[var] = [fc_wofs_perc[var].data[0]]\n",
    "        sw_plot.title.text = \"Spatial wit for time %s\" % time_slice\n",
    "\n",
    "    datasets_time_source = ColumnDataSource(data=dict(time=pd.to_datetime(datasets.box.time.data).astype('str')))\n",
    "    wit_time_source = ColumnDataSource(data=dict(time=wit_data.utc_time.astype('str')))\n",
    "    selected_time_slice = ColumnDataSource(data=dict(time=[]))\n",
    "    time_select = Select(title=\"time slice\", value='', options=list(wit_time_source.data['time']), \n",
    "                         height=50, width=200, sizing_mode=\"fixed\")\n",
    "    time_select.on_change('value', sw_update)\n",
    "    \n",
    "    def minmax_time_update(attrname, old, new):\n",
    "        if radio_group.active == 0:\n",
    "            time_source = wit_time_source.data['time']\n",
    "        elif radio_group.active == 1:\n",
    "            time_source = datasets_time_source.data['time']\n",
    "        if minmax_time_input.value != \"\":\n",
    "            min_time = minmax_time_input.value.split(\";\")[0]\n",
    "            max_time = minmax_time_input.value.split(\";\")[1]\n",
    "            time_source = time_source[(time_source >= min_time) & (time_source <= max_time)]\n",
    "        time_source = list(time_source)\n",
    "        time_select.options = time_source\n",
    "        time_select.value = time_source[0]\n",
    "        \n",
    "    minmax_time_input =  TextInput(value=\"\")\n",
    "    minmax_time_input.on_change('value', minmax_time_update)\n",
    "    \n",
    "    radio_group = RadioGroup(labels=[\"time slices from wit results\", \"time slices from available datasets\"], \n",
    "                             active=0, height=50, width=250, sizing_mode='fixed')\n",
    "    radio_group.on_change('active', minmax_time_update)\n",
    "    \n",
    "    js_code = \"\"\"\n",
    "        const inds=cb_obj.indices;\n",
    "        var min_index = inds[0];\n",
    "        var max_index = inds[0]\n",
    "        var data_s = source.data;\n",
    "        for (var i=0; i<inds.length; i++) {\n",
    "            max_index = Math.max(max_index, inds[i]);\n",
    "            min_index = Math.min(min_index, inds[i]);\n",
    "        }\n",
    "        target_minmax.value = data_s['time'][min_index] + \";\" + data_s['time'][max_index];\n",
    "    \"\"\"\n",
    "    js_callback = CustomJS(args={'source': wit_time_source, 'target_minmax': minmax_time_input}, \n",
    "                           code=js_code)\n",
    "    source.selected.js_on_change('indices', js_callback)\n",
    "    \n",
    "    def reset_time_selection(event):\n",
    "        if radio_group.active == 0:\n",
    "            options_input = wit_time_source.data['time']\n",
    "        elif radio_group.active == 1:\n",
    "            options_input = datasets_time_source.data['time']\n",
    "        time_select.options = list(options_input)\n",
    "        minmax_time_input.value = \"\"\n",
    "        \n",
    "    reset_button = Button(label='reset time selection', height=50, width=80, sizing_mode='fixed', button_type='success')\n",
    "    reset_button.on_click(reset_time_selection)\n",
    "    # above is interactivity between plots\n",
    "    \n",
    "    # down below is spatial wit plot\n",
    "    image_list = {}\n",
    "    color_map_dict = {}\n",
    "    # all below is to setup the pallete\n",
    "    transparent_white = RGB(255, 255, 255, 0)\n",
    "    colbat_blue = [RGB(3, 10, 167, 1)]\n",
    "    neon_blue = [RGB(4, 217, 255, 1)]\n",
    "    grass_green = [RGB(63, 155, 11, t) for t in np.arange(0.1, 1, 0.1)]\n",
    "    soil_brown = [RGB(96, 70, 15, t) for t in np.arange(0.1, 1, 0.1)]\n",
    "    dry_biege = [RGB(230, 218, 166, t) for t in np.arange(0.1, 1, 0.1) ]\n",
    "    var_colors = [soil_brown, grass_green, dry_biege, neon_blue, colbat_blue]\n",
    "    for var, cm in zip(fc_wofs_perc.data_vars, var_colors):\n",
    "        image_list[var] = [fc_wofs_perc[var].data[0]]\n",
    "        color_map_dict[var] = LinearColorMapper([transparent_white]+cm, low=0, high=100,\n",
    "                                   nan_color=transparent_white)\n",
    "    sw_source = ColumnDataSource(data=image_list)\n",
    "\n",
    "    # do the image plot\n",
    "    sw_plot = figure(plot_width=900, plot_height = 900,\n",
    "             tooltips = [\n",
    "        (\"x\", \"$x\"),\n",
    "        (\"y\", \"$y\"),\n",
    "        (\"bare\",\"@BS\"),\n",
    "        (\"dry\", \"@NPV\"),\n",
    "        (\"green\",\"@PV\"),\n",
    "        (\"wet\",\"@TCW\"),\n",
    "        (\"water\",\"@water\")])\n",
    "    \n",
    "\n",
    "    for var in fc_wofs_perc.data_vars:\n",
    "        sw_plot.image(image=var, source=sw_source, x=poly_outline[:, 0].min(), y=poly_outline[:, 1].max(),\n",
    "            dh=(poly_outline[:, 1].max() - poly_outline[:, 1].min()),\n",
    "            dw=(fc_wofs_data.x.data.max() - fc_wofs_data.x.data.min()),\n",
    "            color_mapper = color_map_dict[var])\n",
    "    sw_plot.line(poly_outline[:,0], poly_outline[:,1], line_width=3)\n",
    "    sw_plot.y_range.flipped = True\n",
    "    sw_plot.y_range.update(start=poly_outline[:,1].max(), end=poly_outline[:,1].min())\n",
    "    sw_plot.x_range.update(start=poly_outline[:,0].min(), end=poly_outline[:,0].max())\n",
    "    \n",
    "    sw_plot.title.text = \"Spatial wit at time %s\" % time_slice\n",
    "    sw_plot.title.text_font_size=\"12pt\"\n",
    "    # above is spatial wit plot\n",
    "    \n",
    "    layouts = layout([[radio_group, time_select, reset_button],\n",
    "        [wit_plot],[sw_plot]\n",
    "        ], sizing_mode='scale_both')\n",
    "    doc.add_root(layouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1557\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:34167/autoload.js?bokeh-autoload-element=1557&bokeh-absolute-url=http://localhost:34167&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "a20e93deddbf46138c14fe3728af3cbd"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n",
      "/g/data/v10/public/modules/dea/20200617/lib/python3.6/site-packages/datacube/virtual/utils.py:13: UserWarning: select_unique may have failed: {'crs': 'EPSG:3577', 'grid_mapping': 'spatial_ref'} is not the same as {'crs': 'EPSG:3577'}\n",
      "  .format(first, other))\n"
     ]
    }
   ],
   "source": [
    "show(plot_doc, notebook_url=\"http://localhost:8888\")\n",
    "#notebook_url is the server address of jupyter notebook, change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:45,108.108 - WARNING] output geotif test_1988-01-02T23:56:13.000000_BS.tif exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:output geotif test_1988-01-02T23:56:13.000000_BS.tif exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:45,114.114 - WARNING] output geotif test_1988-01-02T23:56:13.000000_PV.tif exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:output geotif test_1988-01-02T23:56:13.000000_PV.tif exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:45,117.117 - WARNING] output geotif test_1988-01-02T23:56:13.000000_NPV.tif exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:output geotif test_1988-01-02T23:56:13.000000_NPV.tif exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:45,121.121 - WARNING] output geotif test_1988-01-02T23:56:13.000000_TCW.tif exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:output geotif test_1988-01-02T23:56:13.000000_TCW.tif exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 10:22:45,123.123 - WARNING] output geotif test_1988-01-02T23:56:13.000000_water.tif exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:output geotif test_1988-01-02T23:56:13.000000_water.tif exists\n"
     ]
    }
   ],
   "source": [
    "# save spatial WIT as geotiff\n",
    "# each variable will be output to individual COG\n",
    "# file_name works as prefix, the final output file name will be \"file_name_bandname\", e.g. \"test_BS.tif\"\n",
    "# by default force=False := not overwriting existing geotif\n",
    "file_name = \"test_\" + str(time_slice)\n",
    "save_geotiff(fc_wofs_perc, file_name, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** June 16 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tags**: :index:`no_testing`,:index:`NCI compatible`,:index:`landsat 5`, :index:`landsat 7`,  :index:`landsat 8`, :index:`dea_plotting`, :index:`time series`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
