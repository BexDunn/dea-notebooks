{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, work through this notebook starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import rasterio.features\n",
    "from shapely.geometry import Polygon, shape, mapping\n",
    "from shapely.ops import unary_union\n",
    "import geopandas as gp\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import math\n",
    "import geohash as gh\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the functions for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def Generate_list_of_albers_tiles(TileFolder=\"TileFolder\", CustomData=True):\n",
    "    \"\"\"\n",
    "    Generate a list of Albers tiles to loop through for the water body analysis. This \n",
    "    function assumes that the list of tiles will be generated from a custom \n",
    "    datacube-stats run, and the file names will have the format\n",
    "    \n",
    "    */wofs_summary_8_-37_{date}.nc\n",
    "    \n",
    "    The tile number is expected in the 2nd and 3rd last positions when the string has been\n",
    "    broken using `_`. If this is not the case, then this code will not work, and will throw an error. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed in from elsewhere in the notebook. If this is not entered, the default parameter is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of Albers tiles across the analysis region. \n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print(\n",
    "            \"** ERROR ** \\n\"\n",
    "            \"You need to specify a folder of files for running a custom region\")\n",
    "        return\n",
    "\n",
    "    # Grab a list of all of the netCDF files in the tile folder\n",
    "    TileFiles = glob.glob(f\"{TileFolder}*.nc\")\n",
    "\n",
    "    CustomRegionAlbersTiles = set()\n",
    "    for filePath in TileFiles:\n",
    "        AlbersTiles = re.split(\"[_\\.]\", filePath)\n",
    "        if CustomData:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try:\n",
    "                int(AlbersTiles[-5])\n",
    "                int(AlbersTiles[-4])\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_summary_8_-37_{date}.nc\",\\n'\n",
    "                    \"with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_`. \\n\"\n",
    "                    \"Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f\"{AlbersTiles[-5]}_{AlbersTiles[-4]}\"\n",
    "        else:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try:\n",
    "                int(AlbersTiles[-5])\n",
    "                int(AlbersTiles[-4])\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_filtered_summary_8_-37.nc\",\\n'\n",
    "                    \"with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_` and `.`. \\n\"\n",
    "                    \"Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f\"{AlbersTiles[-5]}_{AlbersTiles[-4]}\"\n",
    "        CustomRegionAlbersTiles.add(ThisTile)\n",
    "    CustomRegionAlbersTiles = list(CustomRegionAlbersTiles)\n",
    "    return CustomRegionAlbersTiles\n",
    "\n",
    "\n",
    "def Generate_list_of_tile_datasets(ListofAlbersTiles,\n",
    "                                   Year,\n",
    "                                   TileFolder=\"TileFolder\",\n",
    "                                   CustomData=True):\n",
    "    \"\"\"\n",
    "    Generate a list of Albers tiles datasets to loop through for the water body analysis. Here, the \n",
    "    ListofAlbersTiles is used to generate a list of NetCDF files where the Albers coordinates have \n",
    "    been substituted into the naming file format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of albers tiles to loop through\n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    Year: int\n",
    "        Year for the analysis. This will correspond to the netCDF files for analysis.\n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed from elsewhere in the notebook. If this parameter is not entered, the default value\n",
    "        is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Alltilespaths: list\n",
    "        List of file paths to files to be analysed.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print(\n",
    "            \"** ERROR ** \\n\"\n",
    "            \"You need to specify a folder of files for running a custom region\")\n",
    "        raise\n",
    "\n",
    "    Alltilespaths = []\n",
    "\n",
    "    if CustomData:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            Tiles = glob.glob(f\"{TileFolder}*_{tile}_{Year}0101.nc\")\n",
    "            Alltilespaths.append(\n",
    "                Tiles[0])  # Assumes only one file will be returned\n",
    "    else:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            # Use glob to check that the file actually exists in the format we expect\n",
    "            Tiles = glob.glob(f\"{TileFolder}LS_TCW_PC_{tile}_19870101_20181025.nc\")\n",
    "            # Check that assumption by seeing if the returned list is empty\n",
    "            if not Tiles:\n",
    "                Tiles = glob.glob(f\"{TileFolder}WOFS_3577_{tile}_summary.nc\")\n",
    "            # Check that we actually have something now\n",
    "            if not Tiles:\n",
    "                print(\n",
    "                    \"** ERROR ** \\n\"\n",
    "                    \"An assumption in the file naming conventions has gone wrong somewhere.\\n\"\n",
    "                    \"We assume two file naming formats here: {TileFolder}wofs_filtered_summary_{tile}.nc, \\n\"\n",
    "                    \"and {TileFolder}WOFS_3577_{tile}_summary.nc. The files you have directed to don't meet \\n\"\n",
    "                    \"either assumption. Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.\"\n",
    "                )\n",
    "            Alltilespaths.append(\n",
    "                Tiles[0])  # Assumes only one file will be returned\n",
    "\n",
    "    return Alltilespaths\n",
    "\n",
    "\n",
    "def Filter_shapefile_by_intersection(gpdData,\n",
    "                                     gpdFilter,\n",
    "                                     filtertype=\"intersects\",\n",
    "                                     invertMask=True,\n",
    "                                     returnInverse=False):\n",
    "    \"\"\"\n",
    "    Filter out polygons that intersect with another polygon shapefile. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gpdData: geopandas dataframe\n",
    "        Polygon data that you wish to filter\n",
    "    gpdFilter: geopandas dataframe\n",
    "        Dataset you are using as a filter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    filtertype: default = 'intersects'\n",
    "        Options = ['intersects', 'contains', 'within']\n",
    "    invertMask: boolean\n",
    "        Default = 'True'. This determines whether you want areas that DO ( = 'False') or DON'T ( = 'True')\n",
    "        intersect with the filter shapefile.\n",
    "    returnInnverse: boolean\n",
    "        Default = 'False'. If true, then return both parts of the intersection - those that intersect AND \n",
    "        those that don't as two dataframes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gpdDataFiltered: geopandas dataframe\n",
    "        Filtered polygon set, with polygons that intersect with gpdFilter removed.\n",
    "    IntersectIndex: list of indices of gpdData that intersect with gpdFilter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    if 'returnInverse = True'\n",
    "    gpdDataFiltered, gpdDataInverse: two geopandas dataframes\n",
    "        Filtered polygon set, with polygons that DON'T intersect with gpdFilter removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the coordinate reference systems of both dataframes are the same\n",
    "\n",
    "    # assert gpdData.crs == gpdFilter.crs, 'Make sure the the coordinate reference systems of the two provided dataframes are the same'\n",
    "\n",
    "    Intersections = gp.sjoin(gpdFilter, gpdData, how=\"inner\", op=filtertype)\n",
    "\n",
    "    # Find the index of all the polygons that intersect with the filter\n",
    "    IntersectIndex = sorted(set(Intersections[\"index_right\"]))\n",
    "\n",
    "    # Grab only the polygons NOT in the IntersectIndex\n",
    "    # i.e. that don't intersect with a river\n",
    "    if invertMask:\n",
    "        gpdDataFiltered = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "    else:\n",
    "        gpdDataFiltered = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "\n",
    "    if returnInverse:\n",
    "        # We need to use the indices from IntersectIndex to find the inverse dataset, so we\n",
    "        # will just swap the '~'.\n",
    "\n",
    "        if invertMask:\n",
    "            gpdDataInverse = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "        else:\n",
    "            gpdDataInverse = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "\n",
    "        return gpdDataFiltered, IntersectIndex, gpdDataInverse\n",
    "    else:\n",
    "\n",
    "        return gpdDataFiltered, IntersectIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "The following section walks you through the analysis parameters you will need to set for this workflow. Each section describes the parameter, how it is used, and what value was used for the DEA Waterbodies product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtLeastThisWet = [-350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinSize = 3125  # 5 pixels\n",
    "MaxSize = 5000000000  # approx area of Lake Eyre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinimumValidObs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterOutRivers = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Tiles'></a>\n",
    "### Set up the input datasets for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllOfAustraliaAllTime = False\n",
    "\n",
    "CustomData = False\n",
    "AutoGenerateTileList = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CustomData:\n",
    "    # Path to the files you would like to use for the analysis\n",
    "    TileFolder = '/g/data/r78/cek156/datacube_stats/WOFSDamsAllTimeNSWMDB/'\n",
    "else:\n",
    "    # Default path to the WOfS summary product\n",
    "    TileFolder = 'ls_tcw_percentiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to generate the tile list if we are not doing all of Australia.\n",
    "if not AllOfAustraliaAllTime:\n",
    "    if AutoGenerateTileList:\n",
    "        ListofAlbersTiles = Generate_list_of_albers_tiles(\n",
    "            TileFolder, CustomData)\n",
    "    else:\n",
    "        # Provide you own list of tiles to be run\n",
    "        ListofAlbersTiles = [\n",
    "            '11_-40'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the first temporary polygon dataset\n",
    "\n",
    "This code section:\n",
    "\n",
    "1. Checks that the `AtLeastThisWet` threshold has been correctly entered above\n",
    "2. Sets up a `for` loop that allows the user to input multiple temporal datasets (see below)\n",
    "3. Generates a list of netCDF files to loop through\n",
    "4. Sets up a `for` loop for that list of files. Here we have separate data for each Landsat tile, so this loop loops through the list of tile files\n",
    "5. Opens the netCDF `frequency` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "6. Opens the netCDF `count_clear` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "7. Removes any pixels not observed at least [`MinimumValidObs` times](#valid)\n",
    "8. Sets up a `for` loop for the entered [`AtLeastThisWet` thresholds](#wetnessThreshold)\n",
    "9. Masks out any data that does not meet the wetness threshold\n",
    "10. Converts the data to a Boolean array, with included pixels == 1\n",
    "11. Converts the raster array to a polygon dataset\n",
    "12. Cleans up the polygon dataset\n",
    "13. Resets the `geometry` to a shapely geometry\n",
    "14. Merges any overlapping polygons\n",
    "15. Convert the output of the merging back into a geopandas dataframe\n",
    "16. Calculates the area of each polygon\n",
    "17. Saves the results to a shapefile\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **WaterBodiesShp:** The name and filepath of the intermediate output polygon set\n",
    "- **WOFSshpMerged:** The filepath for the location of temp files during the code run\n",
    "- **WOFSshpFiltered:** The name and filepath of the outputs following the [filtering steps](#Filtering)\n",
    "- **FinalName:** The name and file path of the final, completed waterbodies shapefile\n",
    "- **years to analyse:** `for year in range(x,y)` - note that the last year is NOT included in the analysis. This for loop is set up to allow you to loop through multiple datasets to create multiple polygon outputs. If you only have one input dataset, set this to `range(<year of the analysis>, <year of the analysis + 1>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up some file names for the inputs and outputs\n",
    "# The name and filepath of the intermediate output polygon set\n",
    "WaterBodiesShp = f'output/temp/'\n",
    "\n",
    "# The name and filepath of the temp, filtered output polygon set\n",
    "WOFSshpMerged = f'output/'\n",
    "WOFSshpFiltered = 'output/AusWaterBodiesFiltered.shp'\n",
    "\n",
    "# Final shapefile name\n",
    "FinalName = 'output/AusWaterBodies.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ls_tcw_percentiles/LS_TCW_PC_11_-40_19870101_20181025.nc']\n"
     ]
    }
   ],
   "source": [
    "# Now perform the analysis to generate the first iteration of polygons\n",
    "for year in range(1980, 1981):\n",
    "\n",
    "    ### Get the list of netcdf file names to loop through\n",
    "    if AllOfAustraliaAllTime:\n",
    "        # Grab everything from the published WOfS all time summaries\n",
    "        Alltiles = glob.glob(f'{TileFolder}*.nc')\n",
    "    else:\n",
    "        Alltiles = Generate_list_of_tile_datasets(ListofAlbersTiles, year,\n",
    "                                                  TileFolder, CustomData)\n",
    "        print(Alltiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not set up the hybrid threshold option. If you meant to use this option, please \n",
      "set this option by including two wetness thresholds in the `AtLeastThisWet` variable above. \n",
      "The wetness threshold we will use is [-350].\n",
      "\n",
      "Alltiles ['ls_tcw_percentiles/LS_TCW_PC_11_-40_19870101_20181025.nc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              625.0\n",
      "1             2500.0\n",
      "2             1250.0\n",
      "3             1250.0\n",
      "4             1250.0\n",
      "            ...     \n",
      "24265         1250.0\n",
      "24266        12500.0\n",
      "24267          625.0\n",
      "24268    237727500.0\n",
      "24269       354375.0\n",
      "Name: area, Length: 24270, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First, test whether the wetness threshold has been correctly set\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    print(\n",
    "        f'We will be running a hybrid wetness threshold. Please ensure that the major threshold is \\n'\n",
    "        f'listed second, with the supplementary threshold entered first.'\n",
    "        f'**You have set {AtLeastThisWet[-1]} as the primary threshold,** \\n'\n",
    "        f'**with {AtLeastThisWet[0]} set as the supplementary threshold.**')\n",
    "elif len(AtLeastThisWet) == 1:\n",
    "    print(\n",
    "        f'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        f'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above. \\n'\n",
    "        f'The wetness threshold we will use is {AtLeastThisWet}.')\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f'There is something wrong with your entered wetness threshold. Please enter a list \\n'\n",
    "        f'of either one or two numbers. You have entered {AtLeastThisWet}. \\n'\n",
    "        f'See above for more information')\n",
    "print()\n",
    "# Now perform the analysis to generate the first iteration of polygons\n",
    "for year in range(1980, 1981):\n",
    "\n",
    "    ### Get the list of netcdf file names to loop through\n",
    "    if AllOfAustraliaAllTime:\n",
    "        # Grab everything from the published WOfS all time summaries\n",
    "        Alltiles = glob.glob(f'{TileFolder}*.nc')\n",
    "    else:\n",
    "        Alltiles = Generate_list_of_tile_datasets(ListofAlbersTiles, year,\n",
    "                                                  TileFolder, CustomData)\n",
    "        print('Alltiles',Alltiles)\n",
    "\n",
    "    for WOFSfile in Alltiles:\n",
    "        try:\n",
    "            # Read in the data\n",
    "            # Note that the netCDF files we are using here contain a variable called 'frequency',\n",
    "            # which is what we are using to define our water polygons.\n",
    "            # If you use a different netCDF input source, you may need to change this variable name here\n",
    "            WOFSnetCDFData = xr.open_rasterio(f'NETCDF:{WOFSfile}:TCW_PC_90')\n",
    "            # Remove the superfluous time dimension\n",
    "            WOFSnetCDFData = WOFSnetCDFData.squeeze()\n",
    "\n",
    "#             # Open the clear count variable to generate the minimum observation mask\n",
    "#             # If you use a different netCDF input source, you may need to change this variable name here\n",
    "#             WOFSvalidcount = xr.open_rasterio(f'NETCDF:{WOFSfile}:count_clear')\n",
    "#             WOFSvalidcount = WOFSvalidcount.squeeze()\n",
    "\n",
    "#             # Filter our WOfS classified data layer to remove noise\n",
    "#             # Remove any pixels not abserved at least MinimumValidObs times\n",
    "#             WOFSValidFiltered = WOFSvalidcount >= MinimumValidObs\n",
    "\n",
    "            for Thresholds in AtLeastThisWet:\n",
    "                # Remove any pixels that are wet < AtLeastThisWet% of the time\n",
    "                WOFSfiltered = WOFSnetCDFData > Thresholds\n",
    "\n",
    "                # Now find pixels that meet both the MinimumValidObs and AtLeastThisWet criteria\n",
    "                # Change all zeros to NaN to create a nan/1 mask layer\n",
    "                # Pixels == 1 now represent our water bodies\n",
    "                WOFSfiltered = WOFSfiltered.where((WOFSfiltered != 0)) \n",
    "#                                                   &\n",
    "#                                                   (WOFSValidFiltered != 0))\n",
    "\n",
    "                # Convert the raster to polygons\n",
    "                # We use a mask of '1' to only generate polygons around values of '1' (not NaNs)\n",
    "                WOFSpolygons = rasterio.features.shapes(\n",
    "                    WOFSfiltered.data.astype('float32'),\n",
    "                    mask=WOFSfiltered.data.astype('float32') == 1,\n",
    "                    transform=WOFSnetCDFData.transform)\n",
    "                # The rasterio.features.shapes returns a tuple. We only want to keep the geometry portion,\n",
    "                # not the value of each polygon (which here is just 1 for everything)\n",
    "                WOFSbreaktuple = (a for a, b in WOFSpolygons)\n",
    "\n",
    "                # Put our polygons into a geopandas geodataframe\n",
    "                PolygonGP = gp.GeoDataFrame(list(WOFSbreaktuple))\n",
    "\n",
    "                # Grab the geometries and convert into a shapely geometry\n",
    "                # so we can quickly calcuate the area of each polygon\n",
    "#                 PolygonGP['geometry'] = None\n",
    "#                 for ix, poly in PolygonGP.iterrows():\n",
    "#                     poly['geometry'] = shape(poly)\n",
    "                PolygonGP['geometry'] = None\n",
    "                for i, row in PolygonGP.iterrows():\n",
    "                    PolygonGP.at[i,'geometry'] = shape(row)\n",
    "\n",
    "                # Set the geometry of the dataframe to be the shapely geometry we just created\n",
    "                PolygonGP = PolygonGP.set_geometry('geometry')\n",
    "                # We need to add the crs back onto the dataframe\n",
    "                PolygonGP.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "#                 # Combine any overlapping polygons\n",
    "#                 MergedPolygonsGeoms = unary_union(PolygonGP['geometry'])\n",
    "\n",
    "#                 # Turn the combined multipolygon back into a geodataframe\n",
    "#                 MergedPolygonsGPD = gp.GeoDataFrame(\n",
    "#                     [poly for poly in MergedPolygonsGeoms])\n",
    "#                 # Rename the geometry column\n",
    "#                 MergedPolygonsGPD.columns = ['geometry']\n",
    "#                 # We need to add the crs back onto the dataframe\n",
    "#                 MergedPolygonsGPD.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "                MergedPolygonsGPD = PolygonGP\n",
    "    \n",
    "                # Calculate the area of each polygon again now that overlapping polygons\n",
    "                # have been merged\n",
    "                MergedPolygonsGPD['area'] = MergedPolygonsGPD['geometry'].area\n",
    "                print(MergedPolygonsGPD['area'])\n",
    "\n",
    "                # Save the polygons to a shapefile\n",
    "                schema = {\n",
    "                    'geometry': 'Polygon',\n",
    "                    'properties': {\n",
    "                        'area': 'float'\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Generate our dynamic filename\n",
    "                FileName = f'{WaterBodiesShp}_{Thresholds}.shp'\n",
    "                # Append the file name to the list so we can call it later on\n",
    "\n",
    "                if os.path.isfile(FileName):\n",
    "                    with fiona.open(FileName,\n",
    "                                    \"a\",\n",
    "                                    crs=from_epsg(3577),\n",
    "                                    driver='ESRI Shapefile',\n",
    "                                    schema=schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': poly['area']\n",
    "                                },\n",
    "                                'geometry': mapping(shape(poly['geometry']))\n",
    "                            }))\n",
    "                else:\n",
    "                    with fiona.open(FileName,\n",
    "                                    \"w\",\n",
    "                                    crs=from_epsg(3577),\n",
    "                                    driver='ESRI Shapefile',\n",
    "                                    schema=schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                            output.write(({\n",
    "                                'properties': {\n",
    "                                    'area': poly['area']\n",
    "                                },\n",
    "                                'geometry': mapping(shape(poly['geometry']))\n",
    "                            }))\n",
    "\n",
    "        except:\n",
    "            print(\n",
    "                f'{WOFSfile} did not run. \\n'\n",
    "                f'This is probably because there are no waterbodies present in this tile.'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MergeTiles'></a>\n",
    "\n",
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries. \n",
    "\n",
    "We have created a shapefile that consists of the albers tile boundaries, plus a 1 pixel (25 m) buffer. This shapefile will help us to find any polygons that have a boundary at the edge of an albers tile. We can then find where polygons touch across this boundary, and join them up.\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **AlbersBuffer:** The file location of a shapefile that is a 1 pixel buffer around the Albers tile boundaries\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AlbersBuffer = gp.read_file('/g/data/r78/cek156/ShapeFiles/AlbersBuffer25m.shp')\n",
    "\n",
    "for Threshold in AtLeastThisWet:\n",
    "    print(f'Working on {Threshold} shapefile')\n",
    "    # We are using the more severe wetness threshold as the main polygon dataset.\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the higher threshold listed second.\n",
    "    WaterPolygons = gp.read_file(f'{WaterBodiesShp}_{Threshold}.shp')\n",
    "\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes, NotBoundaryDams = Filter_shapefile_by_intersection(\n",
    "        WaterPolygons, AlbersBuffer, invertMask=False, returnInverse=True)\n",
    "\n",
    "    # Now combine overlapping polygons in `BoundaryDams`\n",
    "    UnionBoundaryDams = BoundaryMergedDams.unary_union\n",
    "\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=WaterPolygons.crs,\n",
    "                               geometry=[UnionBoundaryDams])\n",
    "    MergedDams = UnionGDF.explode()\n",
    "\n",
    "    # Then combine our new merged polygons with the `NotBoundaryDams`\n",
    "    # Combine New merged polygons with the remaining polygons that are not near the tile boundary\n",
    "    AllTogether = gp.GeoDataFrame(\n",
    "        pd.concat([NotBoundaryDams, MergedDams], ignore_index=True,\n",
    "                  sort=True)).set_geometry('geometry')\n",
    "\n",
    "    # Calculate the area of each polygon\n",
    "    AllTogether['area'] = AllTogether.area\n",
    "\n",
    "    # Check for nans\n",
    "    AllTogether.dropna(inplace=True)\n",
    "\n",
    "    schema = {'geometry': 'Polygon', 'properties': {'area': 'float'}}\n",
    "\n",
    "    print(f'Writing out {Threshold} shapefile')\n",
    "\n",
    "    with fiona.open(f'{WOFSshpMerged}Union_{Threshold}.shp',\n",
    "                    \"w\",\n",
    "                    crs=from_epsg(3577),\n",
    "                    driver='ESRI Shapefile',\n",
    "                    schema=schema) as output:\n",
    "        for ix, poly in AllTogether.iterrows():\n",
    "            output.write(({\n",
    "                'properties': {\n",
    "                    'area': poly['area']\n",
    "                },\n",
    "                'geometry': mapping(shape(poly['geometry']))\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Filtering'></a>\n",
    "\n",
    "## Filter the merged polygons by:\n",
    "- **Area:**\n",
    "Based on the `MinSize` and `MaxSize` parameters set [here](#size).\n",
    "- **Coastline:**\n",
    "Using the `Coastline` dataset loaded [here](#coastline).\n",
    "- **CBD location (optional):**\n",
    "Using the `CBDs` dataset loaded [here](#Urban).\n",
    "- **Wetness thresholds:**\n",
    "Here we apply the hybrid threshold described [here](#wetness)\n",
    "- **Intersection with rivers (optional):**\n",
    "Using the `MajorRivers` dataset loaded [here](#rivers)\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not set up the hybrid threshold option. If you meant to use this option, please \n",
      "set this option by including two wetness thresholds in the `AtLeastThisWet` variable above\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    AllTogether = gp.read_file(f'{WOFSshpMerged}Temp_{AtLeastThisWet[1]}.shp')\n",
    "except IndexError:\n",
    "    AllTogether = gp.read_file(f'{WOFSshpMerged}Temp_{AtLeastThisWet[0]}.shp')\n",
    "AllTogether['area'] = pd.to_numeric(AllTogether.area)\n",
    "\n",
    "# Filter out any polygons smaller than MinSize, and greater than MaxSize\n",
    "WaterBodiesBig = AllTogether.loc[((AllTogether['area'] > MinSize) &\n",
    "                                  (AllTogether['area'] <= MaxSize))]\n",
    "\n",
    "# Filter out any ocean in the pixel\n",
    "WaterBodiesLand, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "    WaterBodiesBig, Coastline, invertMask=True)\n",
    "\n",
    "# WOfS has a known bug where deep shadows from high-rise CBD buildings are misclassified\n",
    "# as water. We will use the ABS sa3 dataset to filter out Brisbane, Gold Coast, Sydney,\n",
    "# Melbourne, Adelaide and Perth CBDs.\n",
    "# If you have chosen to set UrbanMask = False, this step will be skipped.\n",
    "if UrbanMask:\n",
    "    NotCities, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        WaterBodiesLand, CBDs)\n",
    "else:\n",
    "    print(\n",
    "        'You have chosen not to filter out waterbodies within CBDs. If you meant to use this option, please \\n'\n",
    "        'set `UrbanMask = True` variable above, and set the path to your urban filter shapefile'\n",
    "    )\n",
    "    NotCities = WaterBodiesLand\n",
    "\n",
    "# Check for hybrid wetness thresholds\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the supplementary threshold listed first.\n",
    "    LowerThreshold = gp.read_file(\n",
    "        f'{WOFSshpMerged}Union_{AtLeastThisWet[0]}.shp')\n",
    "    LowerThreshold['area'] = pd.to_numeric(LowerThreshold.area)\n",
    "    # Filter out those pesky huge polygons\n",
    "    LowerThreshold = LowerThreshold.loc[(LowerThreshold['area'] <= MaxSize)]\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        LowerThreshold, NotCities)\n",
    "    # Pull out the polygons from the supplementary shapefile that intersect with the primary shapefile\n",
    "    LowerThresholdToUse = LowerThreshold.loc[LowerThreshold.index.isin(\n",
    "        IntersectIndexes)]\n",
    "    # Concat the two polygon sets together\n",
    "    CombinedPolygons = gp.GeoDataFrame(\n",
    "        pd.concat([LowerThresholdToUse, NotCities], ignore_index=True))\n",
    "    # Merge overlapping polygons\n",
    "    CombinedPolygonsUnion = CombinedPolygons.unary_union\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=LowerThreshold.crs,\n",
    "                               geometry=[CombinedPolygonsUnion])\n",
    "    HybridDams = UnionGDF.explode()\n",
    "else:\n",
    "    print(\n",
    "        'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "        'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above'\n",
    "    )\n",
    "    HybridDams = NotCities\n",
    "\n",
    "# Here is where we do the river filtering (if FilterOutRivers == True)\n",
    "if FilterOutRivers:\n",
    "    WaterBodiesBigRiverFiltered, IntersectIndexes = Filter_shapefile_by_intersection(\n",
    "        HybridDams, MajorRivers)\n",
    "else:\n",
    "    # If river filtering is turned off, then we just keep all the same polygons\n",
    "    WaterBodiesBigRiverFiltered = HybridDams\n",
    "\n",
    "# We need to add the crs back onto the dataframe\n",
    "WaterBodiesBigRiverFiltered.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "# Calculate the area and perimeter of each polygon again now that overlapping polygons\n",
    "# have been merged\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].length\n",
    "\n",
    "# Calculate the Polsby-Popper value (see below), and write out too\n",
    "WaterBodiesBigRiverFiltered['PPtest'] = (\n",
    "    (WaterBodiesBigRiverFiltered['area'] * 4 * math.pi) /\n",
    "    (WaterBodiesBigRiverFiltered['perimeter']**2))\n",
    "\n",
    "# Save the polygons to a shapefile\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {\n",
    "        'area': 'float',\n",
    "        'perimeter': 'float',\n",
    "        'PPtest': 'float'\n",
    "    }\n",
    "}\n",
    "\n",
    "with fiona.open(WOFSshpFiltered,\n",
    "                \"w\",\n",
    "                crs=from_epsg(3577),\n",
    "                driver='ESRI Shapefile',\n",
    "                schema=schema) as output:\n",
    "    for ix, poly in WaterBodiesBigRiverFiltered.iterrows():\n",
    "        output.write(({\n",
    "            'properties': {\n",
    "                'area': poly['area'],\n",
    "                'perimeter': poly['perimeter'],\n",
    "                'PPtest': poly['PPtest']\n",
    "            },\n",
    "            'geometry': mapping(shape(poly['geometry']))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dividing up very large polygons\n",
    "\n",
    "The size of polygons is determined by the contiguity of waterbody pixels through the landscape. This can result in very large polygons, e.g. where rivers are wide and unobscured by trees, or where waterbodies are connected to rivers or neighbouring waterbodies. The image below shows this for the Menindee Lakes, NSW. The relatively flat terrain in this part of Australia means that the 0.05 wetness threshold results in the connection of a large stretch of river and the individual lakes into a single large polygon that spans 154 km. This polygon is too large to provide useful insights into the changing water surface area of the Menindee Lakes, and needs to be broken into smaller, more useful polygons.\n",
    "\n",
    "![Menindee Lakes original polygon](DocumentationFigures/menindeeLakes.JPG)\n",
    "\n",
    "We do this by applying the [Polsby-Popper test (1991)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2936284). The Polsby-Popper test is an assessment of the 'compactness' of a polygon. This method was originally developed to test the shape of congressional and state legislative districts, to prevent gerrymandering. \n",
    "\n",
    "The Polsby-Popper test examines the ratio between the area of a polygon, and the area of a circle equal to the perimeter of that polygon. The result falls between 0 and 1, with values closer to 1 being assessed as more compact.\n",
    "\n",
    "\\begin{align*}\n",
    "PPtest = \\frac{polygon\\ area * 4\\pi}{polygon\\ perimeter^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The Menindee Lakes polygon above has a PPtest value $\\approx$ 0.00. \n",
    "\n",
    "We selected all polygons with a `PPtest` value <=0.005. This resulted in a subset of 186 polygons. \n",
    "\n",
    "![Polygons with a Polsby-Popper test score of less than 0.005](DocumentationFigures/PPtestlessthan005.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 186 polygons were buffered with a -50 meter (2 pixel) buffer to separate the polygons where they are connected bu two pixels or less. This allows us to split up these very large polygons by using natural thinning points. The resulting negatively buffered polygons was run through the `multipart to singlepart` tool in QGIS, to give the now separated polygons unique IDs. \n",
    "\n",
    "These polygons were then buffered with a +50 meter buffer to return the polygons to approximately their original size. These final polygons were used to separate the 186 original polygons identified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for dividing up the identified very large polygons varied depending on the polygon in question. Where large waterbodies (like the Menindee Lakes) were connected, the buffered polygons were used to determine the cut points in the original polygons. Where additional breaks were required, the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/) `waterbodies` dataset was used as an additional source of information for breaking up connected segments.\n",
    "\n",
    "The buffering method didn't work on large segments of river, which became a series of disconnected pieces when negatively and positively buffered. Instead, we used a combination of tributaries and man-made features such as bridges and weirs to segment these river sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks and recalculation of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaterBodiesBigRiverFiltered = gp.read_file(WOFSshpFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the area and perimeter of each polygon again following the manual checking\n",
    "# step performed above\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered[\n",
    "    'geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the PPtest column, since we don't really want this as an attribute of the final shapefile\n",
    "WaterBodiesBigRiverFiltered.drop(labels='PPtest', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the size filtering, just to check that all of the split and filtered waterbodies are\n",
    "# still in the size range we want\n",
    "DoubleCheckArea = WaterBodiesBigRiverFiltered.loc[(\n",
    "    (WaterBodiesBigRiverFiltered['area'] > MinSize) &\n",
    "    (WaterBodiesBigRiverFiltered['area'] <= MaxSize))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 9` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert from Albers coordinates to lat/lon, in order to generate the geohash\n",
    "GetUniqueID = DoubleCheckArea.to_crs(epsg=4326)\n",
    "\n",
    "# Generate a geohash for the centroid of each polygon\n",
    "GetUniqueID['UID'] = GetUniqueID.apply(lambda x: gh.encode(\n",
    "    x.geometry.centroid.y, x.geometry.centroid.x, precision=9),\n",
    "                                       axis=1)\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert GetUniqueID['UID'].is_unique\n",
    "\n",
    "# Make an arbitrary numerical ID for each polygon. We will first sort the dataframe by geohash\n",
    "# so that polygons close to each other are numbered similarly\n",
    "SortedData = GetUniqueID.sort_values(by=['UID']).reset_index()\n",
    "SortedData['WB_ID'] = SortedData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The step above creates an 'index' column, which we don't actually want, so drop it.\n",
    "SortedData.drop(labels='index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the final results to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackToAlbers = SortedData.to_crs(epsg=3577)\n",
    "BackToAlbers.to_file(FinalName, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some extra curation\n",
    "\n",
    "Following the development of timeseries for each individual polygon, it was determined that a number of polygons do not produce complete timeseries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting polygons that cross swath boundaries\n",
    "\n",
    "Three large polygons were identified that straddle Landsat swath boundaries. This is problematic, as the whole polygon will never be observed on a single day, which trips the requirement for at least 90% of a polygon to be observed in order for an observation to be valid. \n",
    "\n",
    "There are two options for dealing with this issue:\n",
    "- Splitting the polygons using the swath boundaries, so that each half of the polygon will be observed in a single day. This will retain information as to the exact timing of observations. \n",
    "- Creating time averaged timeseries, which would group observations into monthly blocks and provide a value for each month. This would provide information for the whole polygon, but would lose the specific timing information. \n",
    "\n",
    "We chose to go with the first option to keep the high fidelity timing information for each polygon. Three polygons were split using the swath boundaries as a guide. The split polygons were given a new `WB_ID`, and a new geohash was calculated for each new polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaterBodiesSplit = gp.read_file(\n",
    "    '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesSplitEliminate.shp'\n",
    ")\n",
    "\n",
    "# We need to convert from Albers coordinates to lat/lon, in order to generate the geohash\n",
    "GetUniqueID = WaterBodiesSplit.to_crs(epsg=4326)\n",
    "\n",
    "# Only recalculate the geohash for the polygons that have changed:\n",
    "ChangedWB_ID = [145126, 66034, 146567, 295902, 295903, 295904, 295905]\n",
    "\n",
    "for ix, rowz in GetUniqueID.iterrows():\n",
    "    if rowz['WB_ID'] in ChangedWB_ID:\n",
    "        # Generate a geohash for the centroid of each polygon\n",
    "        GetUniqueID.loc[ix, 'WB_ID'] = gh.encode(\n",
    "            GetUniqueID.iloc[ix].geometry.centroid.y,\n",
    "            GetUniqueID.iloc[ix].geometry.centroid.x,\n",
    "            precision=9)\n",
    "        print('Changing geohash')\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert GetUniqueID['UID'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final version of the polygons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackToAlbers = GetUniqueID.to_crs(epsg=3577)\n",
    "BackToAlbers.to_file(\n",
    "    '/g/data/r78/cek156/dea-notebooks/DEAWaterbodies/AusAllTime01-005HybridWaterbodies/AusWaterBodiesFINAL.shp',\n",
    "    driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** December 2019. Peer Code Quality Check Performed, March 2019\n",
    "\n",
    "**Compatible datacube version:** A full list of python packages used to produce DEA Waterbodies is available [here](TurnWaterObservationsIntoWaterbodyPolygons.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Tags: :index:`fiona`, :index:`geopandas`, :index:`Geotiff`, :index:`masking`, :index:`rasterio`, :index:`shapefile`, :index:`WOfS`, :index:`WOFL`, :index:`shapely`, :index:`raster to polygons`, :index:`polygons`, :index:`vectorise`, :index:`no_testing`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
