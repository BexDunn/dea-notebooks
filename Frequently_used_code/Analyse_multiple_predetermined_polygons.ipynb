{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and run analysis on multiple polygons <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with both the `NCI` and `DEA Sandbox` environments\n",
    "* **Products used:** \n",
    "[ga_ls8c_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls8c_ard_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Many users need to run analyses on their own areas of interest. \n",
    "A common use case involves running the same analysis across multiple polygons in a vector file (e.g. ESRI Shapefile or GeoJSON). \n",
    "This notebook will demonstrate how to use a vector file and the Open Data Cube to extract satellite data from Digital Earth Australia corresponding to individual polygon geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "If we have a vector file containing multiple polygons, we can use the python package [geopandas](https://geopandas.org/) to open it as a `GeoDataFrame`. \n",
    "We can then iterate through each geometry and extract satellite data corresponding with the extent of each geometry. \n",
    "Further anlaysis can then be conducted on each resulting `xarray.Dataset`.\n",
    "\n",
    "We can retrieve data for each polygon, perform an analysis like calculating NDVI and plot the data.\n",
    "\n",
    "1. First we open the vector file as a `geopandas.GeoDataFrame`\n",
    "2. Iterate through each polygon in the `GeoDataFrame`, and extract satellite data from DEA\n",
    "3. Calculate NDVI as an example analysis on one of the extracted satellite timeseries\n",
    "4. Plot NDVI for the polygon extent\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Please note the use of `datacube.utils` package `geometry`: \n",
    "this is important for saving the coordinate reference system of the incoming shapefile in a format that the Digital Earth Australia query can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import datacube\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio.crs\n",
    "import rioxarray\n",
    "from datacube.utils import geometry\n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import datacube.utils.cog\n",
    "\n",
    "# import geopandas as gpd\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.cm\n",
    "import matplotlib.colors\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.ui\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "import skimage.color as colour\n",
    "import skimage.io\n",
    "import sklearn.metrics\n",
    "import xarray as xr\n",
    "\n",
    "# import sys\n",
    "# import datacube\n",
    "from datacube.storage.masking import make_mask\n",
    "\n",
    "# from dea_datahandling import load_ard\n",
    "from dea_bandindices import calculate_indices\n",
    "from dea_coastaltools import tidal_stats, tidal_tag\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_datahandling import array_to_geotiff, load_ard\n",
    "from dea_plotting import display_map, map_shapefile, rgb\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_temporaltools import time_buffer\n",
    "from IPython.display import display\n",
    "from odc.ui import with_ui_cbk\n",
    "from shapely.geometry import shape, Polygon\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Connect to the datacube database to enable loading Digital Earth Australia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Analyse_multiple_polygons\")\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Load predetermined polygons and select a region of interest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User defines path to polygon vector file, file name and \n",
    "## the column name for unique integer identifiers for each vector object.\n",
    "%cd '/home/jovyan/dev/dea-notebooks/Claire/'\n",
    "vector_file = \"QISMCQ_polygons.shp\"\n",
    "attribute_col = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the polygon vector file\n",
    "gdf_master = gpd.read_file(vector_file)\n",
    "\n",
    "# Set the crs to match the imagery data retrieved\n",
    "gdf_master.to_crs(epsg=3577, inplace=True)\n",
    "\n",
    "# #  View the unique classes\n",
    "# print(gdf_master[\"BRD_HAB\"].unique())\n",
    "\n",
    "#  Drop unrequired classes\n",
    "gdf_master = gdf_master.drop(\n",
    "    gdf_master[\n",
    "        (gdf_master.BRD_HAB == 'Subtidal consolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal consolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal coral')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal coral')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal unconsolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == \"Intertidal unconsolidated substrate\")\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal algae')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal seagrass')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal algae')\n",
    "    ].index\n",
    ")\n",
    "\n",
    "#  Reset the index of the gdf to infill dropped values\n",
    "gdf_master.reset_index(inplace=True)\n",
    "\n",
    "#  Check that correct classes remain\n",
    "print(gdf_master[\"BRD_HAB\"].unique())\n",
    "\n",
    "# Attribute each class with an integer value\n",
    "val = (gdf_master[\"BRD_HAB\"].unique()).tolist()\n",
    "\n",
    "num_list = []\n",
    "attr_key = []\n",
    "\n",
    "d = 0\n",
    "for x in range(len(gdf_master)):\n",
    "    for d in range(len(val)):\n",
    "        if gdf_master[\"BRD_HAB\"].values[x] == str(val[d]):\n",
    "            num_list.append(d)\n",
    "        # Create a key to interpret the integer attribute for each class\n",
    "        for y in num_list:\n",
    "            if y not in attr_key:\n",
    "                attr_key.append(y)\n",
    "\n",
    "val = [[el] for el in val]\n",
    "for x in attr_key:\n",
    "    val[x].append(attr_key[x])\n",
    "\n",
    "\n",
    "print(\"The attribute values for each class are as follows: \" + str(val))\n",
    "\n",
    "# Update the geodataframe of vector polygons with the integer attribution for each class\n",
    "gdf_master[\"id\"] = num_list\n",
    "\n",
    "# Map the shapefiles from imported vector set\n",
    "roi = map_shapefile(gdf_master, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After identifying an analysis region of interest(roi) on the map,\n",
    "intersect the roi with the master polygon gdf from which imagery \n",
    "will be extracted. Note that when multiple areas are drawn on the\n",
    "map above, it is the final roi that is used for analysis in the \n",
    "following cells\n",
    "'''\n",
    "\n",
    "##  Form a shapely polygon from the coordinates defined by the user on the map\n",
    "polygon_roi = Polygon(roi[-1]['geometry']['coordinates'][0])\n",
    "\n",
    "##  Generate a new geodataframe containing the user defined polygon geometry\n",
    "newdf = gpd.GeoDataFrame(gpd.GeoSeries(polygon_roi), columns=['geometry'], crs='EPSG:4326')\n",
    "newdf = newdf.to_crs(gdf_master.crs)\n",
    "\n",
    "##  Intersect the user-defined region of interest with the master\n",
    "##  to create the working gdf from which imagery will be extracted\n",
    "gdf = gpd.overlay(gdf_master, newdf, how='intersection')\n",
    "print('This selection includes ', len(gdf), ' individual polygons')\n",
    "gdf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load imagery for the region of interest\n",
    "\n",
    "*user to define time-period of interest in the query*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in gdf.iterrows():\n",
    "\n",
    "# #     print(f\"Feature: {index + 1}/{len(gdf)}\")\n",
    "# #     print(gdf[\"BRD_HAB\"].values[index])\n",
    "# #     print(str(index))\n",
    "#     attrs = {'pgid': row['index']}#((row['index']))\n",
    "# #     print (attrs)\n",
    "# #     print('----------')\n",
    "    \n",
    "# # type(attrs)\n",
    "# attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the general query and variables for later\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "align = (0, 0)\n",
    "\n",
    "# Query\n",
    "query = {\n",
    "    \"time\": (\"2013-01-01\", \"2021-01-01\"),\n",
    "    \"measurements\": [\"nbart_red\", \"nbart_green\", \"nbart_blue\", \"nbart_nir\", \"nbart_swir_1\"],\n",
    "    \"output_crs\": \"EPSG:3577\", # Do not change th\n",
    "    \"resolution\": (-30, 30),\n",
    "    \"group_by\": \"solar_day\",\n",
    "    \"dask_chunks\": {\"time\": 1, \"x\": 3000, \"y\": 3000},\n",
    "}\n",
    "\n",
    "# Designate dask chunks\n",
    "# It doesn't really matter how big the chunks we load are, as long as time ~ 1.\n",
    "chunks = {\"time\": 1, \"x\": 3000, \"y\": 3000}\n",
    "\n",
    "# Load data for predetermined polygons\n",
    "# Dictionary to save results\n",
    "results = {}\n",
    "\n",
    "'''List of saved tideposts. In the event that a polygon centroid geometry fails\n",
    "to return an associated tideheight, the tidal_tag function will bring in this list\n",
    "and use the most recent successful polygon centroid geometry to calculate a tideheight\n",
    "for the current polygon'''\n",
    "tideposts = [[0,0]]\n",
    "\n",
    "# Loop through polygons in geodataframe and extract satellite data\n",
    "for index, row in gdf.iterrows():\n",
    "\n",
    "    print(f\"Feature: {index + 1}/{len(gdf)}\")\n",
    "    print(gdf[\"BRD_HAB\"].values[index])\n",
    "    print(str(index))\n",
    "    print(str(row))\n",
    "\n",
    "    if not (str(row[attribute_col]) in results.keys()):\n",
    "        results[str(row[attribute_col])] = {}\n",
    "\n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "\n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load landsat\n",
    "    ds = load_ard(\n",
    "        dc=dc,\n",
    "        products=products,\n",
    "        min_gooddata=0.99,  # only take uncloudy scenes\n",
    "        ls7_slc_off=False,\n",
    "        **query,\n",
    "    )\n",
    "\n",
    "    ## Tidally tag datasets\n",
    "    ds, tidepost_lon, tidepost_lat = tidal_tag(ds,\n",
    "                                               tideposts[-1],\n",
    "                                               return_tideposts=True,\n",
    "                                               ebb_flow=True)\n",
    "    tideposts.append([tidepost_lon, tidepost_lat])\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "\n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    ds = ds.where(mask)\n",
    "    \n",
    "    ## Attach unique polygon id to each dataset\n",
    "    attrs = {'pgid': row['index']}\n",
    "    ds.attrs = attrs\n",
    "\n",
    "    # Append results to a dictionary using the attribute\n",
    "    # column as an key\n",
    "    results[str(row[attribute_col])][str(index)] = ds\n",
    "\n",
    "    print(row[attribute_col], index)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['0']['8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter results by tide height, calculate indices and build ITEM masks\n",
    "\n",
    "*user to define tide_range to keep - set the desired quantile value in `lowest_20` variable*\n",
    "\n",
    "*user defines required indices in the `calculate_indices` function call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = {}\n",
    "\n",
    "for k in results:\n",
    "\n",
    "    if not (str(k) in results2.keys()):\n",
    "        results2[str(k)] = {}\n",
    "        \n",
    "    for kk in results[k]:\n",
    "        \n",
    "        ds = results[k][kk] \n",
    "        \n",
    "        ## Save attributes to reattach later\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        ## Filter data by tide height\n",
    "        lowest_10 = ds.tide_height.quantile([0.10]).values\n",
    "        lowest_20 = ds.tide_height.quantile([0.20]).values  \n",
    "        results2[k][kk] = ds.where(ds.tide_height <= lowest_20, drop=True)      \n",
    "        \n",
    "        ## Compute data from dask - WARNING: time consuming step!\n",
    "        results2[k][kk] = results2[k][kk].compute()       \n",
    "        ds = results2[k][kk]\n",
    "       \n",
    "        ## Drop tide_height and ebb_flow variables\n",
    "        '''\n",
    "        works around the calculate_indices function which was stalling on the additional \n",
    "        coastal variables, drop tide_height and ebb_flow variables\n",
    "        '''\n",
    "        tide_height = ds['tide_height']\n",
    "        ebb_flow = ds['ebb_flow']\n",
    "        ds = ds.drop_vars(names = ('tide_height', 'ebb_flow'))\n",
    "        \n",
    "        # calculate ndvi for pixels inside the polygon\n",
    "        ds = calculate_indices(ds, index=['NDVI', 'MNDWI', 'NDAVI', 'WAVI', 'EVI', 'SAVI', 'NDWI'], \n",
    "                               collection='ga_ls_3', inplace=True)\n",
    "        \n",
    "        # Add tide_height back in to calculate ITEM mask\n",
    "        ds['tide_height'] = tide_height\n",
    "\n",
    "        ## Prepare data to calculate ITEM masks\n",
    "        lowest_10 = ds.where(ds.tide_height <= lowest_10, drop=True)\n",
    "        lowest_20 = ds.where(ds.tide_height <= lowest_20, drop=True)\n",
    "\n",
    "        ## Calculate ITEM layers\n",
    "        lowest_10_mask = lowest_10[['NDWI', 'tide_height']].median(dim='time')\n",
    "        lowest_20_mask = lowest_20[['NDWI', 'tide_height']].median(dim='time')\n",
    "        \n",
    "#         # Add ITEM mask layers to results2 datasets\n",
    "        results2[k][kk]['lowest_10_mask'] = lowest_10_mask.NDWI <= 0 \n",
    "        results2[k][kk]['lowest_20_mask'] = lowest_20_mask.NDWI <= 0 \n",
    "        \n",
    "        results2[str(k)][str(kk)] = xr.merge([results2[str(k)][str(kk)], ds])\n",
    "        \n",
    "        ## Attach unique polygon id to each dataset\n",
    "        results2[str(k)][str(kk)].attrs = attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save loaded results\n",
    "*By default, save the imagery and polygon sub-sampled datasets every time.*\n",
    "\n",
    "*If required to load last dataset, hash out the `save` calls and unhash the `load` calls*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/dev/dea-notebooks/Claire\n"
     ]
    }
   ],
   "source": [
    "## Save variables\n",
    "\n",
    "## Save imagery dict\n",
    "with open('results2_' + query['time'][0] +'.pickle', 'wb') as handle:\n",
    "    pickle.dump(results2, handle)\n",
    " \n",
    "## Save polygon gdf   \n",
    "with open('gdf_' + query['time'][0] +'.pickle', 'wb') as handle:\n",
    "    pickle.dump(gdf, handle)\n",
    "    \n",
    "'''-----------------------------------------'''\n",
    "\n",
    "# ## Load saved variables (hashed out by default)\n",
    "\n",
    "# ## Re-load the query\n",
    "\n",
    "# # Setup the general query and variables for later\n",
    "# products = [\"ga_ls8c_ard_3\"]\n",
    "# align = (0, 0)\n",
    "\n",
    "# # Query\n",
    "# query = {\n",
    "#     \"time\": (\"2013-01-01\", \"2020-08-01\"),\n",
    "#     \"measurements\": [\"nbart_red\", \"nbart_green\", \"nbart_blue\", \"nbart_nir\", \"nbart_swir_1\"],\n",
    "#     \"output_crs\": \"EPSG:3577\", # Do not change th\n",
    "#     \"resolution\": (-30, 30),\n",
    "#     \"group_by\": \"solar_day\",\n",
    "#     \"dask_chunks\": {\"time\": 1, \"x\": 3000, \"y\": 3000},\n",
    "# }\n",
    "\n",
    "# ## ensure that you are working from the same directory as the files are stored\n",
    "# %cd '/home/jovyan/dev/dea-notebooks/Claire'\n",
    "\n",
    "# ## Load imagery dict\n",
    "# with open('results2_' + query['time'][0] +'.pickle', 'rb') as handle:\n",
    "#     results2 = pickle.load(handle)\n",
    "\n",
    "# ## Load polygon gdf    \n",
    "# with open('gdf_' + query['time'][0] +'.pickle', 'rb') as handle:\n",
    "#     gdf = pickle.load(handle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the following cell to define the functions that enable data interrogation and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funcs for py script\n",
    "\n",
    "def coastal_wit(gdf,\n",
    "               results3,\n",
    "               pg,\n",
    "               classes = False,\n",
    "               drop=False,\n",
    "               pc_drop=0,\n",
    "               px_min=0,\n",
    "               water=True):\n",
    "    \"\"\"\n",
    "    Takes a polygon of interest (pg) from the gdf of polygon shapefiles and extracts and plots \n",
    "    the frequency of pixels as assigned into class ranges of NDVI. Also includes pixels identified\n",
    "    as wet. All NDVI pixels are masked by 'dry' range of MNDWI.\n",
    "    If conditional dataset dropping is required, set drop=True and nominate the percent of wet pixels\n",
    "    tolerated and the minimum number of allowable pixels (e.g. pc_drop=90 means drop any timestep dataset\n",
    "    when more than 90% of pixels are wet; px_min=5 means that a timestep dataset will only be dropped\n",
    "    when more than pc_drop pixels are wet AND the remaining pixels sum to less than or = px_min - or\n",
    "    5 in this case)\n",
    "    By default, water is included in the plot. If water suppression is required, set water=False.\n",
    "    If the WIT datasets have already been generated and included in the xarray dataset (`results3`) then \n",
    "    set `classes = True` to avoid re-calculation. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    if water:\n",
    "        ##  Data prep for WIT prototype (automated for all timesteps for all polygons)\n",
    "    \n",
    "        ##  Generate all datasets for the stacked line plot (WIT) by adding as variables to results dataset\n",
    "        for k in results3:\n",
    "            for kk in results3[k]: \n",
    "                \n",
    "                if classes == False:\n",
    "\n",
    "                    ##  Add a non-water pixel mask variable\n",
    "                    results3[k][kk]['mask'] = results3[k][kk].MNDWI <= 0\n",
    "\n",
    "                    ##  Generate the MNDWI_water class\n",
    "                    results3[k][kk]['mndwi_water'] = results3[k][kk].MNDWI > 0\n",
    "\n",
    "                    ##  Generate NDVI classes\n",
    "\n",
    "                    ##  NDVI less than 0.1\n",
    "                    results3[k][kk]['unveg'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI < 0.1).astype(int))\n",
    "\n",
    "                    ##  NDVI 0.1 to 0.33\n",
    "                    results3[k][kk]['ndvi_low'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.1).astype(int) & (results3[k][kk].NDVI < 0.333).astype(int))\n",
    "\n",
    "                    ##  NDVI 0.33 to 0.66\n",
    "                    results3[k][kk]['ndvi_mid'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.333).astype(int) & (results3[k][kk].NDVI < 0.666).astype(int))\n",
    "\n",
    "                    ### NDVI 0.66 to 1\n",
    "                    results3[k][kk]['ndvi_high'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.666).astype(int) & (results3[k][kk].NDVI <= 1).astype(int))\n",
    "\n",
    "                    ##  Mask the NDVI classes to show non-water pixels only\n",
    "                    results3[k][kk]['unveg'] = results3[k][kk]['unveg'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_low'] = results3[k][kk]['ndvi_low'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_mid'] = results3[k][kk]['ndvi_mid'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_high'] = results3[k][kk]['ndvi_high'].where(results3[k][kk]['mask'])\n",
    "\n",
    "\n",
    "        if classes == False:\n",
    "            ##  Automated: Create a new gdf to record the landcover types/pixel counts per timestep for a single polygon and normalise for plotting\n",
    "            ##  Extract the pixel counts and normalise, in preparation for plotting\n",
    "\n",
    "            ## From the results2 dataset, the temporal values for the nominated polygon are stored in the following dataset of the resutls2 dict:\n",
    "            ## class key\n",
    "            cl = gdf.loc[gdf['index'] == pg].id.values.item()\n",
    "            ## class polygon\n",
    "            pg1 = gdf.loc[gdf['index'] == pg].index.values.item()\n",
    "\n",
    "        ## Set up lists to extract pixel counts per timestep into\n",
    "        ls_pixels = []\n",
    "        ls_mndwi = []\n",
    "        ls_unveg = []\n",
    "        ls_ndvilow = []\n",
    "        ls_ndvimid = []\n",
    "        ls_ndvihigh = []\n",
    "        ls_pxsum = []\n",
    "        ls_tide = []\n",
    "\n",
    "        if classes == False:\n",
    "        \n",
    "             ##  Populate pixel count lists per class per timestep\n",
    "            for t in range (0, len(results3[str(cl)][str(pg1)].time)):\n",
    "                ls_pixels.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).nbart_red)))\n",
    "                ls_mndwi.append(np.count_nonzero(results3[str(cl)][str(pg1)].isel(time=t).mndwi_water))\n",
    "                ls_unveg.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).unveg)))\n",
    "                ls_ndvilow.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_low)))\n",
    "                ls_ndvimid.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_mid)))\n",
    "                ls_ndvihigh.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_high)))\n",
    "                ls_pxsum.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).nbart_red)))\n",
    "                ls_tide.append(results3[str(cl)][str(pg1)].isel(time=t).tide_height.median().values)\n",
    "                \n",
    "                index = results3[str(cl)][str(pg1)].time.values\n",
    "                \n",
    "        if classes == True:\n",
    "        \n",
    "             ##  Populate pixel count lists per class per timestep\n",
    "            for t in range (0, len(results3.time)):\n",
    "                ls_pixels.append(np.count_nonzero(~np.isnan(results3.isel(time=t).nbart_red)))\n",
    "#                 ls_mndwi.append(np.count_nonzero((results3.isel(time=t).mndwi_water)))\n",
    "                ls_mndwi.append(np.count_nonzero(~np.isnan(mWIT1.isel(time=t).mndwi_water.where(mWIT1.isel(time=t).mndwi_water == True))))\n",
    "                ls_unveg.append(np.count_nonzero(~np.isnan(results3.isel(time=t).unveg)))\n",
    "                ls_ndvilow.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_low)))\n",
    "                ls_ndvimid.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_mid)))\n",
    "                ls_ndvihigh.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_high)))\n",
    "                ls_pxsum.append(np.count_nonzero(~np.isnan(results3.isel(time=t).nbart_red)))\n",
    "                ls_tide.append(results3.isel(time=t).tide_height.median().values)\n",
    "\n",
    "                index = results3.time.values\n",
    "                \n",
    "        ##  Generate a dataframe summarising class pixel counts per timestep\n",
    "        classes_df = pd.DataFrame(\n",
    "                    {\"pixels\": ls_pixels,\n",
    "                    \"water\": ls_mndwi,\n",
    "                    \"unveg\": ls_unveg,\n",
    "                    \"ndvi_low\": ls_ndvilow,\n",
    "                    \"ndvi_mid\": ls_ndvimid,\n",
    "                    \"ndvi_high\": ls_ndvihigh,\n",
    "                    \"px_sum\": ls_pxsum,\n",
    "                    \"tide_height\": ls_tide},\n",
    "                    index = index\n",
    "                    )\n",
    "\n",
    "        ##  Normalise pixel counts per class per timestep\n",
    "        classes_df['pc_water'] = classes_df['water']/classes_df['pixels']*100\n",
    "        classes_df['pc_unveg'] = classes_df['unveg']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_low'] = classes_df['ndvi_low']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_mid'] = classes_df['ndvi_mid']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_high'] = classes_df['ndvi_high']/classes_df['pixels']*100\n",
    "        classes_df['pc_total'] = (classes_df['pc_water']+\n",
    "                                  classes_df['pc_unveg']+\n",
    "                                  classes_df['pc_ndvi_low']+\n",
    "                                  classes_df['pc_ndvi_mid']+\n",
    "                                  classes_df['pc_ndvi_high'])\n",
    "        classes_df['pc_exposedpx'] = (classes_df['unveg']+\n",
    "                                      classes_df['ndvi_low']+\n",
    "                                      classes_df['ndvi_mid']+\n",
    "                                      classes_df['ndvi_high'])/classes_df['px_sum']*100\n",
    "\n",
    "\n",
    "        if drop:\n",
    "            ##  Conditions on which to drop rows\n",
    "            drop_records = classes_df[(classes_df['pc_water'] >= pc_drop) & \n",
    "                          (classes_df['pc_unveg']+\n",
    "                           classes_df['pc_ndvi_low']+\n",
    "                           classes_df['pc_ndvi_mid']+\n",
    "                           classes_df['pc_ndvi_high'] <= px_min)].index\n",
    "\n",
    "            ##  Apply conditions to df by dropping values\n",
    "            classes_df.drop(drop_records, inplace=True)\n",
    "\n",
    "    if not water:\n",
    "        \n",
    "            ##  Generate all datasets for the stacked line plot (WIT) by adding as variables to results dataset\n",
    "        for k in results3:\n",
    "            for kk in results3[k]: \n",
    "                \n",
    "                if classes == False:\n",
    "\n",
    "                    ##  Add a non-water pixel mask variable\n",
    "                    results3[k][kk]['mask'] = results3[k][kk].MNDWI <= 0\n",
    "\n",
    "                    ##  Generate the MNDWI_water class\n",
    "                    results3[k][kk]['mndwi_water'] = results3[k][kk].MNDWI > 0\n",
    "\n",
    "                    ##  Generate NDVI classes\n",
    "\n",
    "                    ##  NDVI less than 0.1\n",
    "                    results3[k][kk]['unveg'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI < 0.1).astype(int))\n",
    "\n",
    "                    ##  NDVI 0.1 to 0.33\n",
    "                    results3[k][kk]['ndvi_low'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.1).astype(int) & (results3[k][kk].NDVI < 0.333).astype(int))\n",
    "\n",
    "                    ##  NDVI 0.33 to 0.66\n",
    "                    results3[k][kk]['ndvi_mid'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.333).astype(int) & (results3[k][kk].NDVI < 0.666).astype(int))\n",
    "\n",
    "                    ### NDVI 0.66 to 1\n",
    "                    results3[k][kk]['ndvi_high'] = results3[k][kk].NDVI.where(\n",
    "                        (results3[k][kk].NDVI >= 0.666).astype(int) & (results3[k][kk].NDVI <= 1).astype(int))\n",
    "\n",
    "                    ##  Mask the NDVI classes to show non-water pixels only\n",
    "                    results3[k][kk]['unveg'] = results3[k][kk]['unveg'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_low'] = results3[k][kk]['ndvi_low'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_mid'] = results3[k][kk]['ndvi_mid'].where(results3[k][kk]['mask'])\n",
    "                    results3[k][kk]['ndvi_high'] = results3[k][kk]['ndvi_high'].where(results3[k][kk]['mask'])\n",
    "\n",
    "        if classes == False:\n",
    "            ##  Automated: Create a new gdf to record the landcover types/pixel counts per timestep for a single polygon and normalise for plotting\n",
    "            ##  Extract the pixel counts and normalise, in preparation for plotting\n",
    "\n",
    "            ## From the results2 dataset, the temporal values for the nominated polygon are stored in the following dataset of the resutls2 dict:\n",
    "            ## class key\n",
    "            cl = gdf.loc[gdf['index'] == pg].id.values.item()\n",
    "            ## class polygon\n",
    "            pg1 = gdf.loc[gdf['index'] == pg].index.values.item()\n",
    "\n",
    "        ## Set up lists to extract pixel counts per timestep into\n",
    "        ls_pixels = []\n",
    "        # ls_mndwi = []\n",
    "        ls_unveg = []\n",
    "        ls_ndvilow = []\n",
    "        ls_ndvimid = []\n",
    "        ls_ndvihigh = []\n",
    "        ls_pxsum = []\n",
    "        ls_tide = []\n",
    "\n",
    "        if classes == False:\n",
    "        \n",
    "            ##  Populate pixel count lists per class per timestep\n",
    "            for t in range (0, len(results3[str(cl)][str(pg1)].time)):\n",
    "                ls_pixels.append((np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).nbart_red))) - \n",
    "                                  (np.count_nonzero(results3[str(cl)][str(pg1)].isel(time=t).mndwi_water)))\n",
    "            #     ls_mndwi.append(np.count_nonzero(results3[str(cl)][str(pg1)].isel(time=t).mndwi_water))\n",
    "                ls_unveg.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).unveg)))\n",
    "                ls_ndvilow.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_low)))\n",
    "                ls_ndvimid.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_mid)))\n",
    "                ls_ndvihigh.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).ndvi_high)))\n",
    "                ls_pxsum.append(np.count_nonzero(~np.isnan(results3[str(cl)][str(pg1)].isel(time=t).nbart_red)))\n",
    "                ls_tide.append(results3[str(cl)][str(pg1)].isel(time=t).tide_height.median().values)\n",
    "                \n",
    "                index = results3[str(cl)][str(pg1)].time.values                \n",
    "                \n",
    "        if classes == True:\n",
    "        \n",
    "            ##  Populate pixel count lists per class per timestep\n",
    "            for t in range (0, len(results3.time)):\n",
    "                ls_pixels.append((np.count_nonzero((results3.isel(time=t).nbart_red))) - \n",
    "                                 (ls_mndwi.append(np.count_nonzero(~np.isnan(mWIT1.isel(time=t).mndwi_water.where(mWIT1.isel(time=t).mndwi_water == True))))))\n",
    "#                                   (np.count_nonzero((results3.isel(time=t).mndwi_water))))\n",
    "            #     ls_mndwi.append(np.count_nonzero(results3[str(cl)][str(pg1)].isel(time=t).mndwi_water))\n",
    "                ls_unveg.append(np.count_nonzero(~np.isnan(results3.isel(time=t).unveg)))\n",
    "                ls_ndvilow.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_low)))\n",
    "                ls_ndvimid.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_mid)))\n",
    "                ls_ndvihigh.append(np.count_nonzero(~np.isnan(results3.isel(time=t).ndvi_high)))\n",
    "                ls_pxsum.append(np.count_nonzero(~np.isnan(results3.isel(time=t).nbart_red)))\n",
    "                ls_tide.append(results3.isel(time=t).tide_height.median().values)\n",
    "\n",
    "                index = results3.time.values\n",
    "                \n",
    "        ##  Generate a dataframe summarising class pixel counts per timestep\n",
    "        classes_df = pd.DataFrame(\n",
    "                    {\"pixels\": ls_pixels,\n",
    "        #             \"water\": ls_mndwi,\n",
    "                    \"unveg\": ls_unveg,\n",
    "                    \"ndvi_low\": ls_ndvilow,\n",
    "                    \"ndvi_mid\": ls_ndvimid,\n",
    "                    \"ndvi_high\": ls_ndvihigh,\n",
    "                    \"px_sum\": ls_pxsum,\n",
    "                    \"tide_height\": ls_tide},\n",
    "                    index = index\n",
    "                    )\n",
    "\n",
    "        ##  Normalise pixel counts per class per timestep\n",
    "        # classes_df['pc_water'] = classes_df['water']/classes_df['pixels']*100\n",
    "        classes_df['pc_unveg'] = classes_df['unveg']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_low'] = classes_df['ndvi_low']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_mid'] = classes_df['ndvi_mid']/classes_df['pixels']*100\n",
    "        classes_df['pc_ndvi_high'] = classes_df['ndvi_high']/classes_df['pixels']*100\n",
    "        classes_df['pc_total'] = (classes_df['pc_unveg']+\n",
    "                                  classes_df['pc_ndvi_low']+\n",
    "                                  classes_df['pc_ndvi_mid']+\n",
    "                                  classes_df['pc_ndvi_high'])#+classes_df['pc_water']\n",
    "        classes_df['pc_exposedpx'] = (classes_df['unveg']+\n",
    "                                      classes_df['ndvi_low']+\n",
    "                                      classes_df['ndvi_mid']+\n",
    "                                      classes_df['ndvi_high'])/classes_df['px_sum']*100\n",
    "\n",
    "\n",
    "        drop_records = classes_df[(classes_df['pixels'] == 0)].index\n",
    "\n",
    "        ##  Apply conditions to df by dropping values\n",
    "        classes_df.drop(drop_records, inplace=True)\n",
    "        \n",
    "        if drop:\n",
    "            ##  Conditions on which to drop rows\n",
    "            drop_records = classes_df[(classes_df['pc_water'] >= pc_drop) & \n",
    "                          (classes_df['pc_unveg']+\n",
    "                           classes_df['pc_ndvi_low']+\n",
    "                           classes_df['pc_ndvi_mid']+\n",
    "                           classes_df['pc_ndvi_high'] <= px_min)].index\n",
    "\n",
    "            ##  Apply conditions to df by dropping values\n",
    "            classes_df.drop(drop_records, inplace=True)\n",
    "    return classes_df\n",
    "\n",
    "def onclick_timeseries(event):\n",
    "    '''\n",
    "    This widget allows the user to select a time point from a plotted time series. \n",
    "    It then translates the chosen point back into the approriate datetime object so\n",
    "    that it can be used to find the location of this time point within the extracted\n",
    "    datasets. The index location of this time step is also returned. \n",
    "    \n",
    "    '''\n",
    "    global time_slice, TimeIndex, pixelx, pixely\n",
    "    \n",
    "    # Get time from x axis of plot \n",
    "    timeOfInterest = event.xdata\n",
    "    \n",
    "    # Get x and y coordinates from click\n",
    "    pixelx, pixely = int(event.xdata), int(event.ydata)\n",
    "    \n",
    "    # Add point to image\n",
    "    plt.plot(pixelx, pixely, 'ro', markersize=5)\n",
    "    \n",
    "    # Convert clicked int to datetime format\n",
    "    time_slice = matplotlib.dates.num2date(timeOfInterest).date()\n",
    "    \n",
    "    # Convert clicked value to str\n",
    "    time_slice = str(time_slice)\n",
    "    \n",
    "    # Convert clicked value to correct datetime format\n",
    "    time_slice = pd.to_datetime(time_slice, format='%Y-%m-%d')\n",
    "    \n",
    "    # Find the time index of the chosen time slice\n",
    "    TimeIndex = results3[str(cl)][str(pg1)].indexes['time'].get_loc(time_slice, method='nearest')\n",
    "    \n",
    "    # Print the date of the image closest to clicked pixel\n",
    "    ClosestImage = results3[str(cl)][str(pg1)].time[TimeIndex].values\n",
    "    \n",
    "    # Update text below plot\n",
    "    w2.value = 'Closest imagery date : {}'.format(ClosestImage)\n",
    "    \n",
    "        \n",
    "def temporal_stats (gdf, \n",
    "                    results2,\n",
    "                    zonal=True,\n",
    "                    pixel=True,\n",
    "                    mask= 'lowest_20_mask'):\n",
    "\n",
    "    '''\n",
    "    This function calculates and returns the temporal mean and standard deviation\n",
    "    for a range of indices for every polygon and every pixel within\n",
    "    every polygon in the form of a geodataframe.  \n",
    "    For now, leave zonal and pixel set to True to calculate all results. Func may\n",
    "    not work if either of these vars are set to False.\n",
    "    ITEM masking is available using the `mask` variable. If no masking is required, set\n",
    "    this variable to one of the bands such as 'nbart_nir'. If masking, two ITEM\n",
    "    layers are generated by default to choose from. Either 'lowest_20_mask' for\n",
    "    all images except those associated with the lowest 20% of observed tides, or\n",
    "    'lowest_10_mask' to mask by the equivalent in the 10% range.\n",
    "    '''\n",
    "    ##  Spectral indices available in the `calculate_indices` function (dea_bandindices.py)\n",
    "    index_ls = [\n",
    "        \"NDVI\",\n",
    "        \"EVI\",\n",
    "        \"NDAVI\", \n",
    "        \"WAVI\",\n",
    "        \"LAI\",\n",
    "        \"SAVI\",\n",
    "        \"MSAVI\",\n",
    "        \"NDMI\",\n",
    "        \"NBR\",\n",
    "        \"BAI\",\n",
    "        \"NDCI\",\n",
    "        \"NDSI\",\n",
    "        \"NDTI\",\n",
    "        \"NDWI\",\n",
    "        \"MNDWI\",\n",
    "        \"NDBI\",\n",
    "        \"BUI\",\n",
    "        \"BAEI\",\n",
    "        \"NBI\",\n",
    "        \"BSI\",\n",
    "        \"AWEI_ns\",\n",
    "        \"AWEI_sh\",\n",
    "        \"WI\",\n",
    "        \"TCW\",\n",
    "        \"TCG\",\n",
    "        \"TCB\",\n",
    "        \"CMR\",\n",
    "        \"FMR\",\n",
    "        \"IOR\",\n",
    "    ]\n",
    "\n",
    "       \n",
    "    \n",
    "    ## Calculate zonal polygon results\n",
    "    if zonal:\n",
    "        \n",
    "        gdf_zonal = gdf \n",
    "        \n",
    "        ##  Generate a list of keys for polygons in class '0' of results2\n",
    "        keylist = list(results2['0'].keys())\n",
    "\n",
    "        ##  For each nominated indice:\n",
    "        for var in index_ls:\n",
    "            if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "                lsmean = []\n",
    "                lsstd = []\n",
    "\n",
    "                ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "                ##  Append polygon id and temporal statistic to gdf\n",
    "                for k in results2:\n",
    "\n",
    "                    for kk in results2[k]:\n",
    "                        temporalmean = ((results2[k][kk][var]\n",
    "                                         .where(results2[k][kk][mask])\n",
    "                                         .mean('y')\n",
    "                                         .mean('x'))\n",
    "                                         .mean())\n",
    "                        lsmean.append([int(kk), temporalmean.values])\n",
    "                        name1 = str(var) + ' zonal mean'\n",
    "                        results2[str(k)][str(kk)][name1] = (temporalmean.values)\n",
    "                        \n",
    "                        temporalstd = ((results2[k][kk][var]\n",
    "                                        .where(results2[k][kk][mask])\n",
    "                                        .std('y')\n",
    "                                        .std('x'))\n",
    "                                        .std())\n",
    "                        lsstd.append([int(kk), temporalstd.values])  \n",
    "                        name2 = str(var) + ' zonal std'\n",
    "                        results2[str(k)][str(kk)][name2] = (temporalstd.values)                        \n",
    "\n",
    "                ##  Sort the list by polygon id to match up to the original polygon gdf\n",
    "                lsmean = sorted(lsmean)\n",
    "                lsstd = sorted(lsstd)\n",
    "\n",
    "                ##  Separate the sorted polygon ids from the indice statistic to build into a pd.DataFrame\n",
    "                indicemean = []\n",
    "                indicestd = []\n",
    "                polyid = []\n",
    "\n",
    "                for x in lsmean:\n",
    "                    polyid.append(x[0])\n",
    "                    indicemean.append(x[1])\n",
    "\n",
    "                for x in lsstd:\n",
    "                    indicestd.append(x[1])\n",
    "\n",
    "                # Build a pd.DataFrame from the sorted polygon id and indice statistics. \n",
    "                # Nominate a name for the new column.\n",
    "                indexstats = pd.DataFrame(\n",
    "                    indicemean, index=polyid, columns=[str(var) + \" zonal mean\"]\n",
    "                )\n",
    "\n",
    "#                 indexstats[str(var) + ' zonal mean'] = indicemean\n",
    "#                 indexstats['polyid'] = polyid \n",
    "    \n",
    "                indexstats[str(var) + \" zonal std\"] = None\n",
    "                indexstats.loc[polyid, (str(var) + \" zonal std\")] = indicestd\n",
    "\n",
    "                # Workaround to handle automatic addition of key_0 column at merge step\n",
    "                if \"key_0\" in gdf_zonal.columns:\n",
    "                    gdf_zonal.drop(columns=[\"key_0\"], inplace=True)\n",
    "\n",
    "                # # Merge the indice statistic for each polygon into the original polygon gdf\n",
    "                gdf_zonal = gdf_zonal.merge(indexstats, on=indexstats.index)\n",
    "\n",
    "        ## Rename the 'index' column to avoid confusion with the gdf.index\n",
    "        gdf_zonal.rename(columns={'index' : 'pgid'}, inplace=True)\n",
    "        gdf_zonal.rename(columns={'geometry' : 'pg_geometry'}, inplace=True)\n",
    "\n",
    "#                         return gdf\n",
    "\n",
    "    ## Calculate pixel results\n",
    "    if pixel:\n",
    "\n",
    "        ## New dict to store arrays\n",
    "#         pxsummary = {}\n",
    "\n",
    "        ## Master gdf\n",
    "        gdf_px = gpd.GeoDataFrame()\n",
    "        gdf_px['geometry'] = None\n",
    "\n",
    "        ## List to store geom, unique pixel id \n",
    "        # pxid = []\n",
    "        uniquepx = []\n",
    "        pxgeom = []\n",
    "\n",
    "        ##  Generate a list of keys for polygons in class '0' of results2\n",
    "        keylist = list(results2['0'].keys())\n",
    "\n",
    "\n",
    "        ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "        ##  Append polygon id and temporal statistic to gdf\n",
    "        for k in results2:\n",
    "\n",
    "#             if not (str(k) in pxsummary.keys()):\n",
    "#                 pxsummary[str(k)] = {}\n",
    "\n",
    "            for kk in results2[k]:\n",
    "\n",
    "                lon = []\n",
    "                for value in results2[k][kk].x.values:\n",
    "                    lon.append(value)\n",
    "\n",
    "                lat = []\n",
    "                for value in results2[k][kk].y.values:\n",
    "                    lat.append(value)\n",
    "\n",
    "                ds = xr.Dataset()\n",
    "\n",
    "                ##  For each nominated indice:\n",
    "                for var in index_ls:\n",
    "                    if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "                        temporalmean = (results2[str(k)][str(kk)][var]\n",
    "                                        .where(results2[k][kk][mask])\n",
    "                                        .mean(dim='time'))\n",
    "                        name1 = str(var) + ' px mean'\n",
    "                        ds[name1] = (('y', 'x'), temporalmean)\n",
    "                        results2[str(k)][str(kk)][name1] = (('y', 'x'), temporalmean)\n",
    "\n",
    "                        temporalstd = (results2[k][kk][var]\n",
    "                                       .where(results2[k][kk][mask])\n",
    "                                       .std(dim='time'))\n",
    "                        name2 = str(var) + ' px std'\n",
    "                        ds[name2] = (('y', 'x'), temporalstd)\n",
    "                        results2[str(k)][str(kk)][name2] = (('y', 'x'), temporalstd)\n",
    "\n",
    "                ds.coords['lon'] = ('x'), lon\n",
    "                ds.coords['lat'] = ('y'), lat\n",
    "\n",
    "\n",
    "                ## Extract pixel geometry/shape for input into gdf for choropleth plotting\n",
    "                # Extract dataset matching polygon\n",
    "                closest_ds = results2[k][kk]\n",
    "\n",
    "                ## Extract pgid attributes to attach later\n",
    "                attrs = closest_ds.attrs\n",
    "                ds.attrs = attrs\n",
    "\n",
    "                ## Skip empty arrays\n",
    "                if closest_ds.x.size == 0:\n",
    "                    print('Empty arrays: k: ', k, 'kk: ', kk)\n",
    "                    continue\n",
    "                else:       \n",
    "                    # Input array (based on red band) to segment and vectorise\n",
    "                    input_array = closest_ds.nbart_red\n",
    "                    input_transform = closest_ds.affine  \n",
    "                    input_crs = input_array.crs\n",
    "\n",
    "                    # Create array with a unique value per cell\n",
    "                    unique_pixels = np.arange(input_array.size).reshape(input_array.shape)\n",
    "\n",
    "                    # Vectorise each unique feature in array\n",
    "                    vectors = rasterio.features.shapes(\n",
    "                        source=unique_pixels.astype(np.int16), transform=input_transform\n",
    "                    )\n",
    "\n",
    "                    # Extract polygons and values from generator\n",
    "                    vectors = list(vectors)\n",
    "                    values = [value for polygon, value in vectors]\n",
    "                    polygons = [shape(polygon) for polygon, value in vectors]\n",
    "                    pp = np.array(polygons)\n",
    "                    pp = pp.reshape(len(input_array.y), len(input_array.x))\n",
    "\n",
    "#              # Create a geopandas dataframe populated with the polygon shapes\n",
    "#              closestdate_poly_gdf = gpd.GeoDataFrame(data={\"id\": values}, geometry=polygons, crs=input_crs)\n",
    "\n",
    "                    ds['geometry'] = (('y', 'x'), pp)\n",
    "\n",
    "                results2[k][kk]['geometry'] = (('y', 'x'), pp)\n",
    "                results2[k][kk].set_coords('geometry')\n",
    "\n",
    "\n",
    "                ## Append [geom, unique pixel id] to list for later addition to gdf, merging on geom\n",
    "                pxid2 = []\n",
    "                pgid2 = []\n",
    "\n",
    "                for x in ds['x']:\n",
    "                    for y in ds['y']:\n",
    "                        pxid1 = str(attrs['pgid'])+'_'+str(ds['y'][y].item())+'_'+str(ds['x'][x].item())\n",
    "                        gm = pp[ds['y'][y].item()][ds['x'][x].item()]\n",
    "\n",
    "                        uniquepx.append(pxid1)\n",
    "                        pxgeom.append(gm)\n",
    "                        pxid2.append(pxid1)\n",
    "                        pgid2.append(attrs['pgid'])\n",
    "\n",
    "                pxid2 = np.array(pxid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "                pgid2 = np.array(pgid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "                results2[k][kk]['pxid'] = (('y', 'x'), pxid2)\n",
    "\n",
    "                ds['pgid'] = (('y', 'x'), pgid2)\n",
    "\n",
    "                ## Reattach attrs\n",
    "                results2[k][kk].attrs = attrs\n",
    "\n",
    "#                 pxsummary[k][kk] = ds\n",
    "\n",
    "                ## Append ds to df_pxsummary\n",
    "                ds = ds.to_dataframe()\n",
    "                gdf_px = gdf_px.append(ds)\n",
    "                gdf_px.crs = gdf.crs\n",
    "\n",
    "        pxid = gpd.GeoDataFrame(uniquepx, geometry=pxgeom, columns=['pxid'], crs=input_crs)\n",
    "\n",
    "        gdf_px.reset_index(inplace=True)\n",
    "\n",
    "        gdf_px = gdf_px.merge(pxid)\n",
    "        gdf_px.set_index('pxid', inplace=True, drop=False)\n",
    "        gdf_px.dropna(axis=0, inplace=True)\n",
    "        gdf_px.sort_index(level='pxid', inplace=True)\n",
    "\n",
    "\n",
    "    ## Merge the (zonal) gdf and gdf_px\n",
    "    gdf_merged = gdf_px.merge(gdf_zonal, on = 'pgid')\n",
    "\n",
    "    ## Drop the pg_geometry to use the pixel geometry default\n",
    "    gdf_merged.drop(columns='pg_geometry', inplace=True)\n",
    "\n",
    "    ##  Create new gdf's for each class to plot\n",
    "    grasses = gdf_merged.drop(\n",
    "        gdf_merged[gdf_merged.BRD_HAB != \"Intertidal grass-herb-sedge-other succulent\"].index\n",
    "    )\n",
    "    mangroves = gdf_merged.drop(\n",
    "        gdf_merged[gdf_merged.BRD_HAB != \"Intertidal mangroves and other trees & shrubs\"].index\n",
    "    )\n",
    "    seagrass = gdf_merged.drop(gdf_merged[gdf_merged.BRD_HAB != \"Intertidal seagrass\"].index)\n",
    "\n",
    "    ##  Drop polygons containing NaN values\n",
    "    ##  ASSUMPTION: if NDVI contains NaNs, all indices will contain NaNs. \n",
    "    ##  ToDo: write a loop or func that looks for NaNs in any of the supplied indices\n",
    "    grasses = grasses.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std'])\n",
    "    mangroves = mangroves.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std'])\n",
    "    seagrass = seagrass.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std']) \n",
    "\n",
    "    return gdf_merged, results2, grasses, mangroves, seagrass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4487e33859e4ea895f079f76dcea0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.077569349098606, 151.5419280732554], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Test cell: how to incorporate ITEM masking\n",
    "\n",
    "# # results2.where(results2[k][kk]['lowest_20_mask'])\n",
    "# gdf_merged, results2, grasses, mangroves, seagrass = temporal_stats(gdf, \n",
    "#                                                                     results2, \n",
    "#                                                                     mask='lowest_20_mask')\n",
    "\n",
    "# map_shapefile(seagrass, attribute = 'NDWI px mean', continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Superceded functions\n",
    "\n",
    "# For every polygon, extract list pairs for each polygon pixel of the temporal mean and std of the nominated indice\n",
    "\n",
    "# def px_heatmap (results2, index_ls):\n",
    "\n",
    "#     ## New dict to store arrays\n",
    "#     pxsummary = {}\n",
    "\n",
    "#     ## Master gdf\n",
    "#     df_pxsummary = gpd.GeoDataFrame()\n",
    "#     df_pxsummary['geometry'] = None\n",
    "\n",
    "#     ## List to store geom, unique pixel id \n",
    "#     # pxid = []\n",
    "#     uniquepx = []\n",
    "#     pxgeom = []\n",
    "\n",
    "#     ##  Generate a list of keys for polygons in class '0' of results2\n",
    "#     keylist = list(results2['0'].keys())\n",
    "\n",
    "\n",
    "#     ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "#     ##  Append polygon id and temporal statistic to gdf\n",
    "#     for k in results2:\n",
    "\n",
    "#         if not (str(k) in pxsummary.keys()):\n",
    "#             pxsummary[str(k)] = {}\n",
    "\n",
    "#         for kk in results2[k]:\n",
    "\n",
    "#             lon = []\n",
    "#             for value in results2[k][kk].x.values:\n",
    "#                 lon.append(value)\n",
    "\n",
    "#             lat = []\n",
    "#             for value in results2[k][kk].y.values:\n",
    "#                 lat.append(value)\n",
    "\n",
    "#             ds = xr.Dataset()\n",
    "\n",
    "#             ##  For each nominated indice:\n",
    "#             for var in index_ls:\n",
    "#                 if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "#                     temporalmean = (results2[str(k)][str(kk)][var].mean(dim='time'))\n",
    "#                     name1 = str(var) + ' px mean'\n",
    "#                     ds[name1] = (('y', 'x'), temporalmean)\n",
    "\n",
    "#                     temporalstd = (results2[k][kk][var].std(dim='time'))\n",
    "#                     name2 = str(var) + ' px std'\n",
    "#                     ds[name2] = (('y', 'x'), temporalstd)\n",
    "\n",
    "#             ds.coords['lon'] = ('x'), lon\n",
    "#             ds.coords['lat'] = ('y'), lat\n",
    "            \n",
    "\n",
    "#             ## Extract pixel geometry/shape for input into gdf for choropleth plotting\n",
    "#             # Extract dataset matching polygon\n",
    "#             closest_ds = results2[k][kk]\n",
    "\n",
    "#             ## Extract pgid attributes to attach later\n",
    "#             attrs = closest_ds.attrs\n",
    "#             ds.attrs = attrs\n",
    "            \n",
    "#             ## Skip empty arrays\n",
    "#             if closest_ds.x.size == 0:\n",
    "#                 print('Empty arrays: k: ', k, 'kk: ', kk)\n",
    "#                 continue\n",
    "#             else:       \n",
    "#                 # Input array (based on red band) to segment and vectorise\n",
    "#                 input_array = closest_ds.nbart_red\n",
    "#                 input_transform = closest_ds.affine  \n",
    "#                 input_crs = input_array.crs\n",
    "\n",
    "#                 # Create array with a unique value per cell\n",
    "#                 unique_pixels = np.arange(input_array.size).reshape(input_array.shape)\n",
    "\n",
    "#                 # Vectorise each unique feature in array\n",
    "#                 vectors = rasterio.features.shapes(\n",
    "#                     source=unique_pixels.astype(np.int16), transform=input_transform\n",
    "#                 )\n",
    "\n",
    "#                 # Extract polygons and values from generator\n",
    "#                 vectors = list(vectors)\n",
    "#                 values = [value for polygon, value in vectors]\n",
    "#                 polygons = [shape(polygon) for polygon, value in vectors]\n",
    "#                 pp = np.array(polygons)\n",
    "#                 pp = pp.reshape(len(input_array.y), len(input_array.x))\n",
    "\n",
    "#         #         # Create a geopandas dataframe populated with the polygon shapes\n",
    "#         #         closestdate_poly_gdf = gpd.GeoDataFrame(data={\"id\": values}, geometry=polygons, crs=input_crs)\n",
    "\n",
    "#                 ds['geometry'] = (('y', 'x'), pp)\n",
    "\n",
    "#             results2[k][kk]['geometry'] = (('y', 'x'), pp)\n",
    "#             results2[k][kk].set_coords('geometry')\n",
    "\n",
    "            \n",
    "#             ## Append [geom, unique pixel id] to list for later addition to gdf, merging on geom\n",
    "#             ## CHECK THAT THE X'S AND Y'S ARE THE CORRECT WAY AROUND FOR MERGING LATER ON\n",
    "#             pxid2 = []\n",
    "#             pgid2 = []\n",
    "            \n",
    "#             for x in ds['x']:\n",
    "#                 for y in ds['y']:\n",
    "# #                     pxid1 = str(k)+str(kk)+str(ds['y'][y].item())+str(ds['x'][x].item())\n",
    "#                     pxid1 = str(attrs['pgid'])+'_'+str(ds['y'][y].item())+'_'+str(ds['x'][x].item())\n",
    "#                     gm = pp[ds['y'][y].item()][ds['x'][x].item()]\n",
    "#     #                 pxid.append([pxid1, gm])\n",
    "#                     uniquepx.append(pxid1)\n",
    "#                     pxgeom.append(gm)\n",
    "#                     pxid2.append(pxid1)\n",
    "#                     pgid2.append(attrs['pgid'])\n",
    "            \n",
    "#             pxid2 = np.array(pxid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "#             pgid2 = np.array(pgid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "#             results2[k][kk]['pxid'] = (('y', 'x'), pxid2)\n",
    "            \n",
    "#             ds['pgid'] = (('y', 'x'), pgid2)\n",
    "            \n",
    "#             ## Reattach attrs\n",
    "# #             ds.attrs = attrs\n",
    "#             results2[k][kk].attrs = attrs\n",
    "                \n",
    "#             pxsummary[k][kk] = ds\n",
    "\n",
    "# #             uniquepx1 = (np.array(uniquepx)).reshape(ds.y.values, ds.x.values)\n",
    "# #             results2[k][kk]['pxid'] = (('y', 'x'), uniquepx1)\n",
    "\n",
    "#             ## Append ds to df_pxsummary\n",
    "#             ds = ds.to_dataframe()\n",
    "#             df_pxsummary = df_pxsummary.append(ds)\n",
    "#             df_pxsummary.crs = gdf.crs\n",
    "            \n",
    "#     # pxsummary\n",
    "\n",
    "#     pxid = gpd.GeoDataFrame(uniquepx, geometry=pxgeom, columns=['pxid'], crs=input_crs)\n",
    "\n",
    "#     df_pxsummary.reset_index(inplace=True)\n",
    "\n",
    "#     df_pxsummary1 = df_pxsummary.merge(pxid)\n",
    "#     df_pxsummary1.set_index('pxid', inplace=True, drop=False)\n",
    "#     df_pxsummary1.dropna(axis=0, inplace=True)\n",
    "#     df_pxsummary1.sort_index(level='pxid', inplace=True)\n",
    "\n",
    "#     return df_pxsummary1, pxsummary, results2\n",
    "\n",
    "# # For every polygon, extract list pairs on the polygon id and temporal mean and std of the nominated indice\n",
    "# def zonal(results2, index_ls, gdf):\n",
    "    \n",
    "# #     indexstats = pd.DataFrame() ## New\n",
    "#     gdf_zonal = gdf\n",
    "    \n",
    "#     ##  Generate a list of keys for polygons in class '0' of results2\n",
    "#     keylist = list(results2['0'].keys())\n",
    "\n",
    "#     ##  For each nominated indice:\n",
    "#     for var in index_ls:\n",
    "#         if var in results2[\"0\"][keylist[0]].var():\n",
    "            \n",
    "#             lsmean = []\n",
    "#             lsstd = []\n",
    "\n",
    "#             ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "#             ##  Append polygon id and temporal statistic to gdf\n",
    "#             for k in results2:\n",
    "\n",
    "#                 for kk in results2[k]:\n",
    "#                     temporalmean = (results2[k][kk][var].mean('y').mean('x')).mean()\n",
    "#                     lsmean.append([int(kk), temporalmean.values])\n",
    "\n",
    "#                     temporalstd = (results2[k][kk][var].std('y').std('x')).std()\n",
    "#                     lsstd.append([int(kk), temporalstd.values])                \n",
    "\n",
    "#             ##  Sort the list by polygon id to match up to the original polygon gdf\n",
    "#             lsmean = sorted(lsmean)\n",
    "#             lsstd = sorted(lsstd)\n",
    "\n",
    "#             ##  Separate the sorted polygon ids from the indice statistic to build into a pd.DataFrame\n",
    "#             indicemean = []\n",
    "#             indicestd = []\n",
    "#             polyid = []\n",
    "\n",
    "#             for x in lsmean:\n",
    "#                 polyid.append(x[0])\n",
    "#                 indicemean.append(x[1])\n",
    "\n",
    "#             for x in lsstd:\n",
    "#                 indicestd.append(x[1])\n",
    "\n",
    "#             # Build a pd.DataFrame from the sorted polygon id and indice statistics. Nominate a name for the new column.\n",
    "#             indexstats = pd.DataFrame(\n",
    "#                 indicemean, index=polyid, columns=[str(var) + \" zonal mean\"]\n",
    "#             )\n",
    "# #             indexstats[str(var) + ' zonal mean'] = indicemean\n",
    "# #             indexstats['polyid'] = polyid \n",
    "            \n",
    "            \n",
    "#             indexstats[str(var) + \" zonal std\"] = None\n",
    "#             indexstats.loc[polyid, (str(var) + \" zonal std\")] = indicestd\n",
    "\n",
    "#             # Workaround to handle automatic addition of key_0 column at merge step\n",
    "#             if \"key_0\" in gdf_zonal.columns:\n",
    "#                 gdf_zonal.drop(columns=[\"key_0\"], inplace=True)\n",
    "\n",
    "#             # # Merge the indice statistic for each polygon into the original polygon gdf\n",
    "#             gdf_zonal = gdf_zonal.merge(indexstats, on=indexstats.index)\n",
    "            \n",
    "#     ## Rename the 'index' column to avoid confusion with the gdf.index\n",
    "#     gdf_zonal.rename(columns={'index' : 'pgid'}, inplace=True)\n",
    "#     gdf_zonal.rename(columns={'geometry' : 'pg_geometry'}, inplace=True)\n",
    "\n",
    "#     return gdf_zonal, indexstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy of temporal_stats func. Can be deleted after successful run through of notebook ~ CP 18/02/2021\n",
    "\n",
    "# def temporal_stats (gdf, \n",
    "#                     results2,\n",
    "#                     zonal=True,\n",
    "#                     pixel=True):\n",
    "\n",
    "#     '''\n",
    "#     This function calculates and returns the temporal mean and standard deviation\n",
    "#     for a range of indices for every polygon and every pixel within\n",
    "#     every polygon in the form of a geodataframe.  \n",
    "#     For now, leave zonal and pixel set to True to calculate all results. Func may\n",
    "#     not work if either of these vars are set to False.\n",
    "#     '''\n",
    "#     ##  Spectral indices available in the `calculate_indices` function (dea_bandindices.py)\n",
    "#     index_ls = [\n",
    "#         \"NDVI\",\n",
    "#         \"EVI\",\n",
    "#         \"NDAVI\", \n",
    "#         \"WAVI\",\n",
    "#         \"LAI\",\n",
    "#         \"SAVI\",\n",
    "#         \"MSAVI\",\n",
    "#         \"NDMI\",\n",
    "#         \"NBR\",\n",
    "#         \"BAI\",\n",
    "#         \"NDCI\",\n",
    "#         \"NDSI\",\n",
    "#         \"NDTI\",\n",
    "#         \"NDWI\",\n",
    "#         \"MNDWI\",\n",
    "#         \"NDBI\",\n",
    "#         \"BUI\",\n",
    "#         \"BAEI\",\n",
    "#         \"NBI\",\n",
    "#         \"BSI\",\n",
    "#         \"AWEI_ns\",\n",
    "#         \"AWEI_sh\",\n",
    "#         \"WI\",\n",
    "#         \"TCW\",\n",
    "#         \"TCG\",\n",
    "#         \"TCB\",\n",
    "#         \"CMR\",\n",
    "#         \"FMR\",\n",
    "#         \"IOR\",\n",
    "#     ]\n",
    "\n",
    "       \n",
    "    \n",
    "#     ## Calculate zonal polygon results\n",
    "#     if zonal:\n",
    "        \n",
    "#         gdf_zonal = gdf \n",
    "        \n",
    "#         ##  Generate a list of keys for polygons in class '0' of results2\n",
    "#         keylist = list(results2['0'].keys())\n",
    "\n",
    "#         ##  For each nominated indice:\n",
    "#         for var in index_ls:\n",
    "#             if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "#                 lsmean = []\n",
    "#                 lsstd = []\n",
    "\n",
    "#                 ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "#                 ##  Append polygon id and temporal statistic to gdf\n",
    "#                 for k in results2:\n",
    "\n",
    "#                     for kk in results2[k]:\n",
    "#                         temporalmean = (results2[k][kk][var].mean('y').mean('x')).mean()\n",
    "#                         lsmean.append([int(kk), temporalmean.values])\n",
    "#                         name1 = str(var) + ' zonal mean'\n",
    "#                         results2[str(k)][str(kk)][name1] = (temporalmean.values)\n",
    "                        \n",
    "#                         temporalstd = (results2[k][kk][var].std('y').std('x')).std()\n",
    "#                         lsstd.append([int(kk), temporalstd.values])  \n",
    "#                         name2 = str(var) + ' zonal std'\n",
    "#                         results2[str(k)][str(kk)][name2] = (temporalstd.values)                        \n",
    "\n",
    "#                 ##  Sort the list by polygon id to match up to the original polygon gdf\n",
    "#                 lsmean = sorted(lsmean)\n",
    "#                 lsstd = sorted(lsstd)\n",
    "\n",
    "#                 ##  Separate the sorted polygon ids from the indice statistic to build into a pd.DataFrame\n",
    "#                 indicemean = []\n",
    "#                 indicestd = []\n",
    "#                 polyid = []\n",
    "\n",
    "#                 for x in lsmean:\n",
    "#                     polyid.append(x[0])\n",
    "#                     indicemean.append(x[1])\n",
    "\n",
    "#                 for x in lsstd:\n",
    "#                     indicestd.append(x[1])\n",
    "\n",
    "#                 # Build a pd.DataFrame from the sorted polygon id and indice statistics. \n",
    "#                 # Nominate a name for the new column.\n",
    "#                 indexstats = pd.DataFrame(\n",
    "#                     indicemean, index=polyid, columns=[str(var) + \" zonal mean\"]\n",
    "#                 )\n",
    "\n",
    "# #                 indexstats[str(var) + ' zonal mean'] = indicemean\n",
    "# #                 indexstats['polyid'] = polyid \n",
    "    \n",
    "#                 indexstats[str(var) + \" zonal std\"] = None\n",
    "#                 indexstats.loc[polyid, (str(var) + \" zonal std\")] = indicestd\n",
    "\n",
    "#                 # Workaround to handle automatic addition of key_0 column at merge step\n",
    "#                 if \"key_0\" in gdf_zonal.columns:\n",
    "#                     gdf_zonal.drop(columns=[\"key_0\"], inplace=True)\n",
    "\n",
    "#                 # # Merge the indice statistic for each polygon into the original polygon gdf\n",
    "#                 gdf_zonal = gdf_zonal.merge(indexstats, on=indexstats.index)\n",
    "\n",
    "#         ## Rename the 'index' column to avoid confusion with the gdf.index\n",
    "#         gdf_zonal.rename(columns={'index' : 'pgid'}, inplace=True)\n",
    "#         gdf_zonal.rename(columns={'geometry' : 'pg_geometry'}, inplace=True)\n",
    "\n",
    "# #                         return gdf\n",
    "\n",
    "#     ## Calculate pixel results\n",
    "#     if pixel:\n",
    "\n",
    "#         ## New dict to store arrays\n",
    "# #         pxsummary = {}\n",
    "\n",
    "#         ## Master gdf\n",
    "#         gdf_px = gpd.GeoDataFrame()\n",
    "#         gdf_px['geometry'] = None\n",
    "\n",
    "#         ## List to store geom, unique pixel id \n",
    "#         # pxid = []\n",
    "#         uniquepx = []\n",
    "#         pxgeom = []\n",
    "\n",
    "#         ##  Generate a list of keys for polygons in class '0' of results2\n",
    "#         keylist = list(results2['0'].keys())\n",
    "\n",
    "\n",
    "#         ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "#         ##  Append polygon id and temporal statistic to gdf\n",
    "#         for k in results2:\n",
    "\n",
    "# #             if not (str(k) in pxsummary.keys()):\n",
    "# #                 pxsummary[str(k)] = {}\n",
    "\n",
    "#             for kk in results2[k]:\n",
    "\n",
    "#                 lon = []\n",
    "#                 for value in results2[k][kk].x.values:\n",
    "#                     lon.append(value)\n",
    "\n",
    "#                 lat = []\n",
    "#                 for value in results2[k][kk].y.values:\n",
    "#                     lat.append(value)\n",
    "\n",
    "#                 ds = xr.Dataset()\n",
    "\n",
    "#                 ##  For each nominated indice:\n",
    "#                 for var in index_ls:\n",
    "#                     if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "#                         temporalmean = (results2[str(k)][str(kk)][var].mean(dim='time'))\n",
    "#                         name1 = str(var) + ' px mean'\n",
    "#                         ds[name1] = (('y', 'x'), temporalmean)\n",
    "#                         results2[str(k)][str(kk)][name1] = (('y', 'x'), temporalmean)\n",
    "\n",
    "#                         temporalstd = (results2[k][kk][var].std(dim='time'))\n",
    "#                         name2 = str(var) + ' px std'\n",
    "#                         ds[name2] = (('y', 'x'), temporalstd)\n",
    "#                         results2[str(k)][str(kk)][name2] = (('y', 'x'), temporalstd)\n",
    "\n",
    "#                 ds.coords['lon'] = ('x'), lon\n",
    "#                 ds.coords['lat'] = ('y'), lat\n",
    "\n",
    "\n",
    "#                 ## Extract pixel geometry/shape for input into gdf for choropleth plotting\n",
    "#                 # Extract dataset matching polygon\n",
    "#                 closest_ds = results2[k][kk]\n",
    "\n",
    "#                 ## Extract pgid attributes to attach later\n",
    "#                 attrs = closest_ds.attrs\n",
    "#                 ds.attrs = attrs\n",
    "\n",
    "#                 ## Skip empty arrays\n",
    "#                 if closest_ds.x.size == 0:\n",
    "#                     print('Empty arrays: k: ', k, 'kk: ', kk)\n",
    "#                     continue\n",
    "#                 else:       \n",
    "#                     # Input array (based on red band) to segment and vectorise\n",
    "#                     input_array = closest_ds.nbart_red\n",
    "#                     input_transform = closest_ds.affine  \n",
    "#                     input_crs = input_array.crs\n",
    "\n",
    "#                     # Create array with a unique value per cell\n",
    "#                     unique_pixels = np.arange(input_array.size).reshape(input_array.shape)\n",
    "\n",
    "#                     # Vectorise each unique feature in array\n",
    "#                     vectors = rasterio.features.shapes(\n",
    "#                         source=unique_pixels.astype(np.int16), transform=input_transform\n",
    "#                     )\n",
    "\n",
    "#                     # Extract polygons and values from generator\n",
    "#                     vectors = list(vectors)\n",
    "#                     values = [value for polygon, value in vectors]\n",
    "#                     polygons = [shape(polygon) for polygon, value in vectors]\n",
    "#                     pp = np.array(polygons)\n",
    "#                     pp = pp.reshape(len(input_array.y), len(input_array.x))\n",
    "\n",
    "#             #         # Create a geopandas dataframe populated with the polygon shapes\n",
    "#             #         closestdate_poly_gdf = gpd.GeoDataFrame(data={\"id\": values}, geometry=polygons, crs=input_crs)\n",
    "\n",
    "#                     ds['geometry'] = (('y', 'x'), pp)\n",
    "\n",
    "#                 results2[k][kk]['geometry'] = (('y', 'x'), pp)\n",
    "#                 results2[k][kk].set_coords('geometry')\n",
    "\n",
    "\n",
    "#                 ## Append [geom, unique pixel id] to list for later addition to gdf, merging on geom\n",
    "#                 pxid2 = []\n",
    "#                 pgid2 = []\n",
    "\n",
    "#                 for x in ds['x']:\n",
    "#                     for y in ds['y']:\n",
    "#     #                     pxid1 = str(k)+str(kk)+str(ds['y'][y].item())+str(ds['x'][x].item())\n",
    "#                         pxid1 = str(attrs['pgid'])+'_'+str(ds['y'][y].item())+'_'+str(ds['x'][x].item())\n",
    "#                         gm = pp[ds['y'][y].item()][ds['x'][x].item()]\n",
    "#         #                 pxid.append([pxid1, gm])\n",
    "#                         uniquepx.append(pxid1)\n",
    "#                         pxgeom.append(gm)\n",
    "#                         pxid2.append(pxid1)\n",
    "#                         pgid2.append(attrs['pgid'])\n",
    "\n",
    "#                 pxid2 = np.array(pxid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "#                 pgid2 = np.array(pgid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "#                 results2[k][kk]['pxid'] = (('y', 'x'), pxid2)\n",
    "\n",
    "#                 ds['pgid'] = (('y', 'x'), pgid2)\n",
    "\n",
    "#                 ## Reattach attrs\n",
    "#                 results2[k][kk].attrs = attrs\n",
    "\n",
    "# #                 pxsummary[k][kk] = ds\n",
    "\n",
    "#                 ## Append ds to df_pxsummary\n",
    "#                 ds = ds.to_dataframe()\n",
    "#                 gdf_px = gdf_px.append(ds)\n",
    "#                 gdf_px.crs = gdf.crs\n",
    "\n",
    "#         pxid = gpd.GeoDataFrame(uniquepx, geometry=pxgeom, columns=['pxid'], crs=input_crs)\n",
    "\n",
    "#         gdf_px.reset_index(inplace=True)\n",
    "\n",
    "#         gdf_px = gdf_px.merge(pxid)\n",
    "#         gdf_px.set_index('pxid', inplace=True, drop=False)\n",
    "#         gdf_px.dropna(axis=0, inplace=True)\n",
    "#         gdf_px.sort_index(level='pxid', inplace=True)\n",
    "\n",
    "\n",
    "#     ## Merge the (zonal) gdf and gdf_px\n",
    "#     gdf_merged = gdf_px.merge(gdf_zonal, on = 'pgid')\n",
    "\n",
    "#     ## Drop the pg_geometry to use the pixel geometry default\n",
    "#     gdf_merged.drop(columns='pg_geometry', inplace=True)\n",
    "\n",
    "#     ##  Create new gdf's for each class to plot\n",
    "#     grasses = gdf_merged.drop(\n",
    "#         gdf_merged[gdf_merged.BRD_HAB != \"Intertidal grass-herb-sedge-other succulent\"].index\n",
    "#     )\n",
    "#     mangroves = gdf_merged.drop(\n",
    "#         gdf_merged[gdf_merged.BRD_HAB != \"Intertidal mangroves and other trees & shrubs\"].index\n",
    "#     )\n",
    "#     seagrass = gdf_merged.drop(gdf_merged[gdf_merged.BRD_HAB != \"Intertidal seagrass\"].index)\n",
    "\n",
    "#     ##  Drop polygons containing NaN values\n",
    "#     ##  ASSUMPTION: if NDVI contains NaNs, all indices will contain NaNs. \n",
    "#     ##  ToDo: write a loop or func that looks for NaNs in any of the supplied indices\n",
    "#     grasses = grasses.dropna(axis=0, how='any', subset=[\n",
    "#                                                'NDVI zonal mean', \n",
    "#                                                'NDVI px mean', \n",
    "#                                                'NDVI zonal std', \n",
    "#                                                'NDVI px std'])\n",
    "#     mangroves = mangroves.dropna(axis=0, how='any', subset=[\n",
    "#                                                'NDVI zonal mean', \n",
    "#                                                'NDVI px mean', \n",
    "#                                                'NDVI zonal std', \n",
    "#                                                'NDVI px std'])\n",
    "#     seagrass = seagrass.dropna(axis=0, how='any', subset=[\n",
    "#                                                'NDVI zonal mean', \n",
    "#                                                'NDVI px mean', \n",
    "#                                                'NDVI zonal std', \n",
    "#                                                'NDVI px std']) \n",
    "\n",
    "#     return gdf_merged, results2, grasses, mangroves, seagrass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the shapefile\n",
    "# print('Select attribute from: ', seagrass.columns)\n",
    "# roi = map_shapefile(seagrass, attribute=\"NDAVI px std\", continuous=True) #CHECK THAT THE LEGEND AND POP UP WINDOWS ARE STILL WORKING AS INTENDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End testing zone :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate zonal and temporal stats for each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate zonal and pixel stats and attach to class polygons\n",
    "gdf_merged, results2, grasses, mangroves, seagrass = temporal_stats(gdf, \n",
    "                                                                    results2, \n",
    "                                                                    mask='lowest_20_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the zonal statistics temporal summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select attribute from:  Index(['index', 'geometry', 'NDVI px mean', 'NDVI px std', 'EVI px mean',\n",
      "       'EVI px std', 'NDAVI px mean', 'NDAVI px std', 'WAVI px mean',\n",
      "       'WAVI px std', 'SAVI px mean', 'SAVI px std', 'NDWI px mean',\n",
      "       'NDWI px std', 'MNDWI px mean', 'MNDWI px std', 'lon', 'lat', 'pgid',\n",
      "       'pxid', 'key_0', 'OBJECTID', 'CONSOL', 'DOM_TYPE', 'DOM_LABEL',\n",
      "       'CO_TYPES', 'TIDE_ZONE', 'BRD_HAB', 'Shape_Leng', 'Shape_Area', 'id',\n",
      "       'NDVI zonal mean', 'NDVI zonal std', 'EVI zonal mean', 'EVI zonal std',\n",
      "       'NDAVI zonal mean', 'NDAVI zonal std', 'WAVI zonal mean',\n",
      "       'WAVI zonal std', 'SAVI zonal mean', 'SAVI zonal std',\n",
      "       'NDWI zonal mean', 'NDWI zonal std', 'MNDWI zonal mean',\n",
      "       'MNDWI zonal std'],\n",
      "      dtype='object')\n",
      "        index                                           geometry  \\\n",
      "1394  (13, 0)  POLYGON ((1958520.000 -2749350.000, 1958520.00...   \n",
      "1395   (6, 0)  POLYGON ((1958310.000 -2749350.000, 1958310.00...   \n",
      "\n",
      "      NDVI px mean  NDVI px std  EVI px mean  EVI px std  NDAVI px mean  \\\n",
      "1394      0.493296     0.054165     0.227642    0.020183       0.611475   \n",
      "1395      0.263509     0.032634     0.127501    0.011550       0.415492   \n",
      "\n",
      "      NDAVI px std  WAVI px mean  WAVI px std  ...  NDAVI zonal mean  \\\n",
      "1394      0.097031      0.264431     0.032788  ...          0.403437   \n",
      "1395      0.068864      0.181909     0.025597  ...          0.403437   \n",
      "\n",
      "      NDAVI zonal std  WAVI zonal mean  WAVI zonal std  SAVI zonal mean  \\\n",
      "1394         0.007031         0.184731         0.00275          0.12674   \n",
      "1395         0.007031         0.184731         0.00275          0.12674   \n",
      "\n",
      "      SAVI zonal std  NDWI zonal mean  NDWI zonal std  MNDWI zonal mean  \\\n",
      "1394        0.002934        -0.293464        0.006089         -0.271223   \n",
      "1395        0.002934        -0.293464        0.006089         -0.271223   \n",
      "\n",
      "     MNDWI zonal std  \n",
      "1394        0.002356  \n",
      "1395        0.002356  \n",
      "\n",
      "[2 rows x 45 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d009a3c49aba40de88424ec4a0b09444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.079586567766732, 151.5424156014604], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Plot grasses\n",
    "print('Select attribute from: ', gdf_merged.columns)\n",
    "\n",
    "map_shapefile(grasses, attribute=\"NDVI px std\", continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fba499173f144c2bb1f03f1ed35ac1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.080422454151574, 151.54197348550593], controls=(ZoomControl(options=['position', 'zoom_in_text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Plot mangroves\n",
    "map_shapefile(mangroves, attribute=\"NDVI px std\", continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855af84481b84bf69aed9a2a90fd8675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.076897322880512, 151.5419280732554], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Plot seagrass\n",
    "map_shapefile(seagrass, attribute=\"NDVI px mean\", continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Per polygon spatiotemporal analysis\n",
    "\n",
    "For a user defined polygon (search the maps above to identify polygon/s of interest), show a stacked bar graph (WIT style)\n",
    "that groups the pixels in the polygon into major classes/cover types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for code development only: update to `results2` upon successful workflow\n",
    "## TODO: code successful. Update code to use `results2` variable and remove `results3` variable\n",
    "\n",
    "results3 = results2.copy()\n",
    "\n",
    "results2 == results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ******From the interactive zonal summary plots above, IDENTIFY A POLYGON to interrogate*******\n",
    "pg = 3636\n",
    "\n",
    "cl = gdf.loc[gdf['index'] == pg].id.values.item()\n",
    "## class polygon\n",
    "pg1 = gdf.loc[gdf['index'] == pg].index.values.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot interactive WIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coastal_wit func call\n",
    "classes_df = coastal_wit(gdf=gdf,\n",
    "               results3=results3,\n",
    "               pg=pg, \n",
    "               water=True, \n",
    "               drop=False,\n",
    "               pc_drop=80,\n",
    "               px_min=20)     \n",
    "\n",
    "\n",
    "##  Duplicated cell interactive stacked plot\n",
    "%matplotlib widget\n",
    "pal = [\n",
    "       sns.xkcd_rgb[\"cobalt blue\"],\n",
    "       sns.xkcd_rgb[\"beige\"],\n",
    "       sns.xkcd_rgb[\"light green\"],\n",
    "       sns.xkcd_rgb[\"green\"],\n",
    "       sns.xkcd_rgb[\"dark green\"]]\n",
    "\n",
    "plt.clf()\n",
    "plt.close(fig=None)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), constrained_layout=True)\n",
    "gs = fig.add_gridspec(8,1)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0:2, :])\n",
    "ax1.set_title('% of polygon exposed')\n",
    "ax2 = fig.add_subplot(gs[4:, :])\n",
    "ax2.set_title('% cover in exposed polygon')\n",
    "ax3 = fig.add_subplot(gs[2:4, :])\n",
    "ax3.set_title('modelled tide height')\n",
    "\n",
    "ax2.stackplot(classes_df.index, \n",
    "              classes_df['pc_water'], \n",
    "              classes_df['pc_unveg'],\n",
    "              classes_df['pc_ndvi_low'], \n",
    "              classes_df['pc_ndvi_mid'], \n",
    "              classes_df['pc_ndvi_high'],\n",
    "              labels=[\n",
    "                  'water',\n",
    "                  'unveg',\n",
    "                  'low veg',\n",
    "                  'medium veg',\n",
    "                  'dense veg',\n",
    "                 ], \n",
    "              baseline='zero',\n",
    "              colors=pal, \n",
    "              alpha = 0.6\n",
    "             )\n",
    "\n",
    "ax1.plot(classes_df.index, \n",
    "        classes_df['pc_exposedpx'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "ax3.plot(classes_df.index, \n",
    "        classes_df['tide_height'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "\n",
    "# plt.ylim((0,100))\n",
    "ax1.set_ylim([0,100])\n",
    "ax3.set_ylim([-1,0])\n",
    "\n",
    "#add a legend and a tight plot box\n",
    "ax2.legend(loc='best', framealpha=0.0)#, bbox_to_anchor=(1.00,1.00))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Run on click event\n",
    "w2 = widgets.HTML(\"Click on the pixel you would like to interrogate\")\n",
    "ka = fig.canvas.mpl_connect('button_press_event', onclick_timeseries)\n",
    "display(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial WIT equivalent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polygonise pixel edges as per Bex's `Polygonise_pixel_edges.ipynb`\n",
    "\n",
    "## Extract dataset matching polygon and date selected from WIT plot\n",
    "sWIT = results3[str(cl)][str(pg1)].isel(time=TimeIndex)\n",
    "\n",
    "## Colour palette as per WIT\n",
    "ndvi_pal = [\n",
    "#        sns.xkcd_rgb[\"cobalt blue\"],\n",
    "       sns.xkcd_rgb[\"beige\"],\n",
    "       sns.xkcd_rgb[\"light green\"],\n",
    "       sns.xkcd_rgb[\"green\"],\n",
    "       sns.xkcd_rgb[\"dark green\"]]\n",
    "mndwi_pal = [\n",
    "    sns.xkcd_rgb[\"white\"],\n",
    "    sns.xkcd_rgb[\"cobalt blue\"]]\n",
    "\n",
    "# Plot raster data\n",
    "plt.clf()\n",
    "plt.close()#'all')\n",
    "\n",
    "fig, ax1 = plt.subplots()#figsize=[8, 8])\n",
    "\n",
    "sWIT.MNDWI.where(sWIT.MNDWI > 0).plot(ax=ax1, levels=[-1, 0, 1], colors=mndwi_pal)\n",
    "sWIT.NDVI.where(sWIT.mask).plot(levels=[0, 0.1, 0.33, 0.66, 1], colors = ndvi_pal, ax=ax1)\n",
    "\n",
    "mpl.axes.Axes.set_aspect(ax1, aspect=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select regions to plot and explore change\n",
    "\n",
    "TODO: Use `map_shapefile` changes (specifically the roi functionality) to subset the gdf and process intersected data from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal pixel means/stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the temporal per-pixel stats for the time series\n",
    "df_pxsummary1, pxsummary, results2 = px_heatmap(results2, index_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Select attribute from: ', df_pxsummary1.columns)\n",
    "roi = map_shapefile(df_pxsummary1, attribute='NDAVI px mean', continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniWIT\n",
    "Select a region of interest from the temporal per pixel stats map to investigate using a WIT style stacked line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Form a shapely polygon from the coordinates defined by the user on the map\n",
    "miniwit_roi = Polygon(roi[-1]['geometry']['coordinates'][0])\n",
    "\n",
    "##  Generate a new geodataframe containing the user defined polygon geometry\n",
    "miniwit_df = gpd.GeoDataFrame(gpd.GeoSeries(miniwit_roi), columns=['geometry'], crs='EPSG:4326')\n",
    "miniwit_df = miniwit_df.to_crs(gdf.crs)\n",
    "\n",
    "##  Intersect the user-defined region of interest with the master\n",
    "##  to create the working gdf from which imagery will be extracted\n",
    "miniwit_gdf = gpd.overlay(df_pxsummary1, miniwit_df, how='intersection')\n",
    "print('This selection includes ', len(miniwit_gdf), ' individual polygons')\n",
    "miniwit_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsample the original gdf with the miniwit_df to extract the polygon class and unique polygon id to enable faster pixel searching within the corresponding xarray.\n",
    "from pprint import pprint\n",
    "\n",
    "polyid_gdf = gpd.overlay(miniwit_df, gdf, how='intersection')\n",
    "\n",
    "## Check the number of class polygons that are included in the roi selection\n",
    "\n",
    "for x in range(0,len(polyid_gdf['index'])):\n",
    "    print (x)\n",
    "    pprint(polyid_gdf.iloc[x]['geometry'])\n",
    "    print ('........')\n",
    "\n",
    "## If more than 1, check the shape matches your targeted class polygon by substituting\n",
    "## x into the call to `polyid_gdf.iloc[]['geometry']` call\n",
    "\n",
    "polyid_gdf.iloc[0]['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Substitute the x value that represents your targeted polygon into the `pg = ...iloc[]['index']` call below to plot your pixel selection\n",
    "\n",
    "## From the results2 dataset, the temporal values for the nominated polygon are stored in the following dataset of the resutls2 dict:\n",
    "pg = polyid_gdf.iloc[0]['index']\n",
    "## class key\n",
    "cl = polyid_gdf.loc[polyid_gdf['index'] == pg].id.values.item()\n",
    "## class polygon\n",
    "# pg1 = polyid_gdf.loc[polyid_gdf['index'] == pg].index.values.item()\n",
    "pg1 = polyid_gdf.loc[polyid_gdf['index'] == pg].key_0.values.item()\n",
    "\n",
    "print('pg: ', pg, ' cl: ', cl, ' pg1: ', pg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that I know my target polygon, I can intersect the geometries (somehow) of the pixel roi \n",
    "## with the pixels in the xarray and prepare them as per WIT above\n",
    "\n",
    "mWIT = (results2[str(cl)][str(pg1)]).set_coords('geometry')\n",
    "\n",
    "## 1D list of all geometries in class polygon of interest\n",
    "a = mWIT['geometry'].stack(z=(\"y\",\"x\")).values\n",
    "\n",
    "## list polygon bounds(coords)\n",
    "aa = []\n",
    "for polygon in a:\n",
    "    aa.append(polygon.bounds)\n",
    "\n",
    "## Record mWIT shapes\n",
    "mw_yshape = mWIT.isel(time=0).dims['y']\n",
    "mw_xshape = mWIT.isel(time=0).dims['x']\n",
    "len_a = len(a)\n",
    "\n",
    "## Confirm that 2D coords match with 1D (stacked) coords (all print statements should read `True`)\n",
    "f = mWIT['geometry'].values\n",
    "# print(f[0][0].bounds == a[0].bounds)   \n",
    "# print(f[0][-1].bounds == a[mw_xshape-1].bounds)  \n",
    "# print(f[-1][0].bounds == a[-mw_xshape].bounds)\n",
    "# print(f[-1][-1].bounds == a[-1].bounds) \n",
    "\n",
    "## 1D list of all geometries in pixels-roi\n",
    "b = np.asarray(miniwit_gdf['geometry'].values)\n",
    "len_b = len(b)\n",
    "\n",
    "## list polygon bounds(coords)\n",
    "bb=[]\n",
    "for polygon in b:\n",
    "    bb.append(polygon.bounds)\n",
    "\n",
    "## Find matching pixels from roi in the class polygon\n",
    "## For every pixel in the class polygon, compare against every pixel in roi.\n",
    "## Store boolean results in px_df\n",
    "px_ls = []\n",
    "\n",
    "for val in bb:\n",
    "#     print(val)\n",
    "    for value in aa:\n",
    "        px = np.all(np.isin(val,value))\n",
    "        px_ls.append(px)\n",
    "\n",
    "# reshape boolean results\n",
    "px_array = np.array(px_ls)\n",
    "px_array = px_array.reshape(len_b, len_a)#######\n",
    "px_df = pd.DataFrame(px_array)\n",
    "\n",
    "## Summarise px_df results into a single `array2`\n",
    "## Record `True` for every pixel in the class polygon that also appears in the roi set of pixels. Else, record `False` \n",
    "array2 = []\n",
    "\n",
    "for x in range(0,len(px_df.iloc[0])):\n",
    "#     print (row)\n",
    "    if np.any(px_df[x]) == True:\n",
    "        array2.append('True')\n",
    "    else:\n",
    "        array2.append('False')\n",
    "    \n",
    "## Reshape and merge `array2` booleans into mWIT\n",
    "mwpx_array = np.array(array2)\n",
    "mwpx_array = mwpx_array.reshape(mw_yshape, mw_xshape)\n",
    "mWIT['miniwit px'] = (('y', 'x'), mwpx_array)\n",
    "\n",
    "## Mask mWIT to show only roi pixels\n",
    "mWIT1 = mWIT.where(mWIT['miniwit px'] == 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plot full class polygon vs the roi pixels\n",
    "# %matplotlib inline\n",
    "\n",
    "# t=6\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=2, sharey=True, figsize=[16,5])\n",
    "\n",
    "# mWIT.isel(time=t)['mndwi_water'].plot(ax=axes[0])\n",
    "# mWIT1.isel(time=t)['mndwi_water'].plot(ax=axes[1])\n",
    "\n",
    "# plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miniwit time!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coastal_wit func call\n",
    "classes_df = coastal_wit(gdf=miniwit_gdf,\n",
    "               results3=mWIT1,\n",
    "               pg=pg, \n",
    "               classes=True,\n",
    "               water=True, \n",
    "               drop=False,\n",
    "               pc_drop=80,\n",
    "               px_min=20)     \n",
    "\n",
    "\n",
    "##  Duplicated cell interactive stacked plot\n",
    "%matplotlib widget\n",
    "pal = [\n",
    "       sns.xkcd_rgb[\"cobalt blue\"],\n",
    "       sns.xkcd_rgb[\"beige\"],\n",
    "       sns.xkcd_rgb[\"light green\"],\n",
    "       sns.xkcd_rgb[\"green\"],\n",
    "       sns.xkcd_rgb[\"dark green\"]]\n",
    "\n",
    "plt.clf()\n",
    "plt.close(fig=None)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), constrained_layout=True)\n",
    "gs = fig.add_gridspec(8,1)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0:2, :])\n",
    "ax1.set_title('% of polygon exposed')\n",
    "ax2 = fig.add_subplot(gs[4:, :])\n",
    "ax2.set_title('% cover in exposed polygon')\n",
    "ax3 = fig.add_subplot(gs[2:4, :])\n",
    "ax3.set_title('modelled tide height')\n",
    "\n",
    "ax2.stackplot(classes_df.index, \n",
    "              classes_df['pc_water'], \n",
    "              classes_df['pc_unveg'],\n",
    "              classes_df['pc_ndvi_low'], \n",
    "              classes_df['pc_ndvi_mid'], \n",
    "              classes_df['pc_ndvi_high'],\n",
    "              labels=[\n",
    "                  'water',\n",
    "                  'unveg',\n",
    "                  'low veg',\n",
    "                  'medium veg',\n",
    "                  'dense veg',\n",
    "                 ], \n",
    "              baseline='zero',\n",
    "              colors=pal, \n",
    "              alpha = 0.6\n",
    "             )\n",
    "\n",
    "ax1.plot(classes_df.index, \n",
    "        classes_df['pc_exposedpx'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "ax3.plot(classes_df.index, \n",
    "        classes_df['tide_height'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "\n",
    "# plt.ylim((0,100))\n",
    "ax1.set_ylim([0,100])\n",
    "ax3.set_ylim([-1,0])\n",
    "\n",
    "#add a legend and a tight plot box\n",
    "ax2.legend(loc='best', framealpha=0.0)#, bbox_to_anchor=(1.00,1.00))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Run on click event\n",
    "w2 = widgets.HTML(\"Click on the pixel you would like to interrogate\")\n",
    "ka = fig.canvas.mpl_connect('button_press_event', onclick_timeseries)\n",
    "display(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITEM masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2\n",
    "\n",
    "## Plan:\n",
    "## Create a dummy dataset to prototype then build it into a loop to generate for all datasets in the\n",
    "## `results2` xarray dict\n",
    "\n",
    "ds = results2['2']['6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_intervals = (ds[['MNDWI', 'tide_height']]\n",
    "                     .median(dim='time'))\n",
    "landsat_intervals.MNDWI.plot(cmap='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# LOT_mask = landsat_intervals.where(landsat_intervals.MNDWI>= 0)\n",
    "ds['NDVI'].isel(time=0).where(landsat_intervals.MNDWI<= 0).plot()\n",
    "# LOT_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(landsat_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(landsat_intervals.where(landsat_intervals.MNDWI >= 0)).plot()\n",
    "\n",
    "# landsat_intervals\n",
    "mask.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "045ab485daf24868b7c2382f424cbcb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b0ea6054510a498e8a7c56ce35cc5c4c",
       "style": "IPY_MODEL_f660eac494c147198a27997ad8b22470"
      }
     },
     "0746427347e2416f920f9ce96228b721": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletGeoJSONModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "data": {
        "bbox": [
         142.4524578701443,
         -32.36320549945,
         142.54736615559136,
         -32.29586369982442
        ],
        "features": [
         {
          "bbox": [
           142.51524944800926,
           -32.31838789697648,
           142.54736615559136,
           -32.29586369982442
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.51524944800926,
              -32.296091685646715
             ],
             [
              142.546601533663,
              -32.29586369982442
             ],
             [
              142.54736615559136,
              -32.3183698586848
             ],
             [
              142.51526741493365,
              -32.31838789697648
             ],
             [
              142.51524944800926,
              -32.296091685646715
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "0",
          "properties": {
           "id": 2,
           "style": {
            "color": "black",
            "fillColor": "#ffffcc",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         },
         {
          "bbox": [
           142.4524578701443,
           -32.36320549945,
           142.4845749551165,
           -32.34069269280065
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.4524578701443,
              -32.340907825281136
             ],
             [
              142.483823262827,
              -32.34069269280065
             ],
             [
              142.4845749551165,
              -32.36320063502121
             ],
             [
              142.45246271443352,
              -32.36320549945
             ],
             [
              142.4524578701443,
              -32.340907825281136
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "1",
          "properties": {
           "id": 1,
           "style": {
            "color": "black",
            "fillColor": "#800026",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         }
        ],
        "type": "FeatureCollection"
       },
       "style": {
        "fillOpacity": 0.8
       }
      }
     },
     "140008a05899493688edb1cfd07bb91d": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_8563b85ab05a465b900584182485dc98"
      }
     },
     "215e008ea30446418c9489a3fc2a3509": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "2f3cbd62daf645a4a47a3132fde027d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "600px",
       "width": "800px"
      }
     },
     "331f5be31bf049e4a4c2ab49668abcdd": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "472e3957cd724d3fb57312a3fa86dae1": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     },
     "8563b85ab05a465b900584182485dc98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8912f038ad0947a7ae08ed0ccdb15644": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapModel",
      "state": {
       "_dom_classes": [],
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "basemap": {
        "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
        "max_zoom": 20,
        "name": "Esri.WorldImagery",
        "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
       },
       "center": [
        -32.32953459963721,
        142.4999120128678
       ],
       "controls": [
        "IPY_MODEL_331f5be31bf049e4a4c2ab49668abcdd",
        "IPY_MODEL_472e3957cd724d3fb57312a3fa86dae1"
       ],
       "default_style": "IPY_MODEL_d14c0de5ce3b48bd9eb7729afbe8d701",
       "dragging_style": "IPY_MODEL_a6bbab8038c3446a93e3a549b8f8dd79",
       "east": 142.5685501098633,
       "fullscreen": false,
       "interpolation": "bilinear",
       "layers": [
        "IPY_MODEL_99119a1935514365b929a6d6c4b31a95",
        "IPY_MODEL_0746427347e2416f920f9ce96228b721"
       ],
       "layout": "IPY_MODEL_2f3cbd62daf645a4a47a3132fde027d0",
       "modisdate": "yesterday",
       "north": -32.28597166993233,
       "options": [
        "basemap",
        "bounce_at_zoom_limits",
        "box_zoom",
        "center",
        "close_popup_on_click",
        "double_click_zoom",
        "dragging",
        "fullscreen",
        "inertia",
        "inertia_deceleration",
        "inertia_max_speed",
        "interpolation",
        "keyboard",
        "keyboard_pan_offset",
        "keyboard_zoom_offset",
        "max_zoom",
        "min_zoom",
        "scroll_wheel_zoom",
        "tap",
        "tap_tolerance",
        "touch_zoom",
        "world_copy_jump",
        "zoom",
        "zoom_animation_threshold",
        "zoom_start"
       ],
       "south": -32.373002604986546,
       "style": "IPY_MODEL_215e008ea30446418c9489a3fc2a3509",
       "west": 142.4312210083008,
       "zoom": 13
      }
     },
     "99119a1935514365b929a6d6c4b31a95": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletTileLayerModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
       "base": true,
       "max_native_zoom": 18,
       "max_zoom": 20,
       "min_native_zoom": 0,
       "min_zoom": 1,
       "name": "Esri.WorldImagery",
       "no_wrap": false,
       "options": [
        "attribution",
        "detect_retina",
        "max_native_zoom",
        "max_zoom",
        "min_native_zoom",
        "min_zoom",
        "no_wrap",
        "tile_size"
       ],
       "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
      }
     },
     "a6bbab8038c3446a93e3a549b8f8dd79": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "move"
      }
     },
     "b0ea6054510a498e8a7c56ce35cc5c4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d14c0de5ce3b48bd9eb7729afbe8d701": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "f660eac494c147198a27997ad8b22470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
