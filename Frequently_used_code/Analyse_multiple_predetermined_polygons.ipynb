{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and run analysis on multiple polygons <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with both the `NCI` and `DEA Sandbox` environments\n",
    "* **Products used:** \n",
    "[ga_ls8c_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls8c_ard_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Many users need to run analyses on their own areas of interest. \n",
    "A common use case involves running the same analysis across multiple polygons in a vector file (e.g. ESRI Shapefile or GeoJSON). \n",
    "This notebook will demonstrate how to use a vector file and the Open Data Cube to extract satellite data from Digital Earth Australia corresponding to individual polygon geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "If we have a vector file containing multiple polygons, we can use the python package [geopandas](https://geopandas.org/) to open it as a `GeoDataFrame`. \n",
    "We can then iterate through each geometry and extract satellite data corresponding with the extent of each geometry. \n",
    "Further anlaysis can then be conducted on each resulting `xarray.Dataset`.\n",
    "\n",
    "We can retrieve data for each polygon, perform an analysis like calculating NDVI and plot the data.\n",
    "\n",
    "1. First we open the vector file as a `geopandas.GeoDataFrame`\n",
    "2. Iterate through each polygon in the `GeoDataFrame`, and extract satellite data from DEA\n",
    "3. Calculate NDVI as an example analysis on one of the extracted satellite timeseries\n",
    "4. Plot NDVI for the polygon extent\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Please note the use of `datacube.utils` package `geometry`: \n",
    "this is important for saving the coordinate reference system of the incoming shapefile in a format that the Digital Earth Australia query can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import datacube\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio.crs\n",
    "import rioxarray\n",
    "from datacube.utils import geometry\n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import datacube.utils.cog\n",
    "\n",
    "# import geopandas as gpd\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.cm\n",
    "import matplotlib.colors\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.ui\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "import skimage.color as colour\n",
    "import skimage.io\n",
    "import sklearn.metrics\n",
    "import xarray as xr\n",
    "\n",
    "# import sys\n",
    "# import datacube\n",
    "from datacube.storage.masking import make_mask\n",
    "\n",
    "# from dea_datahandling import load_ard\n",
    "from dea_bandindices import calculate_indices\n",
    "from dea_coastaltools import tidal_stats, tidal_tag\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_datahandling import array_to_geotiff, load_ard\n",
    "from dea_plotting import display_map, map_shapefile, rgb\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_temporaltools import time_buffer\n",
    "from IPython.display import display\n",
    "from odc.ui import with_ui_cbk\n",
    "from shapely.geometry import shape, Polygon\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# %matplotlib widget\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Connect to the datacube database to enable loading Digital Earth Australia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Analyse_multiple_polygons\")\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Load predetermined polygons and select a region of interest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User defines path to polygon vector file, file name and \n",
    "## the column name for unique integer identifiers for each vector object.\n",
    "%cd '/home/jovyan/dev/dea-notebooks/Claire/'\n",
    "vector_file = \"QISMCQ_polygons.shp\"\n",
    "attribute_col = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the polygon vector file\n",
    "gdf_master = gpd.read_file(vector_file)\n",
    "\n",
    "# Set the crs to match the imagery data retrieved\n",
    "gdf_master.to_crs(epsg=3577, inplace=True)\n",
    "\n",
    "# #  View the unique classes\n",
    "# print(gdf_master[\"BRD_HAB\"].unique())\n",
    "\n",
    "#  Drop unrequired classes\n",
    "gdf_master = gdf_master.drop(\n",
    "    gdf_master[\n",
    "        (gdf_master.BRD_HAB == 'Subtidal consolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal consolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal coral')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal coral')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal unconsolidated substrate')\n",
    "        | (gdf_master.BRD_HAB == \"Intertidal unconsolidated substrate\")\n",
    "        | (gdf_master.BRD_HAB == 'Intertidal algae')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal seagrass')\n",
    "        | (gdf_master.BRD_HAB == 'Subtidal algae')\n",
    "    ].index\n",
    ")\n",
    "\n",
    "#  Reset the index of the gdf to infill dropped values\n",
    "gdf_master.reset_index(inplace=True)\n",
    "\n",
    "#  Check that correct classes remain\n",
    "print(gdf_master[\"BRD_HAB\"].unique())\n",
    "\n",
    "# Attribute each class with an integer value\n",
    "val = (gdf_master[\"BRD_HAB\"].unique()).tolist()\n",
    "\n",
    "num_list = []\n",
    "attr_key = []\n",
    "\n",
    "d = 0\n",
    "for x in range(len(gdf_master)):\n",
    "    for d in range(len(val)):\n",
    "        if gdf_master[\"BRD_HAB\"].values[x] == str(val[d]):\n",
    "            num_list.append(d)\n",
    "        # Create a key to interpret the integer attribute for each class\n",
    "        for y in num_list:\n",
    "            if y not in attr_key:\n",
    "                attr_key.append(y)\n",
    "\n",
    "val = [[el] for el in val]\n",
    "for x in attr_key:\n",
    "    val[x].append(attr_key[x])\n",
    "\n",
    "\n",
    "print(\"The attribute values for each class are as follows: \" + str(val))\n",
    "\n",
    "# Update the geodataframe of vector polygons with the integer attribution for each class\n",
    "gdf_master[\"id\"] = num_list\n",
    "\n",
    "# Map the shapefiles from imported vector set\n",
    "roi = map_shapefile(gdf_master, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After identifying an analysis region of interest(roi) on the map,\n",
    "intersect the roi with the master polygon gdf from which imagery \n",
    "will be extracted. Note that when multiple areas are drawn on the\n",
    "map above, it is the final roi that is used for analysis in the \n",
    "following cells\n",
    "'''\n",
    "\n",
    "##  Form a shapely polygon from the coordinates defined by the user on the map\n",
    "polygon_roi = Polygon(roi[-1]['geometry']['coordinates'][0])\n",
    "\n",
    "##  Generate a new geodataframe containing the user defined polygon geometry\n",
    "newdf = gpd.GeoDataFrame(gpd.GeoSeries(polygon_roi), columns=['geometry'], crs='EPSG:4326')\n",
    "newdf = newdf.to_crs(gdf_master.crs)\n",
    "\n",
    "##  Intersect the user-defined region of interest with the master\n",
    "##  to create the working gdf from which imagery will be extracted\n",
    "gdf = gpd.overlay(gdf_master, newdf, how='intersection')\n",
    "print('This selection includes ', len(gdf), ' individual polygons')\n",
    "gdf.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load imagery for the region of interest\n",
    "\n",
    "*user to define time-period of interest in the query*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the general query and variables for later\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "align = (0, 0)\n",
    "\n",
    "# Query\n",
    "query = {\n",
    "    \"time\": (\"2013-01-01\", \"2021-01-01\"),\n",
    "    \"measurements\": [\"nbart_red\", \"nbart_green\", \"nbart_blue\", \"nbart_nir\", \"nbart_swir_1\"],\n",
    "    \"output_crs\": \"EPSG:3577\", # Do not change th\n",
    "    \"resolution\": (-30, 30),\n",
    "    \"group_by\": \"solar_day\",\n",
    "    \"dask_chunks\": {\"time\": 1, \"x\": 3000, \"y\": 3000},\n",
    "}\n",
    "\n",
    "# Designate dask chunks\n",
    "# It doesn't really matter how big the chunks we load are, as long as time ~ 1.\n",
    "chunks = {\"time\": 1, \"x\": 3000, \"y\": 3000}\n",
    "\n",
    "# Load data for predetermined polygons\n",
    "# Dictionary to save results\n",
    "results = {}\n",
    "\n",
    "'''List of saved tideposts. In the event that a polygon centroid geometry fails\n",
    "to return an associated tideheight, the tidal_tag function will bring in this list\n",
    "and use the most recent successful polygon centroid geometry to calculate a tideheight\n",
    "for the current polygon'''\n",
    "tideposts = [[0,0]]\n",
    "\n",
    "# Loop through polygons in geodataframe and extract satellite data\n",
    "for index, row in gdf.iterrows():\n",
    "\n",
    "    print(f\"Feature: {index + 1}/{len(gdf)}\")\n",
    "    print(gdf[\"BRD_HAB\"].values[index])\n",
    "    print(str(index))\n",
    "    print(str(row))\n",
    "\n",
    "    if not (str(row[attribute_col]) in results.keys()):\n",
    "        results[str(row[attribute_col])] = {}\n",
    "\n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "\n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load landsat\n",
    "    ds = load_ard(\n",
    "        dc=dc,\n",
    "        products=products,\n",
    "        min_gooddata=0.99,  # only take uncloudy scenes\n",
    "        ls7_slc_off=False,\n",
    "        **query,\n",
    "    )\n",
    "\n",
    "    ## Tidally tag datasets\n",
    "    ds, tidepost_lon, tidepost_lat = tidal_tag(ds,\n",
    "                                               tideposts[-1],\n",
    "                                               return_tideposts=True,\n",
    "                                               ebb_flow=True)\n",
    "    tideposts.append([tidepost_lon, tidepost_lat])\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "\n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    ds = ds.where(mask)\n",
    "    \n",
    "    ## Attach unique polygon id to each dataset\n",
    "    attrs = {'pgid': row['OBJECTID']}\n",
    "    ds.attrs = attrs\n",
    "\n",
    "    # Append results to a dictionary using the attribute\n",
    "    # column as an key\n",
    "    results[str(row[attribute_col])][str(index)] = ds\n",
    "\n",
    "    print(row[attribute_col], index)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter results by tide height, calculate indices and build ITEM masks\n",
    "\n",
    "*user to define tide_range to keep - set the desired quantile value in `lowest_20` variable*\n",
    "\n",
    "*user defines required indices in the `calculate_indices` function call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = {}\n",
    "x=1\n",
    "\n",
    "for k in results:\n",
    "\n",
    "    if not (str(k) in results2.keys()):\n",
    "        results2[str(k)] = {}\n",
    "        \n",
    "    for kk in results[k]:\n",
    "        \n",
    "        ds = results[k][kk] \n",
    "        \n",
    "        ## Save attributes to reattach later\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        ## Filter data by tide height\n",
    "        lowest_10 = ds.tide_height.quantile([0.10]).values\n",
    "        lowest_20 = ds.tide_height.quantile([0.20]).values  \n",
    "        results2[k][kk] = ds.where(ds.tide_height <= lowest_20, drop=True)      \n",
    "        \n",
    "        ## Compute data from dask - WARNING: time consuming step!\n",
    "        results2[k][kk] = results2[k][kk].compute()       \n",
    "        ds = results2[k][kk]\n",
    "       \n",
    "        ## Drop tide_height and ebb_flow variables\n",
    "        '''\n",
    "        works around the calculate_indices function which was stalling on the additional \n",
    "        coastal variables, drop tide_height and ebb_flow variables\n",
    "        '''\n",
    "        tide_height = ds['tide_height']\n",
    "        ebb_flow = ds['ebb_flow']\n",
    "        ds = ds.drop_vars(names = ('tide_height', 'ebb_flow'))\n",
    "        \n",
    "        # calculate ndvi for pixels inside the polygon\n",
    "        ds = calculate_indices(ds, index=['NDVI', 'MNDWI', 'NDAVI', 'WAVI', 'EVI', 'SAVI', 'NDWI'], \n",
    "                               collection='ga_ls_3', inplace=True)\n",
    "        \n",
    "        # Add tide_height back in to calculate ITEM mask\n",
    "        ds['tide_height'] = tide_height\n",
    "\n",
    "        ## Prepare data to calculate ITEM masks\n",
    "        lowest_10 = ds.where(ds.tide_height <= lowest_10, drop=True)\n",
    "        lowest_20 = ds.where(ds.tide_height <= lowest_20, drop=True)\n",
    "\n",
    "        ## Calculate ITEM layers\n",
    "        lowest_10_mask = lowest_10[['NDWI', 'tide_height']].median(dim='time')\n",
    "        lowest_20_mask = lowest_20[['NDWI', 'tide_height']].median(dim='time')\n",
    "        \n",
    "#         # Add ITEM mask layers to results2 datasets\n",
    "        results2[k][kk]['lowest_10_mask'] = lowest_10_mask.NDWI <= 0 \n",
    "        results2[k][kk]['lowest_20_mask'] = lowest_20_mask.NDWI <= 0 \n",
    "        \n",
    "        results2[str(k)][str(kk)] = xr.merge([results2[str(k)][str(kk)], ds])\n",
    "        \n",
    "        ## Attach unique polygon id to each dataset\n",
    "        results2[str(k)][str(kk)].attrs = attrs\n",
    "        \n",
    "        ## Update progress\n",
    "        ## New line. If cell fails, explore here first. \n",
    "        ## Probably an issue with the call to `index` from cell above\n",
    "        print(f\"Completed feature: {x}/{index + 1}\") \n",
    "        \n",
    "        x=x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save loaded results\n",
    "*By default, save the imagery and polygon sub-sampled datasets every time.*\n",
    "\n",
    "*If required to load last dataset, hash out the `save` calls and unhash the `load` calls*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/dev/dea-notebooks/Claire\n"
     ]
    }
   ],
   "source": [
    "# ## Save variables\n",
    "\n",
    "# ## Save imagery dict\n",
    "# with open('results2_' + query['time'][0] +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results2, handle)\n",
    " \n",
    "# ## Save polygon gdf   \n",
    "# with open('gdf_' + query['time'][0] +'.pickle', 'wb') as handle:\n",
    "#     pickle.dump(gdf, handle)\n",
    "    \n",
    "'''-----------------------------------------'''\n",
    "\n",
    "# Load saved variables (hashed out by default)\n",
    "\n",
    "## Re-load the query\n",
    "\n",
    "# Setup the general query and variables for later\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "align = (0, 0)\n",
    "\n",
    "# Query\n",
    "query = {\n",
    "    \"time\": (\"2013-01-01\", \"2020-08-01\"),\n",
    "    \"measurements\": [\"nbart_red\", \"nbart_green\", \"nbart_blue\", \"nbart_nir\", \"nbart_swir_1\"],\n",
    "    \"output_crs\": \"EPSG:3577\", # Do not change th\n",
    "    \"resolution\": (-30, 30),\n",
    "    \"group_by\": \"solar_day\",\n",
    "    \"dask_chunks\": {\"time\": 1, \"x\": 3000, \"y\": 3000},\n",
    "}\n",
    "\n",
    "## ensure that you are working from the same directory as the files are stored\n",
    "%cd '/home/jovyan/dev/dea-notebooks/Claire'\n",
    "\n",
    "## Load imagery dict\n",
    "with open('results2_' + query['time'][0] +'.pickle', 'rb') as handle:\n",
    "    results2 = pickle.load(handle)\n",
    "\n",
    "## Load polygon gdf    \n",
    "with open('gdf_' + query['time'][0] +'.pickle', 'rb') as handle:\n",
    "    gdf = pickle.load(handle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Load the following cell to define the functions that enable data interrogation and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funcs for py script\n",
    "\n",
    "## Testing updated coastal_wit func. Drafted in BKUP notebook and copied over to this cell\n",
    "\n",
    "def coastal_wit(\n",
    "               results2,\n",
    "               pg,\n",
    "               ITEM_mask = 'lowest_20_mask',\n",
    "               classes = False\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Takes a polygon or polygons in a region of interest ([pg]) from the gdf of polygon shapefiles and extracts \n",
    "    and plots the frequency of pixels as assigned into class ranges of NDVI. Also includes pixels identified\n",
    "    as wet. All NDVI pixels are masked by 'dry' range of NDWI and everything is masked by the ITEM mask.\n",
    "    If conditional dataset dropping is required, set drop=True and nominate the percent of wet pixels\n",
    "    tolerated and the minimum number of allowable pixels (e.g. pc_drop=90 means drop any timestep dataset\n",
    "    when more than 90% of pixels are wet; px_min=5 means that a timestep dataset will only be dropped\n",
    "    when more than pc_drop pixels are wet AND the remaining pixels sum to less than or = px_min - or\n",
    "    5 in this case)\n",
    "    If the WIT datasets have already been generated and included in the xarray dataset (`results2`) then \n",
    "    set `classes = True` to avoid re-calculation. Default is False.\n",
    "    ITEM_mask is the layer to nominate for ITEM masking. If no masking required, select one of the \n",
    "    original bands such as 'nbart_red'\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ## Set up lists to extract pixel counts per timestep into\n",
    "    ls_pixels = []\n",
    "    ls_ndwi = []\n",
    "    ls_unveg = []\n",
    "    ls_ndvilow = []\n",
    "    ls_ndvimid = []\n",
    "    ls_ndvihigh = []\n",
    "    ls_pxsum = []\n",
    "    ls_tide = []\n",
    "    ls_index = []\n",
    "\n",
    "    ##  Data prep for WIT prototype (automated for all timesteps for all polygons)\n",
    "\n",
    "    ##  Generate all datasets for the stacked line plot (WIT) by adding as variables to results dataset\n",
    "    for k in results2:\n",
    "        for kk in results2[k]: \n",
    "\n",
    "            if classes == False:\n",
    "\n",
    "                ##  Add a non-water pixel mask variable\n",
    "                results2[k][kk]['mask'] = ((results2[k][kk].NDWI)\n",
    "                                           .where(results2[k][kk][ITEM_mask]) <= 0)\n",
    "\n",
    "                ##  Generate the NDWI_water class\n",
    "                results2[k][kk]['ndwi_water'] = ((results2[k][kk].NDWI)\n",
    "                                                  .where(results2[k][kk][ITEM_mask]) > 0)\n",
    "\n",
    "                ##  Generate NDVI classes\n",
    "\n",
    "                ##  NDVI less than 0.1\n",
    "                results2[k][kk]['unveg'] = (results2[k][kk].NDVI\n",
    "                                            .where((results2[k][kk].NDVI < 0.1).astype(int))\n",
    "                                            .where(results2[k][kk][ITEM_mask]))\n",
    "\n",
    "                ##  NDVI 0.1 to 0.33\n",
    "                results2[k][kk]['ndvi_low'] = (results2[k][kk].NDVI\n",
    "                                               .where((results2[k][kk].NDVI >= 0.1).astype(int) & \n",
    "                                                      (results2[k][kk].NDVI < 0.333).astype(int))\n",
    "                                               .where(results2[k][kk][ITEM_mask]))\n",
    "\n",
    "                ##  NDVI 0.33 to 0.66\n",
    "                results2[k][kk]['ndvi_mid'] = (results2[k][kk].NDVI\n",
    "                                               .where((results2[k][kk].NDVI >= 0.333).astype(int) & \n",
    "                                                      (results2[k][kk].NDVI < 0.666).astype(int))\n",
    "                                               .where(results2[k][kk][ITEM_mask]))\n",
    "\n",
    "                ### NDVI 0.66 to 1\n",
    "                results2[k][kk]['ndvi_high'] = (results2[k][kk].NDVI\n",
    "                                                .where((results2[k][kk].NDVI >= 0.666).astype(int) & \n",
    "                                                       (results2[k][kk].NDVI <= 1).astype(int))\n",
    "                                                .where(results2[k][kk][ITEM_mask]))\n",
    "\n",
    "                ##  Mask the NDVI classes to show non-water pixels only\n",
    "                results2[k][kk]['unveg'] = results2[k][kk]['unveg'].where(results2[k][kk]['mask'])\n",
    "                results2[k][kk]['ndvi_low'] = results2[k][kk]['ndvi_low'].where(results2[k][kk]['mask'])\n",
    "                results2[k][kk]['ndvi_mid'] = results2[k][kk]['ndvi_mid'].where(results2[k][kk]['mask'])\n",
    "                results2[k][kk]['ndvi_high'] = results2[k][kk]['ndvi_high'].where(results2[k][kk]['mask'])\n",
    "\n",
    "                for x in pg:\n",
    "                    if x == results2[str(k)][str(kk)].attrs['pgid']:\n",
    "                         ##  Populate pixel count lists per class per timestep\n",
    "                        for t in range (0, len(results2[str(k)][str(kk)].time)):\n",
    "                            ls_pixels.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].where(results2[k][kk][ITEM_mask]).isel(time=t).nbart_red)))\n",
    "                            ls_ndwi.append(np.count_nonzero(results2[str(k)][str(kk)].isel(time=t).ndwi_water))\n",
    "                            ls_unveg.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].isel(time=t).unveg)))\n",
    "                            ls_ndvilow.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].isel(time=t).ndvi_low)))\n",
    "                            ls_ndvimid.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].isel(time=t).ndvi_mid)))\n",
    "                            ls_ndvihigh.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].isel(time=t).ndvi_high)))\n",
    "                            ls_pxsum.append(np.count_nonzero(~np.isnan(results2[str(k)][str(kk)].where(results2[k][kk][ITEM_mask]).isel(time=t).nbart_red)))\n",
    "                            ls_tide.append(results2[str(k)][str(kk)].isel(time=t).tide_height.median().values)\n",
    "\n",
    "                        ls_index.append(results2[str(k)][str(kk)].time.values)\n",
    "    ls_index = np.concatenate(ls_index).tolist()\n",
    "#                                 index = results2[str(k)][str(kk)].time.values\n",
    "\n",
    "#         if classes == True:\n",
    "\n",
    "#              ##  Populate pixel count lists per class per timestep\n",
    "#             for t in range (0, len(results2.time)):\n",
    "#                 ls_pixels.append(np.count_nonzero(~np.isnan(results2.isel(time=t).nbart_red)))\n",
    "# #                 ls_mndwi.append(np.count_nonzero((results2.isel(time=t).mndwi_water)))\n",
    "#                 ls_mndwi.append(np.count_nonzero(~np.isnan(mWIT1.isel(time=t).mndwi_water.where(mWIT1.isel(time=t).mndwi_water == True))))\n",
    "#                 ls_unveg.append(np.count_nonzero(~np.isnan(results2.isel(time=t).unveg)))\n",
    "#                 ls_ndvilow.append(np.count_nonzero(~np.isnan(results2.isel(time=t).ndvi_low)))\n",
    "#                 ls_ndvimid.append(np.count_nonzero(~np.isnan(results2.isel(time=t).ndvi_mid)))\n",
    "#                 ls_ndvihigh.append(np.count_nonzero(~np.isnan(results2.isel(time=t).ndvi_high)))\n",
    "#                 ls_pxsum.append(np.count_nonzero(~np.isnan(results2.isel(time=t).nbart_red)))\n",
    "#                 ls_tide.append(results2.isel(time=t).tide_height.median().values)\n",
    "\n",
    "#                 index = results2.time.values\n",
    "\n",
    "\n",
    "    ## Return index values to datetime64\n",
    "    for x in range(0, len(ls_index)):\n",
    "        ls_index[x] = np.datetime64(int(ls_index[x]), 'ns')\n",
    "\n",
    "    ##  Generate a dataframe summarising class pixel counts per timestep\n",
    "    classes_df = pd.DataFrame(\n",
    "                {\"pixels\": ls_pixels,\n",
    "                \"water\": ls_ndwi,\n",
    "                \"unveg\": ls_unveg,\n",
    "                \"ndvi_low\": ls_ndvilow,\n",
    "                \"ndvi_mid\": ls_ndvimid,\n",
    "                \"ndvi_high\": ls_ndvihigh,\n",
    "                \"px_sum\": ls_pxsum,\n",
    "                \"tide_height\": ls_tide},\n",
    "                index = ls_index\n",
    "                )\n",
    "\n",
    "    ## Aggregate the datasets to account for identical imagery dates spanning different polygons\n",
    "    ## All pixel values are summed. Tide_height values are averaged\n",
    "    ## Separate tide_height dataset and convert to dataframe\n",
    "    classes_df_th = classes_df.tide_height\n",
    "    classes_df_th = pd.DataFrame(classes_df_th)\n",
    "\n",
    "    ## Drop tide_height from main df then group rows with duplicate indices and then sum them up\n",
    "    classes_df = classes_df.drop(['tide_height'], axis=1)\n",
    "    classes_df = classes_df.groupby(classes_df.index).sum()\n",
    "\n",
    "    ## Take tide-height values and convert from array to int\n",
    "    intlist=[]\n",
    "    for x in range (0,len(classes_df_th.tide_height.values.tolist())):\n",
    "        intlist.append(classes_df_th.tide_height.values.tolist()[x].tolist())\n",
    "\n",
    "    ## Add integer list to tide_height dataframe and drop array list\n",
    "    classes_df_th['th_int'] = intlist\n",
    "    classes_df_th = classes_df_th.drop('tide_height', axis=1)\n",
    "\n",
    "    ## Group by index dates as per classes_df and calculate mean tide_height value\n",
    "    classes_df_th = classes_df_th.groupby(classes_df_th.index).mean()\n",
    "\n",
    "    ## Merge the pixel and tide_height datasets back together\n",
    "    classes_df = classes_df.merge(classes_df_th, left_on = classes_df.index, right_on = classes_df_th.index)\n",
    "\n",
    "    ## Rename tide_height column to something sensible\n",
    "    classes_df = classes_df.rename(columns={'th_int':'tide_height'})\n",
    "\n",
    "    ## Reset the index to observation dates\n",
    "    classes_df.set_index('key_0', inplace=True)\n",
    "\n",
    "    ##  Normalise pixel counts per class per timestep\n",
    "    classes_df['pc_water'] = classes_df['water']/classes_df['pixels']*100\n",
    "    classes_df['pc_unveg'] = classes_df['unveg']/classes_df['pixels']*100\n",
    "    classes_df['pc_ndvi_low'] = classes_df['ndvi_low']/classes_df['pixels']*100\n",
    "    classes_df['pc_ndvi_mid'] = classes_df['ndvi_mid']/classes_df['pixels']*100\n",
    "    classes_df['pc_ndvi_high'] = classes_df['ndvi_high']/classes_df['pixels']*100\n",
    "    classes_df['pc_total'] = (classes_df['pc_water']+\n",
    "                              classes_df['pc_unveg']+\n",
    "                              classes_df['pc_ndvi_low']+\n",
    "                              classes_df['pc_ndvi_mid']+\n",
    "                              classes_df['pc_ndvi_high'])\n",
    "    classes_df['pc_exposedpx'] = (classes_df['unveg']+\n",
    "                                  classes_df['ndvi_low']+\n",
    "                                  classes_df['ndvi_mid']+\n",
    "                                  classes_df['ndvi_high'])/classes_df['px_sum']*100\n",
    "\n",
    "\n",
    "    return classes_df, results2\n",
    "\n",
    "def onclick_timeseries(event):\n",
    "    '''\n",
    "    This widget allows the user to select a time point from a plotted time series. \n",
    "    It then translates the chosen point back into the approriate datetime object so\n",
    "    that it can be used to find the location of this time point within the extracted\n",
    "    datasets. The index location of this time step is also returned. \n",
    "    \n",
    "    '''\n",
    "    global time_slice, TimeIndex, pixelx, pixely\n",
    "    \n",
    "    # Get time from x axis of plot \n",
    "    timeOfInterest = event.xdata\n",
    "    \n",
    "    # Get x and y coordinates from click\n",
    "    pixelx, pixely = int(event.xdata), int(event.ydata)\n",
    "    \n",
    "    # Add point to image\n",
    "    plt.plot(pixelx, pixely, 'ro', markersize=5)\n",
    "    \n",
    "    # Convert clicked int to datetime format\n",
    "    time_slice = matplotlib.dates.num2date(timeOfInterest).date()\n",
    "    \n",
    "    # Convert clicked value to str\n",
    "    time_slice = str(time_slice)\n",
    "    \n",
    "    # Convert clicked value to correct datetime format\n",
    "    time_slice = pd.to_datetime(time_slice, format='%Y-%m-%d')\n",
    "    \n",
    "    # Find the time index of the chosen time slice\n",
    "    TimeIndex = results2[str(cl)][str(pg1)].indexes['time'].get_loc(time_slice, method='nearest')\n",
    "    \n",
    "    # Print the date of the image closest to clicked pixel\n",
    "    ClosestImage = results2[str(cl)][str(pg1)].time[TimeIndex].values\n",
    "    \n",
    "    # Update text below plot\n",
    "    w2.value = 'Closest imagery date : {}'.format(ClosestImage)\n",
    "    \n",
    "        \n",
    "def temporal_stats (gdf, \n",
    "                    results2,\n",
    "                    zonal=True,\n",
    "                    pixel=True,\n",
    "                    mask= 'lowest_20_mask'):\n",
    "\n",
    "    '''\n",
    "    This function calculates and returns the temporal mean and standard deviation\n",
    "    for a range of indices for every polygon and every pixel within\n",
    "    every polygon in the form of a geodataframe.  \n",
    "    For now, leave zonal and pixel set to True to calculate all results. Func may\n",
    "    not work if either of these vars are set to False.\n",
    "    ITEM masking is available using the `mask` variable. If no masking is required, set\n",
    "    this variable to one of the bands such as 'nbart_nir'. If masking, two ITEM\n",
    "    layers are generated by default to choose from. Either 'lowest_20_mask' for\n",
    "    all images except those associated with the lowest 20% of observed tides, or\n",
    "    'lowest_10_mask' to mask by the equivalent in the 10% range.\n",
    "    '''\n",
    "    ##  Spectral indices available in the `calculate_indices` function (dea_bandindices.py)\n",
    "    index_ls = [\n",
    "        \"NDVI\",\n",
    "        \"EVI\",\n",
    "        \"NDAVI\", \n",
    "        \"WAVI\",\n",
    "        \"LAI\",\n",
    "        \"SAVI\",\n",
    "        \"MSAVI\",\n",
    "        \"NDMI\",\n",
    "        \"NBR\",\n",
    "        \"BAI\",\n",
    "        \"NDCI\",\n",
    "        \"NDSI\",\n",
    "        \"NDTI\",\n",
    "        \"NDWI\",\n",
    "        \"MNDWI\",\n",
    "        \"NDBI\",\n",
    "        \"BUI\",\n",
    "        \"BAEI\",\n",
    "        \"NBI\",\n",
    "        \"BSI\",\n",
    "        \"AWEI_ns\",\n",
    "        \"AWEI_sh\",\n",
    "        \"WI\",\n",
    "        \"TCW\",\n",
    "        \"TCG\",\n",
    "        \"TCB\",\n",
    "        \"CMR\",\n",
    "        \"FMR\",\n",
    "        \"IOR\",\n",
    "    ]\n",
    "\n",
    "       \n",
    "    \n",
    "    ## Calculate zonal polygon results\n",
    "    if zonal:\n",
    "        \n",
    "        gdf_zonal = gdf \n",
    "        \n",
    "        ##  Generate a list of keys for polygons in class '0' of results2\n",
    "        keylist = list(results2['0'].keys())\n",
    "\n",
    "        ##  For each nominated indice:\n",
    "        for var in index_ls:\n",
    "            if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "                lsmean = []\n",
    "                lsstd = []\n",
    "\n",
    "                ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "                ##  Append polygon id and temporal statistic to gdf\n",
    "                for k in results2:\n",
    "\n",
    "                    for kk in results2[k]:\n",
    "                        temporalmean = ((results2[k][kk][var]\n",
    "                                         .where(results2[k][kk][mask])\n",
    "                                         .mean('y')\n",
    "                                         .mean('x'))\n",
    "                                         .mean())\n",
    "                        lsmean.append([int(kk), temporalmean.values])\n",
    "                        name1 = str(var) + ' zonal mean'\n",
    "                        results2[str(k)][str(kk)][name1] = (temporalmean.values)\n",
    "                        \n",
    "                        temporalstd = ((results2[k][kk][var]\n",
    "                                        .where(results2[k][kk][mask])\n",
    "                                        .std('y')\n",
    "                                        .std('x'))\n",
    "                                        .std())\n",
    "                        lsstd.append([int(kk), temporalstd.values])  \n",
    "                        name2 = str(var) + ' zonal std'\n",
    "                        results2[str(k)][str(kk)][name2] = (temporalstd.values)                        \n",
    "\n",
    "                ##  Sort the list by polygon id to match up to the original polygon gdf\n",
    "                lsmean = sorted(lsmean)\n",
    "                lsstd = sorted(lsstd)\n",
    "\n",
    "                ##  Separate the sorted polygon ids from the indice statistic to build into a pd.DataFrame\n",
    "                indicemean = []\n",
    "                indicestd = []\n",
    "                polyid = []\n",
    "\n",
    "                for x in lsmean:\n",
    "                    polyid.append(x[0])\n",
    "                    indicemean.append(x[1])\n",
    "\n",
    "                for x in lsstd:\n",
    "                    indicestd.append(x[1])\n",
    "\n",
    "                # Build a pd.DataFrame from the sorted polygon id and indice statistics. \n",
    "                # Nominate a name for the new column.\n",
    "                indexstats = pd.DataFrame(\n",
    "                    indicemean, index=polyid, columns=[str(var) + \" zonal mean\"]\n",
    "                )\n",
    "\n",
    "#                 indexstats[str(var) + ' zonal mean'] = indicemean\n",
    "#                 indexstats['polyid'] = polyid \n",
    "    \n",
    "                indexstats[str(var) + \" zonal std\"] = None\n",
    "                indexstats.loc[polyid, (str(var) + \" zonal std\")] = indicestd\n",
    "\n",
    "                # Workaround to handle automatic addition of key_0 column at merge step\n",
    "                if \"key_0\" in gdf_zonal.columns:\n",
    "                    gdf_zonal.drop(columns=[\"key_0\"], inplace=True)\n",
    "\n",
    "                # # Merge the indice statistic for each polygon into the original polygon gdf\n",
    "                gdf_zonal = gdf_zonal.merge(indexstats, on=indexstats.index)\n",
    "\n",
    "        ## Rename the 'index' column to avoid confusion with the gdf.index\n",
    "        gdf_zonal.rename(columns={'OBJECTID' : 'pgid'}, inplace=True)\n",
    "        gdf_zonal.rename(columns={'geometry' : 'pg_geometry'}, inplace=True)\n",
    "\n",
    "#                         return gdf\n",
    "\n",
    "    ## Calculate pixel results\n",
    "    if pixel:\n",
    "\n",
    "        ## New dict to store arrays\n",
    "#         pxsummary = {}\n",
    "\n",
    "        ## Master gdf\n",
    "        gdf_px = gpd.GeoDataFrame()\n",
    "        gdf_px['geometry'] = None\n",
    "\n",
    "        ## List to store geom, unique pixel id \n",
    "        # pxid = []\n",
    "        uniquepx = []\n",
    "        pxgeom = []\n",
    "\n",
    "        ##  Generate a list of keys for polygons in class '0' of results2\n",
    "        keylist = list(results2['0'].keys())\n",
    "\n",
    "\n",
    "        ##  Generate zonal indice stats for polygon followed by temporal statistic\n",
    "        ##  Append polygon id and temporal statistic to gdf\n",
    "        for k in results2:\n",
    "\n",
    "#             if not (str(k) in pxsummary.keys()):\n",
    "#                 pxsummary[str(k)] = {}\n",
    "\n",
    "            for kk in results2[k]:\n",
    "\n",
    "                lon = []\n",
    "                for value in results2[k][kk].x.values:\n",
    "                    lon.append(value)\n",
    "\n",
    "                lat = []\n",
    "                for value in results2[k][kk].y.values:\n",
    "                    lat.append(value)\n",
    "\n",
    "                ds = xr.Dataset()\n",
    "\n",
    "                ##  For each nominated indice:\n",
    "                for var in index_ls:\n",
    "                    if var in results2[\"0\"][keylist[0]].var():\n",
    "\n",
    "                        temporalmean = (results2[str(k)][str(kk)][var]\n",
    "                                        .where(results2[k][kk][mask])\n",
    "                                        .mean(dim='time'))\n",
    "                        name1 = str(var) + ' px mean'\n",
    "                        ds[name1] = (('y', 'x'), temporalmean)\n",
    "                        results2[str(k)][str(kk)][name1] = (('y', 'x'), temporalmean)\n",
    "\n",
    "                        temporalstd = (results2[k][kk][var]\n",
    "                                       .where(results2[k][kk][mask])\n",
    "                                       .std(dim='time'))\n",
    "                        name2 = str(var) + ' px std'\n",
    "                        ds[name2] = (('y', 'x'), temporalstd)\n",
    "                        results2[str(k)][str(kk)][name2] = (('y', 'x'), temporalstd)\n",
    "\n",
    "                ds.coords['lon'] = ('x'), lon\n",
    "                ds.coords['lat'] = ('y'), lat\n",
    "\n",
    "\n",
    "                ## Extract pixel geometry/shape for input into gdf for choropleth plotting\n",
    "                # Extract dataset matching polygon\n",
    "                closest_ds = results2[k][kk]\n",
    "\n",
    "                ## Extract pgid attributes to attach later\n",
    "                attrs = closest_ds.attrs\n",
    "                ds.attrs = attrs\n",
    "\n",
    "                ## Skip empty arrays\n",
    "                if closest_ds.x.size == 0:\n",
    "                    print('Empty arrays: k: ', k, 'kk: ', kk)\n",
    "                    continue\n",
    "                else:       \n",
    "                    # Input array (based on red band) to segment and vectorise\n",
    "                    input_array = closest_ds.nbart_red\n",
    "                    input_transform = closest_ds.affine  \n",
    "                    input_crs = input_array.crs\n",
    "\n",
    "                    # Create array with a unique value per cell\n",
    "                    unique_pixels = np.arange(input_array.size).reshape(input_array.shape)\n",
    "\n",
    "                    # Vectorise each unique feature in array\n",
    "                    vectors = rasterio.features.shapes(\n",
    "                        source=unique_pixels.astype(np.int16), transform=input_transform\n",
    "                    )\n",
    "\n",
    "                    # Extract polygons and values from generator\n",
    "                    vectors = list(vectors)\n",
    "                    values = [value for polygon, value in vectors]\n",
    "                    polygons = [shape(polygon) for polygon, value in vectors]\n",
    "                    pp = np.array(polygons)\n",
    "                    pp = pp.reshape(len(input_array.y), len(input_array.x))\n",
    "\n",
    "#              # Create a geopandas dataframe populated with the polygon shapes\n",
    "#              closestdate_poly_gdf = gpd.GeoDataFrame(data={\"id\": values}, geometry=polygons, crs=input_crs)\n",
    "\n",
    "                    ds['geometry'] = (('y', 'x'), pp)\n",
    "\n",
    "                results2[k][kk]['geometry'] = (('y', 'x'), pp)\n",
    "                results2[k][kk].set_coords('geometry')\n",
    "\n",
    "\n",
    "                ## Append [geom, unique pixel id] to list for later addition to gdf, merging on geom\n",
    "                pxid2 = []\n",
    "                pgid2 = []\n",
    "\n",
    "                for x in ds['x']:\n",
    "                    for y in ds['y']:\n",
    "                        pxid1 = str(attrs['pgid'])+'_'+str(ds['y'][y].item())+'_'+str(ds['x'][x].item())\n",
    "                        gm = pp[ds['y'][y].item()][ds['x'][x].item()]\n",
    "\n",
    "                        uniquepx.append(pxid1)\n",
    "                        pxgeom.append(gm)\n",
    "                        pxid2.append(pxid1)\n",
    "                        pgid2.append(attrs['pgid'])\n",
    "\n",
    "                pxid2 = np.array(pxid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "                pgid2 = np.array(pgid2).reshape(ds.y.shape[0], ds.x.shape[0])\n",
    "                results2[k][kk]['pxid'] = (('y', 'x'), pxid2)\n",
    "\n",
    "                ds['pgid'] = (('y', 'x'), pgid2)\n",
    "\n",
    "                ## Reattach attrs\n",
    "                results2[k][kk].attrs = attrs\n",
    "\n",
    "#                 pxsummary[k][kk] = ds\n",
    "\n",
    "                ## Append ds to gdf_px\n",
    "                ds = ds.to_dataframe()\n",
    "                gdf_px = gdf_px.append(ds)\n",
    "                gdf_px.crs = gdf.crs\n",
    "\n",
    "        pxid = gpd.GeoDataFrame(uniquepx, geometry=pxgeom, columns=['pxid'], crs=input_crs)\n",
    "\n",
    "        gdf_px.reset_index(inplace=True)\n",
    "\n",
    "        gdf_px = gdf_px.merge(pxid)\n",
    "        gdf_px.set_index('pxid', inplace=True, drop=False)\n",
    "        gdf_px.dropna(axis=0, inplace=True)\n",
    "        gdf_px.sort_index(level='pxid', inplace=True)\n",
    "\n",
    "\n",
    "    ## Merge the (zonal) gdf and gdf_px\n",
    "    gdf_merged = gdf_px.merge(gdf_zonal, on = 'pgid')\n",
    "\n",
    "    ## Drop the pg_geometry to use the pixel geometry default\n",
    "    gdf_merged.drop(columns='pg_geometry', inplace=True)\n",
    "\n",
    "    ##  Create new gdf's for each class to plot\n",
    "    grasses = gdf_merged.drop(\n",
    "        gdf_merged[gdf_merged.BRD_HAB != \"Intertidal grass-herb-sedge-other succulent\"].index\n",
    "    )\n",
    "    mangroves = gdf_merged.drop(\n",
    "        gdf_merged[gdf_merged.BRD_HAB != \"Intertidal mangroves and other trees & shrubs\"].index\n",
    "    )\n",
    "    seagrass = gdf_merged.drop(gdf_merged[gdf_merged.BRD_HAB != \"Intertidal seagrass\"].index)\n",
    "\n",
    "    ##  Drop polygons containing NaN values\n",
    "    ##  ASSUMPTION: if NDVI contains NaNs, all indices will contain NaNs. \n",
    "    ##  ToDo: write a loop or func that looks for NaNs in any of the supplied indices\n",
    "    grasses = grasses.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std'])\n",
    "    mangroves = mangroves.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std'])\n",
    "    seagrass = seagrass.dropna(axis=0, how='any', subset=[\n",
    "                                               'NDVI zonal mean', \n",
    "                                               'NDVI px mean', \n",
    "                                               'NDVI zonal std', \n",
    "                                               'NDVI px std']) \n",
    "\n",
    "    return gdf_merged, results2, grasses, mangroves, seagrass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate zonal and temporal stats for each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate zonal and pixel stats and attach to class polygons\n",
    "gdf_merged, results2, grasses, mangroves, seagrass = temporal_stats(gdf, \n",
    "                                                                    results2, \n",
    "                                                                    mask='lowest_20_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the zonal statistics temporal summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select attribute from:  Index(['index_x', 'geometry', 'NDVI px mean', 'NDVI px std', 'EVI px mean',\n",
      "       'EVI px std', 'NDAVI px mean', 'NDAVI px std', 'WAVI px mean',\n",
      "       'WAVI px std', 'SAVI px mean', 'SAVI px std', 'NDWI px mean',\n",
      "       'NDWI px std', 'MNDWI px mean', 'MNDWI px std', 'lon', 'lat', 'pgid',\n",
      "       'pxid', 'key_0', 'index_y', 'CONSOL', 'DOM_TYPE', 'DOM_LABEL',\n",
      "       'CO_TYPES', 'TIDE_ZONE', 'BRD_HAB', 'Shape_Leng', 'Shape_Area', 'id',\n",
      "       'NDVI zonal mean', 'NDVI zonal std', 'EVI zonal mean', 'EVI zonal std',\n",
      "       'NDAVI zonal mean', 'NDAVI zonal std', 'WAVI zonal mean',\n",
      "       'WAVI zonal std', 'SAVI zonal mean', 'SAVI zonal std',\n",
      "       'NDWI zonal mean', 'NDWI zonal std', 'MNDWI zonal mean',\n",
      "       'MNDWI zonal std'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8539ef69304e418589f3c138a99d831b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.07642278650487, 151.5269758506437], controls=(ZoomControl(options=['position', 'zoom_in_text',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  Plot grasses\n",
    "print('Select attribute from: ', gdf_merged.columns)\n",
    "\n",
    "roi = map_shapefile(grasses, attribute=\"NDVI px std\", continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf3c21c40e74c5ead593d19825e7c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.075317892939943, 151.5305794537412], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  Plot mangroves\n",
    "roi = map_shapefile(mangroves, attribute=\"NDVI px std\", continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123faf2bf5994108b28b7c8e04e869b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-24.068350983865248, 151.53901381211955], controls=(ZoomControl(options=['position', 'zoom_in_text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  Plot seagrass\n",
    "roi = map_shapefile(seagrass, attribute=\"NDVI px std\", continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIT Preparation - single habitat polygon \n",
    "For single polygon interrogation, run this cell and skip the following (roi). Move on to the WIT Plotting part of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ******From the interactive zonal summary plots above, IDENTIFY A POLYGON to interrogate*******\n",
    "pg = [9450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIT preparation - from any region of interest\n",
    "\n",
    "To select any region for interrogation, use the 'draw a polygon' option on any of the above 3 plots.\n",
    "Upon completion of the polygon, the geometry will automatically be saved to memory and the following cell\n",
    "will prepare the data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Feature',\n",
       " 'properties': {'style': {'stroke': True,\n",
       "   'color': '#3388ff',\n",
       "   'weight': 4,\n",
       "   'opacity': 0.5,\n",
       "   'fill': True,\n",
       "   'fillColor': None,\n",
       "   'fillOpacity': 0.2,\n",
       "   'clickable': True}},\n",
       " 'geometry': {'type': 'Polygon',\n",
       "  'coordinates': [[[151.535763, -24.110792],\n",
       "    [151.542767, -24.092615],\n",
       "    [151.522579, -24.043964],\n",
       "    [151.504589, -24.045218],\n",
       "    [151.507885, -24.09788],\n",
       "    [151.535763, -24.110792]]]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This selection includes  4098  individual pixel polygons from  14  individual habitat class polygons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87cd3cc880049c6961423f6b00d1ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26f6d8a8d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WIT preparation from a roi\n",
    "\n",
    "##  Form a shapely polygon from the coordinates defined by the user on the map\n",
    "miniwit_roi = Polygon(roi[-1]['geometry']['coordinates'][0])\n",
    "\n",
    "##  Generate a new geodataframe containing the user defined polygon geometry\n",
    "miniwit_df = gpd.GeoDataFrame(gpd.GeoSeries(miniwit_roi), columns=['geometry'], crs='EPSG:4326')\n",
    "miniwit_df = miniwit_df.to_crs(gdf.crs)\n",
    "\n",
    "##  Intersect the user-defined region of interest with the master\n",
    "##  to create the working gdf from which imagery will be extracted\n",
    "miniwit_gdf = gpd.overlay(roi[0], miniwit_df, how='intersection')  \n",
    "\n",
    "# miniwit_gdf.plot()\n",
    "\n",
    "## Isolate the pgid's in the roi and convert to ints\n",
    "roi_pg = miniwit_gdf.pgid.unique()\n",
    "roi_pg = [int(x) for x in roi_pg]\n",
    "\n",
    "## Isolate the pxid's in the roi\n",
    "roi_px = miniwit_gdf.pxid.to_list()\n",
    "roi_px = [str(x) for x in roi_px]\n",
    "\n",
    "print('This selection includes ', len(roi_px), ' individual pixel polygons from ', len(roi_pg), ' individual habitat class polygons')\n",
    "\n",
    "## Find the associated class and polygon keys in results2\n",
    "## Save results for an roi search to this new xarray dict (`roi_results2`)\n",
    "roi_results2 = {}\n",
    "\n",
    "for k in results2:\n",
    "    \n",
    "    if not (str(k) in roi_results2.keys()):\n",
    "        roi_results2[str(k)] = {}\n",
    "        \n",
    "    for kk in results2[k]:\n",
    "        for x in roi_pg:\n",
    "            if x == results2[str(k)][str(kk)].attrs['pgid']:\n",
    "#                 print('True')\n",
    "#                 print ('[k][kk]: ', k, ',', kk)\n",
    "                \n",
    "                ## Mask results2 by pixels in roi\n",
    "                mask = (np.isin(results2[str(k)][str(kk)].pxid, roi_px))\n",
    "                mask = mask.reshape(mask.shape[-1], mask.shape[0])\n",
    "                \n",
    "                results2[str(k)][str(kk)]['roi_mask'] = (('x', 'y'), mask)\n",
    "                \n",
    "                roi_results2[str(k)][str(kk)] = (results2[str(k)][str(kk)]\n",
    "                                                 .where((results2[str(k)][str(kk)]\n",
    "                                                         .roi_mask == True)))\n",
    "                \n",
    "miniwit_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIT plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For single polygon interrogation\n",
    "classes_df, results2 = coastal_wit(results2, pg)\n",
    "# pg=pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For region of interest interrogation\n",
    "classes_df, results2 = coastal_wit(roi_results2, roi_pg)\n",
    "# pg = roi_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg=roi_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e98b3ecb44479fa2a9c7a59bc370a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fd95c08ccb4706bfb2d2d5a16ef1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980f6c348498489a96951f4ee6d47ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Click on the pixel you would like to interrogate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This cell prepares the nominated WIT data for plotting\n",
    "%matplotlib widget\n",
    "pal = [\n",
    "       sns.xkcd_rgb[\"cobalt blue\"],\n",
    "       sns.xkcd_rgb[\"beige\"],\n",
    "       sns.xkcd_rgb[\"light green\"],\n",
    "       sns.xkcd_rgb[\"green\"],\n",
    "       sns.xkcd_rgb[\"dark green\"]]\n",
    "\n",
    "plt.clf()\n",
    "plt.close(fig=None)\n",
    "\n",
    "## `cl` and `pg1` are required for the interactive date selection from the plot\n",
    "cl = gdf.loc[gdf['OBJECTID'] == pg[0]].id.values.item()\n",
    "pg1 = gdf.loc[gdf['OBJECTID'] == pg[0]].index.values.item()\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), constrained_layout=False)\n",
    "gs = fig.add_gridspec(8,1)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0:2, :])\n",
    "ax1.set_title('% of polygon exposed')\n",
    "ax2 = fig.add_subplot(gs[4:, :])\n",
    "ax2.set_title('% cover in exposed polygon')\n",
    "ax3 = fig.add_subplot(gs[2:4, :])\n",
    "ax3.set_title('modelled tide height')\n",
    "\n",
    "ax2.stackplot(classes_df.index, \n",
    "              classes_df['pc_water'], \n",
    "              classes_df['pc_unveg'],\n",
    "              classes_df['pc_ndvi_low'], \n",
    "              classes_df['pc_ndvi_mid'], \n",
    "              classes_df['pc_ndvi_high'],\n",
    "              labels=[\n",
    "                  'water',\n",
    "                  'unveg',\n",
    "                  'low veg',\n",
    "                  'medium veg',\n",
    "                  'dense veg',\n",
    "                 ], \n",
    "              baseline='zero',\n",
    "              colors=pal, \n",
    "              alpha = 0.6\n",
    "             )\n",
    "\n",
    "ax1.plot(classes_df.index, \n",
    "        classes_df['pc_exposedpx'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "ax3.plot(classes_df.index, \n",
    "        classes_df['tide_height'], \n",
    "        color='black', \n",
    "        linewidth=0.2, \n",
    "        marker='o',\n",
    "        markersize=3\n",
    "       )\n",
    "\n",
    "\n",
    "# plt.ylim((0,100))\n",
    "ax1.set_ylim([0,100])\n",
    "ax3.set_ylim([-1,0])\n",
    "\n",
    "#add a legend and a tight plot box\n",
    "ax2.legend(loc='best', framealpha=0.0)#, bbox_to_anchor=(1.00,1.00))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Run on click event\n",
    "w2 = widgets.HTML(\"Click on the pixel you would like to interrogate\")\n",
    "ka = fig.canvas.mpl_connect('button_press_event', onclick_timeseries)\n",
    "display(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial WIT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg=roi_pg\n",
    "# results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for axis 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c715d2d8c5e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m## Extract dataset matching polygon and date selected from WIT plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msWIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTimeIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36misel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0mvar_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvar_indexers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m                 \u001b[0mvar_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_indexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdrop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvar_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoord_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                     \u001b[0mcoord_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36misel\u001b[0;34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \"\"\"\n\u001b[1;32m    766\u001b[0m         \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mduck_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.6/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexing_array_and_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for axis 0 with size 11"
     ]
    }
   ],
   "source": [
    "## Polygonise pixel edges as per Bex's `Polygonise_pixel_edges.ipynb`\n",
    "## SO CLOSE TO MULTI POLYGON SPATIAL WIT PLOTTING. TRY TO STOP PLOTS FROM OVERLAPPING.\n",
    "\n",
    "pg=pg[4]\n",
    "# pg=roi_pg\n",
    "\n",
    "# Plot raster data\n",
    "plt.clf()\n",
    "plt.close()#'all')\n",
    "\n",
    "\n",
    "## Colour palette as per WIT\n",
    "ndvi_pal = [\n",
    "#        sns.xkcd_rgb[\"cobalt blue\"],\n",
    "       sns.xkcd_rgb[\"beige\"],\n",
    "       sns.xkcd_rgb[\"light green\"],\n",
    "       sns.xkcd_rgb[\"green\"],\n",
    "       sns.xkcd_rgb[\"dark green\"]]\n",
    "\n",
    "ndwi_pal = [\n",
    "    sns.xkcd_rgb[\"white\"],\n",
    "    sns.xkcd_rgb[\"cobalt blue\"]]\n",
    "\n",
    "# for x in range(0,len(pg)):\n",
    "    \n",
    "## Data preparation - pg must be a single value\n",
    "cl = gdf.loc[gdf['OBJECTID'] == pg].id.values.item()\n",
    "#     print(cl)\n",
    "## class polygon\n",
    "pg1 = gdf.loc[gdf['OBJECTID'] == pg].index.values.item()\n",
    "#     print(pg1)\n",
    "\n",
    "## Extract dataset matching polygon and date selected from WIT plot\n",
    "sWIT = results2[str(cl)][str(pg1)].isel(time=TimeIndex)\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=[8,8])#nrows = len(pg),\n",
    "#                         ncols = 1)#figsize=[8, 8])\n",
    "sWIT.NDWI.where(sWIT.lowest_20_mask).where(sWIT.NDWI > 0).plot(ax=ax1, levels=[-1, 0, 1], colors=ndwi_pal)\n",
    "sWIT.NDVI.where(sWIT.lowest_20_mask).where(sWIT.mask).plot(ax=ax1, levels=[0, 0.1, 0.33, 0.66, 1], colors = ndvi_pal)\n",
    "\n",
    "mpl.axes.Axes.set_aspect(ax1, aspect=1)#, anchor='C')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "045ab485daf24868b7c2382f424cbcb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_b0ea6054510a498e8a7c56ce35cc5c4c",
       "style": "IPY_MODEL_f660eac494c147198a27997ad8b22470"
      }
     },
     "0746427347e2416f920f9ce96228b721": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletGeoJSONModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "data": {
        "bbox": [
         142.4524578701443,
         -32.36320549945,
         142.54736615559136,
         -32.29586369982442
        ],
        "features": [
         {
          "bbox": [
           142.51524944800926,
           -32.31838789697648,
           142.54736615559136,
           -32.29586369982442
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.51524944800926,
              -32.296091685646715
             ],
             [
              142.546601533663,
              -32.29586369982442
             ],
             [
              142.54736615559136,
              -32.3183698586848
             ],
             [
              142.51526741493365,
              -32.31838789697648
             ],
             [
              142.51524944800926,
              -32.296091685646715
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "0",
          "properties": {
           "id": 2,
           "style": {
            "color": "black",
            "fillColor": "#ffffcc",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         },
         {
          "bbox": [
           142.4524578701443,
           -32.36320549945,
           142.4845749551165,
           -32.34069269280065
          ],
          "geometry": {
           "coordinates": [
            [
             [
              142.4524578701443,
              -32.340907825281136
             ],
             [
              142.483823262827,
              -32.34069269280065
             ],
             [
              142.4845749551165,
              -32.36320063502121
             ],
             [
              142.45246271443352,
              -32.36320549945
             ],
             [
              142.4524578701443,
              -32.340907825281136
             ]
            ]
           ],
           "type": "Polygon"
          },
          "id": "1",
          "properties": {
           "id": 1,
           "style": {
            "color": "black",
            "fillColor": "#800026",
            "fillOpacity": 0.8,
            "weight": 0.9
           }
          },
          "type": "Feature"
         }
        ],
        "type": "FeatureCollection"
       },
       "style": {
        "fillOpacity": 0.8
       }
      }
     },
     "140008a05899493688edb1cfd07bb91d": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_8563b85ab05a465b900584182485dc98"
      }
     },
     "215e008ea30446418c9489a3fc2a3509": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "2f3cbd62daf645a4a47a3132fde027d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "600px",
       "width": "800px"
      }
     },
     "331f5be31bf049e4a4c2ab49668abcdd": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "472e3957cd724d3fb57312a3fa86dae1": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     },
     "8563b85ab05a465b900584182485dc98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8912f038ad0947a7ae08ed0ccdb15644": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapModel",
      "state": {
       "_dom_classes": [],
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "basemap": {
        "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
        "max_zoom": 20,
        "name": "Esri.WorldImagery",
        "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
       },
       "center": [
        -32.32953459963721,
        142.4999120128678
       ],
       "controls": [
        "IPY_MODEL_331f5be31bf049e4a4c2ab49668abcdd",
        "IPY_MODEL_472e3957cd724d3fb57312a3fa86dae1"
       ],
       "default_style": "IPY_MODEL_d14c0de5ce3b48bd9eb7729afbe8d701",
       "dragging_style": "IPY_MODEL_a6bbab8038c3446a93e3a549b8f8dd79",
       "east": 142.5685501098633,
       "fullscreen": false,
       "interpolation": "bilinear",
       "layers": [
        "IPY_MODEL_99119a1935514365b929a6d6c4b31a95",
        "IPY_MODEL_0746427347e2416f920f9ce96228b721"
       ],
       "layout": "IPY_MODEL_2f3cbd62daf645a4a47a3132fde027d0",
       "modisdate": "yesterday",
       "north": -32.28597166993233,
       "options": [
        "basemap",
        "bounce_at_zoom_limits",
        "box_zoom",
        "center",
        "close_popup_on_click",
        "double_click_zoom",
        "dragging",
        "fullscreen",
        "inertia",
        "inertia_deceleration",
        "inertia_max_speed",
        "interpolation",
        "keyboard",
        "keyboard_pan_offset",
        "keyboard_zoom_offset",
        "max_zoom",
        "min_zoom",
        "scroll_wheel_zoom",
        "tap",
        "tap_tolerance",
        "touch_zoom",
        "world_copy_jump",
        "zoom",
        "zoom_animation_threshold",
        "zoom_start"
       ],
       "south": -32.373002604986546,
       "style": "IPY_MODEL_215e008ea30446418c9489a3fc2a3509",
       "west": 142.4312210083008,
       "zoom": 13
      }
     },
     "99119a1935514365b929a6d6c4b31a95": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletTileLayerModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "attribution": "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community",
       "base": true,
       "max_native_zoom": 18,
       "max_zoom": 20,
       "min_native_zoom": 0,
       "min_zoom": 1,
       "name": "Esri.WorldImagery",
       "no_wrap": false,
       "options": [
        "attribution",
        "detect_retina",
        "max_native_zoom",
        "max_zoom",
        "min_native_zoom",
        "min_zoom",
        "no_wrap",
        "tile_size"
       ],
       "url": "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
      }
     },
     "a6bbab8038c3446a93e3a549b8f8dd79": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "move"
      }
     },
     "b0ea6054510a498e8a7c56ce35cc5c4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d14c0de5ce3b48bd9eb7729afbe8d701": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletMapStyleModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "cursor": "grab"
      }
     },
     "f660eac494c147198a27997ad8b22470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
